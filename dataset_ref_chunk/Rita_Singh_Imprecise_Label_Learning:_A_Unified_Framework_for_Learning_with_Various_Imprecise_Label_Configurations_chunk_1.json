{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main challenge in machine learning regarding annotated data?", "answer": " The main challenge is collecting annotated data for model training, which can be expensive, laborious, time-consuming, and error-prone.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " What are some reasons why collecting precise labels for data can be difficult?", "answer": " Factors such as lack of annotator expertise, privacy concerns, and intrinsic difficulty in ascertaining labels precisely can make collecting precise labels difficult.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " What is imprecise label learning (ILL)?", "answer": " ILL is a framework for the unification of learning with various imprecise label configurations, treating precise labels as latent variables and considering the entire distribution of all possible labeling entailed by the imprecise information.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " How does ILL leverage expectation-maximization (EM) in modeling imprecise label information?", "answer": " ILL uses expectation-maximization (EM) for modeling imprecise label information by treating precise labels as latent variables and considering the entire distribution of all possible labeling entailed by the imprecise information.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " What settings can ILL seamlessly adapt to?", "answer": " ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " How does ILL differ from previous methods in handling imprecise labels?", "answer": " ILL surpasses existing specified techniques by offering a unified framework with robust and effective performance across various challenging settings, instead of proposing specific designs for each emerging imprecise label configuration.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " What are some typical mechanisms of label imprecision addressed in the literature?", "answer": " Some typical mechanisms of label imprecision include partial label learning (PLL), semi-supervised learning (SSL), and noisy label learning (NLL).", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " What is the illustration in Fig. 1 demonstrating?", "answer": " Fig. 1 illustrates the full label and imprecise label configurations, such as full label, partial label, semi-supervised, and noisy label scenarios, using an example dataset.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " How do existing approaches handle individual configurations of label imprecision?", "answer": " Existing approaches differ substantially and are tailored to specific forms of imprecision, which may complicate the deployment in practical settings where complex annotations with multiple coexisting imprecision configurations exist.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}, {"question": " What is the challenge in adapting previous methods to scenarios with multiple coexisting imprecision configurations?", "answer": " It is challenging to adapt previous methods to scenarios with multiple coexisting imprecision configurations, such as both noisy labels and partial labels, due to the unique assumptions and design requirements of each method.", "ref_chunk": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}], "doc_text": "3 2 0 2 p e S 9 2 ] G L . s c [ 3 v 5 1 7 2 1 . 5 0 3 2 : v i X r a Preprint IMPRECISE LABEL LEARNING: A UNIFIED FRAME- WORK FOR LEARNING WITH VARIOUS IMPRECISE LA- BEL CONFIGURATIONS Hao Chen1\u2217, Ankit Shah1, Jindong Wang2, Ran Tao1, Yidong Wang3, Xing Xie2, Masashi Sugiyama4,5, Rita Singh1, Bhiksha Raj1,6 1Carnegie Mellon University, 2Microsoft Research Asia, 3Peking University, 4RIKEN AIP, 5The University of Tokyo, 6Mohamed bin Zayed University of AI ABSTRACT Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as imprecise labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation- maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more impor- tantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. 1 INTRODUCTION One of the critical challenges in machine learning is the collection of annotated data for model training (He et al., 2016; Vaswani et al., 2017; Devlin et al., 2018; Dosovitskiy et al., 2020; Radford et al., 2021). Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, laborious, time-consuming, and error-prone. Often the labels are just intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and concerns related to privacy can also negatively affect the quality and completeness of the annotations. In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as imprecise. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, partial label learning (PLL) (Cour et al., 2011; Luo & Orabona, 2010; Feng et al., 2020b; Wang et al., 2019a; Wen et al., 2021; Wu et al., 2022; Wang et al., 2022a) allows instances to have a set of candidate labels, instead of a single definitive one. Semi-supervised Learning (SSL) (Lee et al., 2013; Samuli & Timo, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Xie et al., 2020b; Zhang et al., 2021b; Wang et al., 2022c; 2023; Chen et al., 2023) seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set. Noisy label learning (NLL) (Xiao et al., 2015a; Bekker & Goldberger, 2016; Goldberger & Ben-Reuven, 2016; Ghosh et al., 2017; Han et al., 2018; Zhang & Sabuncu, 2018; Li et al., 2018; Wang et al., 2019c; \u2217haoc3@andrew.cmu.edu 1 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 True Label Candidate Label \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Unlabeled \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd2\ud835\udc99\ud835\udfd1Label 1Label 2Label 3 Noisy Label Preprint (a) Full Label (b) Partial Label (c) Semi-Supervised (d) Noisy Label Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled. Liu et al., 2020; Li et al., 2020; Ma et al., 2020b; Zhang et al., 2021d) deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of imprecision, including multiple (imprecise) annotations from crowd-sourcing (Ibrahim et al., 2023; Wei et al., 2023), programmable weak supervision (Zhang et al., 2022; Wu et al., 2023a), and bag-level weak supervision (Ilse et al., 2018; Scott & Zhang, 2020; Garg et al., 2021), among others. While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a specific form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve multiple coexisting imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels (Wei et al., 2023) or the existence of the correct label among label candidates (Campagner, 2021), thus requiring additional algorithmic design. In fact, quite a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label (Lian et al., 2022a; Xu et al., 2023) and semi-supervised partial label learning (Wang et al., 2019b; Wang & Zhang, 2020). However, simply introducing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (e.g. through consistency with adjacent data (Lee et al., 2013; Xie et al., 2020a), iterative"}