{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Cross-Modal_Fine-Tuning:_Align_then_Refine_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of training the embedder before fine-tuning the model body?", "answer": " To make the embedded target features resemble the source features on which the pretrained model body performs well.", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " How are the pretrained source embedder and the randomly initialized target embedder denoted?", "answer": " The pretrained source embedder is denoted as fs and the randomly initialized target embedder is denoted as ft.", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " What is the goal in learning ft with respect to the source embeddings fs?", "answer": " The goal is to minimize the distance between the joint distribution of the target embeddings and that of the source embeddings.", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " What is the shape of the output after choosing k for the embedder?", "answer": " (S, D)", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " How is the transformer output mapped after choosing k for the embedder?", "answer": " (S, k^2K)", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " What is mapped to (K, Hin, Win) after applying pixelshuffle to the transformer output?", "answer": " The transformer output", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " What is the purpose of weight refining for downstream adaptation?", "answer": " To align the embedder and predictor with the pretrained model by updating all model parameters to minimize the target loss.", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " What are three distance metrics mentioned for data alignment during embedding learning?", "answer": " Pairwise Euclidean distance, moment-based maximum mean discrepancy (MMD), optimal transport dataset distance (OTDD).", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " Which metric showed the best overall results in embedder learning in the experiments conducted in the text?", "answer": " OTDD", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}, {"question": " Why is OTDD considered effective for data alignment despite being expensive to compute?", "answer": " It not only aligns individual data points but also groups features with the same label together in the embedding space, facilitating fine-tuning.", "ref_chunk": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}], "doc_text": "data so that they become closer to the pretraining modality. One way to achieve this is to train the embedder before actu- ally \ufb01ne-tuning the model body in a way that makes the em- bedded target features resemble the source features which the pretrained model body is known to perform well on. Formally, let f s : X s \u2192 \u02d9X denote the pretrained source embedder (the part of ms that transforms the raw data to sequence features) and f t the randomly initialized target embedder discussed in the previous section. We can learn f t to minimize the distance between the joint distribution of the target embeddings (cid:0)f t(xt), yt(cid:1) and that of the source embeddings (cid:0)f s(xs), ys(cid:1). There are many metrics for mea- suring this distributional distance. To understand whether they affect adaptation differently, we perform a preliminary study in Section 3.4 on three representatives. 2For example, consider an image with shape (Cin, Hin, Win). We choose k for the embedder such that Hout \u00d7 Wout \u2248 S so the output shape is (D, Hout, Wout). Then, we \ufb02atten the last two dimensions and transpose to get shape (S, D) compatible with the transformer. The transformer output is mapped to (S, k2K) by a linear layer. We transpose and reshape to get (k2K, Hout, Wout) and apply pixelshuf\ufb02e (Shi et al., 2016) to get (K, Hin, Win). Pretrained Transformer Body. The model body g takes the embedding \u02d9x \u2208 \u02d9X as input and outputs features \u02d9y \u2208 \u02d9Y; Cross-Modal Fine-Tuning: Align then Re\ufb01ne Table 2: Prediction errors (\u2193) on 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), and 4 others. \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. See Appendix A.4.2 for the error bars. Hand-designed CIFAR-100 0-1 error (%) 19.39 Spherical 0-1 error (%) 67.41 Darcy Flow PSICOV relative (cid:96)2 MAE8 3.35 8E-3 Cosmic NinaPro 1-AUROC 0-1 error (%) 0.127 8.73 FSD50K 1- mAP 0.62 DeepSEA Satellite 1 - F1 score 0-1 error (%) 1- AUROC ECG 0.28 19.80 0.30 NAS-Bench-360 DASH 23.39 24.37 48.23 71.28 2.6E-2 7.9E-3 2.94 3.30 0.229 0.19 7.34 6.60 0.60 0.60 0.34 0.32 12.51 12.28 0.32 0.28 Perceiver IO FPT 70.04 10.11 82.57 76.38 2.4E-2 2.1E-2 8.06 4.66 0.485 0.233 22.22 15.69 0.72 0.67 0.66 0.50 15.93 20.83 0.38 0.37 ORCA 6.53 29.85 7.28E-3 1.91 0.152 7.54 0.56 0.28 11.59 0.29 3.3. Weight Re\ufb01ning for Downstream Adaptation After training the embedder, we perform full \ufb01ne-tuning by updating all model parameters to minimize the target loss. This step further aligns the embedder and predictor with the pretrained model. In Section 4.1, we compare ORCA with standard \ufb01ne-tuning without data alignment and show that our approach improves performance while reducing variance. There are orthogonal works that study how to best \ufb01ne-tune a model (e.g., Liu et al., 2022; He et al., 2022). We compare with one strategy used in FPT (Lu et al., 2022) in Section 4.1 but leave further exploration for future work. 3.4. Evaluation of Distribution Alignment Metrics We evaluate the effectiveness of three distance metrics for data alignment during embedding learning: (1) the pairwise Euclidean distance, which aligns the scales and ranges of the datasets without using any distributional information; (2) the moment-based maximum mean discrepancy (MMD) (Gretton et al., 2012), which uses the distribution of f (x) to align the feature means; and (3) optimal transport dataset distance (OTDD) (Alvarez-Melis & Fusi, 2020), which uses both the feature and label distributions (cid:0)f (x), y(cid:1) to align the high-level clustering structure of the datasets. We substitute each metric into the ORCA work\ufb02ow (imple- mentation details in Section 4) and evaluate them on 10 tasks from diverse modalities (benchmark details in Section 4.1). The aggregate performance (Figure 2) and per-task rankings (Appendix A.4.4) show that embedder learning with OTDD has the best overall results, so we use it in our subsequent experiments. We conjecture that its good performance is due to how the label information is considered during alignment. 75 1.4 100-Suboptimal Tasks (%) 1.1 MMD 50 1.2 25 OTDD 0 1.5 1.0 Euclidean 1.3 Figure 2: Performance pro\ufb01les (Dolan & Mor\u00b4e, 2002) of ORCA with different alignment metrics. Larger values (fractions of tasks on which a method is within \u03c4 -factor of the best) are better. The OTDD curve being in the upper left shows it is often the best. different labels using the p-Wasserstein distance associated 2 in \u02d9X , which in turn allows with the l2 distance (cid:107) \u02d9xt \u2212 \u02d9xs(cid:107)2 us to measure the distributional difference in \u02d9X \u00d7 Y: d \u02d9X \u00d7Y (cid:0)( \u02d9xt, yt), ( \u02d9xs, ys)(cid:1) = (cid:0)d \u02d9X ( \u02d9xt, \u02d9xs)p + dY (yt, ys)p(cid:1)1/p We refer the readers to Alvarez-Melis & Fusi (2020) for the exact formulation. Yet the implication from our experiments is that, as we learn f t to minimize OTDD, we are not only aligning individual data points, but also grouping features with the same label together in the embedding space, which could potentially facilitate \ufb01ne-tuning. Despite its effectiveness for data alignment, OTDD is gener- ally expensive to compute. In Section A.1 of the Appendix, we analyze its computational complexity and propose an ef\ufb01cient approximation to it using class-wise subsampling. Before ending this section, we emphasize that our goal is not to discover the best alignment metric but to provide a general \ufb01ne-tuning framework that works regardless of the metric used. Thus, we leave designing more suitable distance metrics for future work. Indeed, for both the source and target datasets, OTDD repre- sents each class label as a distribution over the in-class fea- tures: y (cid:55)\u2192 P ( \u02d9X |Y = y)3. This transforms the source and target label sets into the shared space of distributions over \u02d9X . Then, we can de\ufb01ne the distance dY (yt, ys) between 3This step requires that the labels be discrete, as"}