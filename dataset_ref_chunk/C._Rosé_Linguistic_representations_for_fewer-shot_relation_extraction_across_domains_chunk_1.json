{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/C._Rose\u0301_Linguistic_representations_for_fewer-shot_relation_extraction_across_domains_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the text?", "answer": " The main focus of the text is on using linguistic representations to enhance performance in few-shot transfer settings for relation extraction across domains.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " Why is high-quality data availability considered a limitation in specialized domains for implementing machine learning based NLP methods?", "answer": " High-quality data availability is considered a limitation in specialized domains for implementing machine learning based NLP methods because acquiring such data is time-consuming and expensive.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " What is the purpose of linguistic representations in the context of this study?", "answer": " The purpose of linguistic representations in this study is to encourage learning domain-agnostic representations of relations to help models generalize better in a few-shot setting.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " What are the two linguistic formalisms used in the study to evaluate and compare their impact?", "answer": " The two linguistic formalisms used in the study are dependency parses and abstract meaning representations (AMR).", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " Why is procedural text chosen for the study on relation extraction?", "answer": " Procedural text is chosen for the study on relation extraction because it describes processes that share loose semantic correspondence across domains, making it easier to generalize.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " How do linguistic structures benefit models in learning language features?", "answer": " Linguistic structures benefit models by providing abstraction over the variation of natural language, allowing models to learn salient features of the input.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " What is the impact of linguistic graphs on model performance in few-shot settings according to the experiments?", "answer": " Both AMR parses and dependencies significantly enhance model performance in few-shot settings, but the benefit diminishes when models have more training data.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " What is the significance of linguistic representations in relation extraction?", "answer": " Linguistic representations are significant as they provide features that can act as cross-domain pivots, enhancing generalizability in few-shot transfer settings.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " How do linguistic frameworks aim to help models generalize across domains?", "answer": " Linguistic frameworks aim to help models generalize across domains by providing representations that express meaning in domain-general ways.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}, {"question": " What is the main difference between dependency parsing and AMR?", "answer": " The main difference is that AMR seeks to represent meaning at the sentence level in the form of a directed graph, while dependency parsing remains at a lower level of abstraction.", "ref_chunk": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}], "doc_text": "3 2 0 2 l u J 7 ] L C . s c [ 1 v 3 2 8 3 0 . 7 0 3 2 : v i X r a Linguistic representations for fewer-shot relation extraction across domains Sireesh Gururaja and Ritam Dutt and Tinglong Liao and Carolyn Ros\u00e9 Language Technologies Institute Carnegie Mellon University {sgururaj, rdutt, tinglonl, cprose}@cs.cmu.edu Abstract Recent work has demonstrated the positive im- pact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic rep- resentations enhance generalizability by pro- viding features that function as cross-domain pivots. We focus on the task of relation ex- traction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer- based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extrac- tion in multiple domains. We find that while the inclusion of these graphs results in signifi- cantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility. 1 Introduction In many specialized domains, such as healthcare or finance, one of the principal limitations for the im- plementation of machine learning based NLP meth- ods is the availability of high quality data, which tends to be both time-consuming and expensive to acquire. While pre-trained language models allow impressive performance gains across a number of tasks, those gains frequently fail to generalize to specialized domains. Robust representations that allow models to both take advantage of pretrained models and generalize across domains are therefore highly desirable. Recent works such as Prange et al. (2022) have demonstrated the significant potential of using human-annotated linguistic information as scaffold- ing for learning language models. Other works such as Zhang and Ji (2021) and Bai et al. (2021) use automatically generated semantic annotations. These works depend on the idea that the structure that the linguistic frameworks provide allows mod- els to better learn salient features of the input. In ad- dition, however, linguistic structures offer abstrac- tion over the variation of natural language, provid- ing representations that might express meaning in domain-general ways. We therefore extend earlier work to investigate whether including linguistic rep- resentations encourages learning domain-agnostic representations of relations such that models can generalize better in a few-shot setting, i.e. learn- ing from less high-quality data in new domains. We focus on the case of automatically generated linguistic annotations, to evaluate the impact they can have on downstream tasks without expensive human annotation of parse data. We use two linguistic formalisms, to evaluate and compare their impact: dependency parses, and abstract meaning representations (AMR). AMR (Banarescu et al., 2013) seeks to represent meaning at the level of a sentence in the form of a rooted, directed graph. AMR is based on Propbank (Kings- bury and Palmer, 2002), and factors out syntactic transformations due to verb alternations, passiviza- tion, and relativization, leading to a less sparse ex- pression of textual variance. Dependency parsing, by contrast, remains at a low level of abstraction, with structures that do not nest outside of the words in the original text. We study the problem of relation extraction on procedural text. We use procedural text because of what we term their implicit schemas. Across domains, our datasets describe the process of com- bining ingredients under certain conditions in a sequential fashion to produce a desired product. These range from preparing a cooking recipe to synthesizing a chemical compound to extracting materials from ores. As a result, the relations that we derive from each dataset share a loose semantic correspondence, both to each other and to basic semantic relations such as verb arguments and lo- cations. For example, the actions \u201cboil\u201d and \u201cheat\u201d in \u201cBoil the mixture in a medium saucepan\u201d and \u201cHeat the solvent in the crucible\u201d are similar. We hypothesize that the underlying semantics of all of these datasets are similar enough that mod- els should be able to better generalize across do- mains from the explicit inclusion of syntactic and semantic structural features. We use three datasets across two domains: recipes for cooking, and ma- terials science synthesis procedures. Each of these datasets defines the task of generating a comprehen- sive, descriptive graph representation of a proce- dure. We simplify this task into relation extraction in order to better compare the impact of different linguistic formalisms. We augment a popular transformer-based rela- tion extraction baseline with features derived from AMR (Banarescu et al., 2013) and dependency parses and investigate their impact in a few-shot setting both in-domain and across domains. Ex- periments show that both AMR parses and depen- dencies significantly enhance model performance in few-shot settings but that the benefit disappears when models are trained on more data. We addi- tionally find that while cross-domain transfer can degrade the performance of purely text-based mod- els, models that incorporate linguistic graphs pro- vide gains that are robust to those effects. We make our code available with our submis- sion1. 2 Related Work 2.1 Few-shot Relation Extraction The goal of relation extraction (RE) is to detect and classify the relation between specified enti- ties in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (Liu et al., 2022) and the \u201clong-tail\u201d problem of skewness in relation classes. (Ye and Ling, 2019b). Salient work in that direction includes (i) designing RE-specific pretraining objectives for learning better represen- tations (Baldini Soares et al., 2019; Zhenzhen et al., 2022; Wang et al., 2022), (ii) incorporating meta- information such as relation descriptions (Yang 1https://github.com/ShoRit/flow_graphs et al., 2020; Chen and Li, 2021) a global relation"}