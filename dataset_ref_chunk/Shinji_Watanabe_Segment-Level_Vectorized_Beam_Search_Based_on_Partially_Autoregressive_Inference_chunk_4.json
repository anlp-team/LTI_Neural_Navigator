{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Segment-Level_Vectorized_Beam_Search_Based_on_Partially_Autoregressive_Inference_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What determines whether to end the beam search?", "answer": " Observing the next token of the mask token", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " When is the hypothesis terminated and pruned from the overall search?", "answer": " When the first mask detects a space token _ and the second mask detects b", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " Which models were used to test the effectiveness of PAR?", "answer": " E-Branchformer models pre-trained on datasets like AISHELL-11, JSUT2, LibriSpeech-100h3, LibriSpeech-960h4, TED-LIUM25", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " What were the beam size and CTC weight set to for AR decoding?", "answer": " Beam size: 10, CTC weight: 0.3", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " How was PAR decoding different from AR decoding in terms of CTC weight calculation?", "answer": " CTC prefix score was not computed in PAR decoding, so CTC weight was set to 0", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " Which datasets were used for evaluation in the study?", "answer": " LibriSpeech, TED-LIUM2, JSUT, AISHELL-1", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " What metric was used to measure accuracy in the evaluation?", "answer": " Word error rate (WER) or character error rate (CER)", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " What factor was measured to observe the effect of mask parallel decoding?", "answer": " Maximum memory usage of the GPU", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " What does the Speedup column in Table 3 compare between PAR and AR?", "answer": " Compares the RTF values with PAR compared to AR", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}, {"question": " What observation was made regarding the computation speed of the encoder or decoder during PAR decoding?", "answer": " The computation speed of the encoder or decoder has a small impact on the overall inference speed", "ref_chunk": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}], "doc_text": "AR issues. We determine whether to end the beam search by observing the next token of the mask token. For example, in the example in Fig. 1c, when the first mask detects a space token _ and the second mask detects b, the hypothesis is terminated and pruned from the overall search. 5. EXPERIMENT 5.1. Experimental setup 5.1.1. Models To test the effectiveness of PAR, we compare both its performance and inference speeds of various algorithms to AR and NAR. As PAR decoding can be seen as an optimization of AR decoding, PAR can be tested using pre-trained AR models. As a result, for both AR and PAR inference, we used E-Branchformer models [6] that were pre-trained on the following datasets and are publicly avail- able on the HuggingFace hub: AISHELL-11, JSUT2 LibriSpeech- 100h3, LibriSpeech-960h4, TED-LIUM25. Note that we used two types of models trained with LibriSpeech: a model trained with the entire dataset of 960 hours of audio (LS-960), and another model trained with the subset of 100 hours of audio (LS-100). The model trained with the LS-100 dataset is considered less accurate than the one trained with LS-960, so we used LS-100 to investigate the effect of gCTC accuracy on PAR decoding. For comparison against NAR decoding, we trained models using the Conformer [5] architecture on LS-100. We prepared AR and NAR models with approximately 30 million parameters for a fair evaluation. We used the Conformer model to test if there would be any differences caused by encoder architecture. All the models were trained and evaluated using the ESPnet toolkit [35]. 5.1.2. Decoding setup For AR decoding, we set the beam size to 10, the CTC weight to 0.3, and employed the vectorized beam search [23]. For PAR decoding, the beam size is set to 10, but note that for PAR inference, we do not compute the CTC prefix score, so the CTC weight is set to 0. We also set the Pthres to 0.95 and max iteration to 5 for PAR decoding. The decoding speed was measured using a single RTX 2080 Ti. 5.1.3. Evaluation Datasets We used several datasets of different languages for evaluation, described in Table 2. For this study, we used LibriSpeech and TED-LIUM2 as English datasets, JSUT as a Japanese dataset, and AISHELL-1 as a Mandarin dataset. For LibriSpeech, we evaluated dev and test sets, each containing clean and other sets. Each of the five evaluation datasets was evaluated using the AR/PAR model trained on the equivalent dataset. For example, the E-branchformer model pre-trained on the AISHELL-1 dataset was evaluated using the AISHELL-1 evaluation set. 5.1.4. Metrics We evaluated accuracy with word error rate (WER) or character error rate (CER) according to the dataset. We used the real-time factor (RTF) to measure the inference speed. The maximum memory usage of the GPU during inference is also measured to observe the effect of mask parallel decoding. For the RTF and memory usage, we took the mean of all evaluation sets. The speedup column compares the RTF values with PAR compared to AR. 1https://huggingface.co/pyf98/aishell e branchformer 2https://huggingface.co/pyf98/jsut e branchformer 3https://huggingface.co/pyf98/librispeech 100 e branchformer 4https://huggingface.co/asapp/e branchformer librispeech 5https://huggingface.co/pyf98/tedlium2 e branchformer Table 2: Dataset descriptions. The order of evaluation sets corresponds to the error results in Table 3. \u2020Data split based on ESPnet [35]. Dataset Language Hours Token Metric Evaluation Sets AISHELL-1 [36] JSUT [37] LS-100 [38] LS-960 [38] TED-LIUM2 [39] zh ja en en en 170 10 100 960 210 CER char char CER BPE WER BPE WER BPE WER dev / test dev / test (\u2020) dev-{clean,other} / test-{clean,other} dev-{clean,other} / test-{clean,other} dev / test Table 3: Comparison of WER, CER, RTF, elapsed time, and memory usage for each model. RTF, inference time, and memory usage compare mean and standard deviation (in brackets) values. The symbols (\u2193) and (\u2191) indicate a lower or higher number is preferable, respectively. AR PAR Dataset RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] RTF (\u2193) Error [%] (\u2193) Memory Usage [MB] Speedup (\u2191) AISHELL-1 JSUT LS-100 LS-960 TED-LIUM2 0.027 (0.006) 0.129 (0.021) 0.111 (0.038) 0.110 (0.039) 0.182 (0.060) 4.6 / 5.0 11.8 / 13.2 6.2 / 16.8 / 6.4 / 17.1 2.2 / 5.2 / 2.5 / 5.2 7.3 / 7.1 176.3 (4.2) 208.4 (6.2) 197.5 (25.3) 528.4 (44.8) 281.1 (47.7) 0.010 (0.006) 0.018 (0.008) 0.009 (0.006) 0.008 (0.006) 0.012 (0.008) 4.6 / 4.9 12.0 / 13.3 6.5 / 17.2 / 6.7 / 17.7 2.2 / 5.5 / 2.5 / 5.6 7.6 / 7.3 176.3 (5.9) 199.4 (6.4) 210.6 (81.6) 550.5 (132.6) 474.8 (298.9) 2.70\u00d7 7.17\u00d7 12.33\u00d7 13.75\u00d7 15.17\u00d7 encoder ratio is significantly reduced. Furthermore, we can observe that the increase in inference time with respect to audio length is small, similar to that of NAR. This is because, in this work, we set the max iteration to 5, which means that the number of beam search iterations is limited to a maximum of 5. Moreover, if there are no masks, the inference is simply a gCTC result, where we do not run decoder computation. Therefore, the AR process does not have a high computational cost for long audio inputs. We can confirm that the inference speed of PAR depends on the computation speed of each model, such as the encoder or decoder, considering that the in- ference time slightly increases with audio length and the number of search iterations is fixed. The computation speed of the encoder or decoder depends on the audio length, but its impact on the overall inference speed is small. Fig. 3: Average inference time and proportion of time spent on the encoder and decoder computation during PAR decoding. The de- coder\u2019s share is greatly reduced from AR. 5.2. Results 5.2.1. AR and PAR The evaluation results comparing AR and PAR are shown in Table 3. Focusing on accuracy, it is evident that compared to AR, PAR shows a similar WER or CER for all models and evaluation datasets. The beam search process can properly update the tokens that gCTC can- not accurately estimate."}