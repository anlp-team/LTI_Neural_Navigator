{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_BiasX:_\"Thinking_Slow\"_in_Toxic_Content_Moderation_with_Explanations_of_Implied_Social_Biases_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many rounds of training are workers asked to undergo before labeling examples?,answer: Two rounds of training", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " How many participants were randomly assigned to provide labels for 30 selected examples?,answer: 454 participants", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " What is the aim of BIASX in the study?,answer: To improve moderation quality", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " What percentage accuracy improvement was observed with HUMAN-EXPL in hard-toxic and hard-non-toxic examples?,answer: +7.2% and +7.7% respectively", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " What group saw a substantial improvement in moderator performance on a hard-toxic statement in Figure 4a?,answer: BIASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " What was the majority feedback from moderators regarding the awareness of subtle stereotypes in the explanations?,answer: 77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL agreed or strongly agreed", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " What potential downside is mentioned when using explanations in moderation tasks?,answer: Increased mental load and reading time", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " How does the increase in labeling time compare between MODEL-EXPL, HUMAN-EXPL, and LIGHT-EXPL conditions?,answer: 4-5 seconds increase for MODEL-EXPL and HUMAN-EXPL, with a similar increase for LIGHT-EXPL", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " How accurate were the model explanations compared to human-written explanations in the study?,answer: 60% accuracy for model explanations, while human-written explanations always led to correct worker accuracy", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}, {"question": " What is the main aim of the proposed BIASX framework according to the text?,answer: To enable moderators to think more thoroughly about their decisions", "ref_chunk": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}], "doc_text": "can be found in Table 3. 6We use text-davinci-003 in our experiments. 3 \u2212100%0%100% Mentaldemand No-ExplLight-ExplModel-ExplHuman-Expl Light-ExplModel-ExplHuman-Expl disagree \u2212100%0%100% stronglydisagree agree Usefulforsubtle neutral Percentage of participants stronglyagreeResponse Percentage of participants Figure 3: User survey results on mental demand, and whether explanations are useful for subtle stereotypes. rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance. Workers who labeled both posts correctly are recruited into the task stage. A total of N =454 participants are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, partic- ipants also complete a post-study survey which collects their demographics information and sub- jective feedback on the usefulness of the provided explanations and the mental demand of the mod- eration task. Additional details on user interface design are in Appendix C.3. 4 Results and Discussion We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3). BIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL base- line on both hard-toxic (+7.2%) and hard-non-toxic examples (+7.7%), which as a result is reflected as a +4.7% accuracy improvement overall. This indi- cates that explicitly calling out statements\u2019 implied stereotypes or prejudices does encourage content moderators to think more thoroughly about the tox- icity of posts. Illustrating this effect, we show an example of a hard-toxic statement in Figure 4a. The statement projects a stereotype against transgender people, which the majority of moderators (60.3%) in the NO-EXPL condition failed to flag. In contrast, BI- ASX assistance in both MODEL-EXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substan- tially improved moderator performance on this in- 4 A Targeted group: transgender people Implies that transgender people are not their true gender and are just pretending. B Targeted group: transgender women Implies that trans women are not real women. Alabama where eharmony and ancestry are the same website Targeted group: N/A Makes a joke about two unrelated websites.Targeted group: people from Alabama Implies that people from Alabama marry their family. (No-Expl: 39.7%) (Light-Expl: 54.5%) (Model-Expl: 60.2%) (Human-Expl: 58.1%) (No-Expl: 55.4%) (Light-Expl: 52.7%) (Model-Expl: 39.8%) (Human-Expl: 60.0%) After you strip off his makeup and clothes, biologically he's still a man. Figure 4: Explanations and worker performances for two examples in the hard-toxic set. stance. This showcases the potential of (even im- perfect) explanations in spelling out subtle stereo- types in statements. The subjective feedback from moderators further corroborates this observation (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL). Our designed explanation format efficiently pro- motes more thorough decisions. While BIASX helps raise moderators\u2019 awareness of implied bi- ases, it increases the amount of text that moderators read and process, potentially leading to increased mental load and reading time. Thus, we compare our proposed explanation against the LIGHT-EXPL condition, in which moderators only have access to the model-generated targeted group, thus reducing the amount of text to read. Following Bansal et al. (2021), we report me- dian labeling times of the participants across con- ditions in Figure 2b. We indeed see a sizable in- crease (4\u20135s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% par- ticipants agreed or strongly agreed that the task was mentally demanding in the LIGHT-EXPL con- dition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead mod- erators without improving accuracy or efficiency. Explanation quality matters. Compared to expert-written explanations, the effect of model- MODEL-EXPL HUMAN-EXPL Evaluation set E U E U hard toxic hard non-toxic easy overall 60.0 90.0 100.0 83.3 56.4 77.7 98.0 77.4 100.0 100.0 100.0 100.0 64.1 80.1 97.0 80.4 Table 1: Binary accuracy of explanations (E) and users (U) in MODEL-EXPL and HUMAN-EXPL conditions. generated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations are imperfect. In Table 1, we compare the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers al- ways have access to correct explanations. Figure 4b shows an example where the model explains an im- plicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over base- lines, highlighting the potential of our frame- work with higher quality explanations and serv- ing as a proof-of-concept of BIASX, while moti- vating future work to explore methods to gener- ate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting. 5 Conclusion and Future Work In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the ob- jective of enabling moderators to think more thor- oughly about their decisions. In an online user study, we find that by adding explanations, humans perform better on hard-toxic examples. The even greater gain in performance with expert-written ex- planations further highlights the potential of fram- ing content moderation under the lens of human-AI collaborative decision making. Our work serves as a proof-of-concept for future investigation in human-AI content moderation, un- der more descriptive paradigms. Most importantly, our research highlights the importance of explain- 7Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic. 5 ing"}