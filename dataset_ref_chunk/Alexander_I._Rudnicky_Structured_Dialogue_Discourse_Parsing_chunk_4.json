{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Structured_Dialogue_Discourse_Parsing_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the first step in calculating Z(\u03b8)?,        answer: The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix.    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " What is the minor4 L(0,0)(\u03b8) equal to?,        answer: The minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance.    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " What algorithm can find the directed spanning tree \u02dcT with maximum weight given A(\u03b8)?,        answer: The Chiu-Liu-Edmonds (CLE) algorithm can find the directed spanning tree \u02dcT with maximum weight given A(\u03b8).    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " Why do we have to first pick the highest-scoring relation for each edge when using the CLE algorithm?,        answer: We have to first pick the highest-scoring relation for each edge because the original CLE algorithm does not accept labeled trees.    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " How does the CLE algorithm degenerate when \u03b8 is strictly upper triangular?,        answer: When \u03b8 is strictly upper triangular, the CLE algorithm degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm.    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " What are the two metrics used for evaluation in the text?,        answer: The two metrics used for evaluation are Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS).    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " What are the two datasets used to train the discourse parser?,        answer: The two datasets used are the STAC corpus and the Ubuntu IRC corpus.    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " What are the differences between the STAC and Ubuntu IRC datasets?,        answer: The differences between the two datasets are analyzed in the text and include differences in average EDU numbers, lexical distributions, and relation distributions.    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " What is the main result presented in Table 2 regarding in-domain training and testing?,        answer: The main result is that the proposed parser is the best among all parsers, surpassing the previous state-of-the-art by certain F1 scores.    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}, {"question": " What checkpoint is used for the in-domain results in the text?,        answer: The Roberta-Base uncased pretrained checkpoint is used for the in-domain results.    ", "ref_chunk": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}], "doc_text": "should be 1, while others should be 0. We illustrate the padding strategy in Figure 4. Note that this strategy holds whether the size of \u02c6L is odd or even. Now, we are ready to calculate Z(\u03b8). The first step is to calculate the graph Laplacian matrix, which is the difference between the degree matrix and adjacency matrix: (cid:40)(cid:80)n i\u2032=1 Ai\u2032,m(\u03b8), if h = m otherwise Lh,m(\u03b8) = \u2212Ah,m(\u03b8), Then the minor4 L(0,0)(\u03b8) is equal to the sum of the weights of all directed spanning trees rooted at the dummy root utterance (Tutte, 1984): (10) 4.3 Optimization of Tree Since we are given the reference tree \u00afT , we can directly maximize the log probability of eq. (6) us- ing any gradient-descent based algorithms, which is also equivalent to minimizing the KL-divergence between the predicted and reference tree distribu- tions. 4.4 Inference of Tree Z(\u03b8) = det( \u02c6L), 4A minor L(x,y) is the determinant of a submatrix con- structed by removing the x-th row and y-th column of L. (11) There is a well-known algorithm - Chiu-Liu- Edmonds (CLE) (Edmonds, 1967; Chu, 1965) that can find the directed spanning tree \u02dcT with maxi- +1 x += +-++---+-+-++-++--+-+-+-++-++---+-+-+= +1 x Figure 4: This is an efficient padding for calculating batch determinant. The original 4 \u00d7 4 matrix is expanded to a 6 \u00d7 6 (leftmost) one for padding. Note that the last two diagonal elements are all ones. The second matrix encodes the coefficients for multiplying sub-matrices. After a series of cofactor expansions, we can see that the determinant of the padded 6 \u00d7 6 matrix is equivalent to the original unpadded 4 \u00d7 4 matrix. mum weight given A(\u03b8) derived in eq. (8). How- ever, we cannot directly apply the CLE algorithm as the original version does not accept labeled trees. To solve this problem, we have to first pick the highest-scoring relation for each edge h,m = maxr Ah,m,r to get A\u2032 \u2208 R(n+1)\u00d7(n+1). A\u2032 Now we can feed this standard form into the CLE algorithm: \u02dcT = CLE(A\u2032). The correctness of this approach can be proved easily by contradiction: suppose the optimal tree includes one edge that is not the highest-scoring one among {Ah,m,r}17 r=1, we can always substitute that edge with the highest- scoring one to get a better tree (contradiction). Note that for a pair of utterances, we only allow one di- rection of link (\u03b8 is strictly upper triangular) so the CLE algorithm in fact degenerates to its undirected version known as Prim/Kruskal\u2019s algorithm (Prim, 1957; Kruskal, 1956). 5 Experiments 5.1 Datasets a fair comparison. The max utterance length is set to 28. The initial learning rate is set to 2e-5 with a linear decay to 0 for 4 epochs. The batch size is 4. The first 10% of training steps is the warmup stage. For all baselines using large pretrained models, we always use the same model checkpoint and tune the learning rate and batch size for them for a fair comparison. 5.3 Metrics We follow the baselines to use two metrics for eval- uation: Unlabeled Attachment Score (UAS): We only care about the existence of a discourse link. In other words, discourse relations do not affect the results. (Also known as Link F1 score) \u2022 Labeled Attachment Score (LAS): It is much harder as it requires both discourse links and relations to be correct. We focus primarily on this metric since it is more informative and (Also is used in downstream applications. known as Link & Rel F1 score) There are two datasets for us to train the discourse parser, one of which is the STAC (Asher et al., 2016) corpus, which is a multi-party dialogue cor- pus collected from an online game, and the other is the Ubuntu IRC corpus (Li et al., 2020a), which compiles technical discussions about Linux. The differences between these two datasets were ana- lyzed in Liu and Chen (2021), where the takeaway messages are: 1) there is no significant difference in their average EDU numbers, 2) the lexical dis- tributions are significantly different sharing only a small portion of common tokens, 3) relation distri- butions are similar. 5.4 Main Results We present the results in Table 2. The left part of the table focuses on in-domain training and testing, which is the standard setting. Bearing in mind that discourse parsers are often used as the first stage of downstream applications, we follow (Liu and Chen, 2021) to benchmark the performance of all parsers in the cross-domain setting. Note that this is an extremely challenging setting as the domains are completely different (gaming vs Linux technical forum). 5.2 Hyperparameters Following Liu and Chen (2021), we use the Roberta-Base uncased pretrained checkpoint for In-domain We first take a look at the in-domain results. Our proposed parser is the best among all parsers, surpassing the previous state-of-the-art by 2.3 on STAC and 1.5 (F1 scores) on Molweni STAC / STAC MOL / MOL STAC / MOL MOL / STAC UAS LAS UAS LAS UAS LAS UAS LAS MST (Afantenos et al., 2015) ILP (Perret et al., 2016) Deep-Seq. (Shi and Huang, 2019) Hierarchical (Liu and Chen, 2021) Struct-Aware (Wang et al., 2021) This Work 69.6 69.0 73.2 73.1 73.4 74.4 52.1 53.1 54.4 57.1 57.3 59.6 69.0 67.3 76.1 80.1 81.6 83.5 48.7 48.3 53.3 56.1 58.4 59.9 61.5 57.0 53.5 60.1 57.0 64.5 24.0 24.1 21.6 32.1 32.9 38.0 60.5 60.4 42.7 48.9 44.7 50.6 14.8 14.5 15.7 26.8 26.1 31.6 Table 2: STAC / MOL means the training dataset is STAC and the testing dataset is MOL. LAS is the harder setting used for downstream applications. Results are the average of three runs. Note that speaker information is still used in this set of experiments, except our parser does not need to model their relations explicitly as described in \u00a7 2.4. under the LAS setting. The trend is similar for the UAS setting. We also want to highlight the improved performance can NOT be attributed to using a"}