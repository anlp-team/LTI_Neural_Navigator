{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Overview_of_Robust_and_Multilingual_Automatic_Evaluation_Metrics\n\nfor_Open-Domain_Dialogue_Systems_at_DSTC_11_Track_4_chunk_9.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the datasets and models mentioned in the text?", "answer": " Some datasets mentioned are Topical-Chat, Wizard of Wikipedia, Wochat, ConvAI2-GRADE, DailyDialog-GRADE, DSTC7, HCChinese ChatEval, DSTC10. Some models mentioned are DailyDialog-GUPTA, FED-Dial, FED-Turn, Persona-SEE, and others.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " What is the purpose of the LCCC corpus mentioned in the text?", "answer": " The LCCC corpus is designed for pretraining the Chinese dialogue model. It contains dialogues collected mainly from Weibo and other open-source Chinese dialogue corpora, filtered based on heuristic rules and classifiers to remove noise.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " How many dialogues were sampled from the original KdConv corpus to form the KdConv-Eval test dataset?", "answer": " 354 dialogues were sampled from the original KdConv corpus to form the KdConv-Eval test dataset.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " What is the purpose of the HCChinese dataset mentioned in the text?", "answer": " The HCChinese dataset consists of dialogues collected by interacting with three state-of-the-art Chinese chatbots on various topics. It was manually checked to filter out inappropriate responses.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " What is the TBD-Q1-2023 corpus mentioned in the text?", "answer": " The TBD-Q1-2023 corpus consists of dialogues with three chatbots: ChatGPT, GPT-3, and BlenderBot3. Student participants had conversations with the chatbots on different topics, collected between November 2022 and March 2023.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " What are some of the emotion categories used for annotation in the dialogues mentioned in the text?", "answer": " Some of the emotion categories used for annotation are angry, happy, sad, etc. The dialogues were automatically annotated by pre-trained emotion classifiers.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " From which Chinese microblogging website were most of the dialogues in the LCCC corpus collected?", "answer": " Most of the dialogues in the LCCC corpus were collected from Weibo, a Chinese microblogging website.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " What is the total number of dialogues in the ESTC dataset?", "answer": " The total number of dialogues in the ESTC dataset is not explicitly mentioned in the text.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " How many human-chatbot multi-turn conversations were collected for the HCChinese dataset?", "answer": " 531 human-chatbot multi-turn conversations were collected for the HCChinese dataset, with 207 from Plato-XL, 224 from XiaoIce, and 100 from the Chinese Di- aloGPT.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}, {"question": " When were the conversations for TBD-Q1-2023 collected?", "answer": " The conversations for TBD-Q1-2023 were collected between November 2022 and March 2023.", "ref_chunk": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}], "doc_text": "2020) Topical-Chat (Gopalakrishnan et al., 2019) Wizard of Wikipedia (Dinan et al., 2019) Wochat (Haro et al., 2016) 8,509 95,305 304,713 102,960 296,105 14,503 107,220 91,452 3,675 23,197 432,036 512,582 162,064 12,781 12,059 235,281 201,999 19,881 415 4,221 83,097 13,116 35,426 1,000 24,850 9,071 193 1,592 37,884 65,215 10,907 12,781 1,000 10,784 22,311 607 20.5 22.58 3.67 7.85 8.36 14.50 4.31 10.08 19.04 14.57 11.40 7.86 14.86 1.00 12.06 21.82 9.05 32.75 7.31 17.93 13.72 13.96 15.05 10.53 15.88 17.74 9.14 10.98 8.47 13.82 11.72 20.16 20.55 23.23 18.83 6.75 Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En En En En En En Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Total 2,636,322 334,470 236.26 255.77 Development ConvAI2-GRADE (Huang et al., 2020) DailyDialog-GRADE (Huang et al., 2020) DailyDialog-GUPTA (Gupta et al., 2019) DailyDialog-ZHAO (Zhao et al., 2020) DSTC7 (Galley et al., 2019) Empathetic-GRADE (Huang et al., 2020) FED-Dial (Mehri and Eskenazi, 2020a)) FED-Turn (Mehri and Eskenazi, 2020a)) HUMOD (Merdivan et al., 2020) Persona-SEE (See et al., 2019) PersonaChat-USR (Mehri and Eskenazi, 2020b) PersonaChat-ZHAO (Zhao et al., 2020) TOPICAL-USR (Mehri and Eskenazi, 2020b) ECM-Eval (Zhou et al., 2018a) KdConv-Eval (Zhou et al., 2020a) LCCC-Eval (Wang et al., 2020a) 1,800 900 2,460 4,248 34,650 900 1,715 3,888 37,468 39,792 2,790 4,614 4,032 3,004 3,499 3,009 600 300 500 900 9,990 300 125 375 9,499 3,316 300 900 360 1,502 354 589 3.0 3.0 4.92 4.72 3.47 3.0 13.72 10.37 3.94 12.0 9.3 5.13 11.2 2.0 9.88 5.11 12.07 12.60 12.37 12.41 15.39 16.65 11.1 10.78 7.97 9.0 12.08 12.06 23.16 13.13 21.11 11.72 Turn Turn Turn Turn Turn Turn Dial Turn Turn Dial Turn Turn Turn Turn Turn Turn En En En En En En En En En En En En En Zh Zh Zh Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es Zh/Es En En En Total 148,769 29,910 104.76 212.64 Test BlenderBot3 (Giorgi et al., 2023; Shuster et al., 2022) ChatGPT (Giorgi et al., 2023; Radford et al., 2018) GPT-3.5 (Giorgi et al., 2023; Brown et al., 2020) HCChinese ChatEval (Sedoc et al., 2019) DSTC10 (Zhang et al., 2022c) JSALT (Rudnicky et al., 2020) 679 462 560 2,017 400 112 46 21 21 17 187 200 28 13 32.33 22 32.94 10.79 2 4 3.54 16.96 91.07 23.73 8.08 8.13 14 17.26 Turn/Dial Turn/Dial Turn/Dial Turn/Dial Turn Turn Turn En En En Zh En En En Zh/Es Zh/Es Zh/Es En Zh/Es Zh/Es Zh/Es Total 4,276 487 107.60 179.23 Table 7: Summary of the train, development, and test datasets. Some information comes from Yeh et al. (2021). Weibo and post-processing, such as the removal of trivial responses and filtering out potential ad- vertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six differ- ent emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversa- tion dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous clean- ing process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, spe- cial characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversa- tions from three categories (movies, music, and travel). These conversations involve detailed ex- changes about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are col- lected by interacting with three state-of-the-art Chi- nese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chi- nese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as pol- itics and race, was avoided. A manual check is performed on each dialogue, and those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese Di- aloGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Cor- pus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and Blender- Bot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chat- bots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Con- versations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT- 3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0."}