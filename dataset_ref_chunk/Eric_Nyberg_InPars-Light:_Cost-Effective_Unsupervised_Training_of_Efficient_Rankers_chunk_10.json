{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_InPars-Light:_Cost-Effective_Unsupervised_Training_of_Efficient_Rankers_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the procedure described in the text for carrying out a statistical test when each model is presented by multiple outcomes/seeds?", "answer": " Obtain a set of query- and seed-specific metric values, average them over seeds, and then compute statistical significance using a paired difference test.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " How is the cost of generation for the GPT-3 Curie model estimated in the text?", "answer": " The cost of generation for the GPT-3 Curie model is estimated to be $0.002 per one thousand tokens.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " What is the estimated cost of querying the OpenAI GPT-3 Curie API for collections like Robust04 and TREC-COVID, according to the text?", "answer": " The estimated cost can be up to $100 for collections like Robust04 and TREC-COVID.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " What is the estimated cost of generation in the InPars-light study mentioned in the text?", "answer": " The estimated cost of generation in the InPars-light study is under $10 per collection.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " What is the re-ranking throughput of the MiniLM-L6-30M model mentioned in the text?", "answer": " The throughput exceeds 500 passages per second.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " According to the text, what provides re-ranking with neural models as a cloud service?", "answer": " Cohere AI provides re-ranking with neural models as a cloud service.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " How long did the all-domain pretraining of the DeBERTA-v3-435M model mentioned in the text take?", "answer": " The all-domain pretraining of the DeBERTA-v3-435M model took about 28 hours.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " What is the total effectiveness evaluation time for the DeBERTA-v3-435M model mentioned in the text?", "answer": " The total effectiveness evaluation time for the DeBERTA-v3-435M model was about 6 hours.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " How many seeds were used for training the rankers according to the text?", "answer": " Three seeds were used for training the rankers.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}, {"question": " In the context of the text, when is the procedure of consistency checking carried out using a much faster model like MiniLM-L6-30M considered?", "answer": " The procedure is considered in the future as an alternative for carrying out consistency checking.", "ref_chunk": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}], "doc_text": "(e.g., MRR) for each query separately. Let mA i 16 Published in Transactions on Machine Learning Research (MM/YYYY) be sequences of query-specific metric values for models A and B, respectively. The paired statistical test is i \u2212 mB then carried out using a sequence of differences mA i . This procedure is not directly applicable when each model is presented by multiple outcomes/seeds. To overcome this issue, we (1) obtain a set of query- and seed-specific metric values, and (2) average them over seeds, thus, reducing the problem to a single-seed statistical testing. In more details, let mA is be sets of query- and seed-specific metric values for models A and B, respectively. Recall that we have three seeds, so s \u2208 {1, 2, 3}. Then, we obtain seed-average runs mA is and compute statistical significance using a paired difference test. is and mB i = 1/3 P3 i = 1/3 P3 is and mB s=1 mA s=1 mB A.2 Cost and Efficiency In the following sub-section, we discuss both the ranking efficiency and query-generation cost. Although one may argue that the cost of generation using open-source models is negligibly small, in reality this is true only if one owns their own hardware and generates enough queries to justify the initial investment. Thus, we make a more reasonable assessment assuming that the user can employ a cheap cloud service. Cost of Query Generation. For the original InPars Bonifacio et al. (2022), the cost of generation for the GPT-3 Curie model is $0.002 per one thousand tokens. The token count includes the length of the prompt and the prompting document.14 We estimate that (depending on the collection) a single generation involves 300 to 500 tokens: long-document collections Robust04 and TREC-COVID both have close to 500 tokens per generation. Taking an estimate of 500 tokens per generation, the cost of querying OpenAI GPT-3 Curie API can be up to $100 for Robust04 and TREC-COVID. Assuming that sampling from the 137-B FLAN model (used by (Dai et al., 2022)) to be as expensive as from the largest GPT-3 model Davinci (which has a similar number of parameters), each generation in the Promptagator study (Dai et al., 2022), was 10x more expensive compared to InPars study (Bonifacio et al., 2022). Moreover, because Dai et al. (2022) generated one million samples per collection, the Promptagator recipe was about two orders of magnitude more expensive compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data). Aside from all-domain pre-training, the two most time-consuming operations were: Evaluation of model effectiveness on large query sets MS MARCO and NQ, which jointly have about 10K queries; 14https://chengh.medium.com/understand-the-pricing-of-gpt3-e646b2d63320 15https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks 16https://lambdalabs.com/service/gpu-cloud#pricing 17https://docs.cohere.com/docs/reranking 17 Published in Transactions on Machine Learning Research (MM/YYYY) Table 5: Best-Seed Results for Unsupervised Training MS MARCO TREC DL 2020 MRR MAP nDCG@10 Robust04 NQ MAP nDCG@20 nDCG@10 TREC COVID nDCG@10 BM25 (ours) 0.1867 0.3612 0.5159 0.2555 0.4285 0.3248 0.6767 MiniLM-L6-30M results MiniLM (InPars) MiniLM (InPars \u25b6 consist. check) MiniLM (InPars all \u25b6 consist. check) ba0.2197 b0.3562 cba0.2422 b0.3844 ca0.2517 a0.3945 ba0.2415 b0.5151 ba0.2380 ba0.4029 ba0.5753 cb0.2615 cba0.4554 cb0.3297 a0.5769 c0.2671 ca0.4691 ca0.3800 b0.6732 ba0.7483 a0.7709 DeBERTA-v3-435M results ba0.2748 a0.4437 ba0.2847 a0.4479 a0.2804 a0.4414 a0.6779 ba0.2874 a0.6813 ba0.3043 a0.6575 a0.3076 ba0.5131 a0.4872 ba0.5417 ca0.4924 ca0.4746 a0.5505 DeBERTA (InPars) DeBERTA (InPars \u25b6 consist. check) DeBERTA (InPars all \u25b6 consist. check) Notes: Best results are marked by bold font (separately for each model). Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. Super-scripted labels denote the following statistically significant differences (thresholds are given in the main text): a: between a given neural ranking model and BM25; b: between (InPars) and (InPars \u25b6 consist. check) when comparing ranking models of same type. c: between (InPars all \u25b6 consist. check) and (InPars \u25b6 consist. check) when comparing ranking models of same type. a0.8118 a0.8305 a0.8259 Consistency checking using DeBERTA-v3-435M model. The total effectiveness evaluation time for DeBERTA-v3-435 was about 6 hours (for all collections). The consistency checking, however, took about 48 hours. In the future, we may consider carrying out consistency checking using a much faster model, such as MiniLM-L6-30M. A.3 Additional Experimental Results Our rankers were trained using three seeds. However, in the case of all-domain pretraining, DeBERTA converged poorly for one seed. Therefore, in Table 5 we present best-seed results. 18"}