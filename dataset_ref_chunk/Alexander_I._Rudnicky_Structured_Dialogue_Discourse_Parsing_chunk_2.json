{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Structured_Dialogue_Discourse_Parsing_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What model do Afantenos et al. (2015) and Perret et al. (2016) use for encoding?", "answer": " MaxEnt model", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " What does the MaxEnt model used by Afantenos et al. and Perret et al. parameterize?", "answer": " Local pairwise scores between utterance pairs", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " How do Liu and Chen (2021) improve the model used by Afantenos et al. and Perret et al.?", "answer": " By using a hierarchical encoder that models contextual information", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " What issue arises with the two-staged solution proposed by Shi and Huang (2019)?", "answer": " The predicted links are discrete, making it not end-to-end trainable", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " How do Wang et al. (2021) connect the two stages in their approach?", "answer": " By using a fully connected graph between all utterances", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " What is the problem with the previous parsers, according to the text?", "answer": " They propose a fully end-to-end encoder while maintaining structured information", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " How do Perret et al. (2016) find the structure in decoding?", "answer": " By solving an integer linear programming problem", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " How do Liu and Chen (2021) treat the links and relations decoding tasks?", "answer": " As a series of independent multiple-choice problems", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " How is the issue of decoupling the joint learning of links and relations mitigated?", "answer": " By unifying the prediction space of links and relations", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}, {"question": " What kind of features are utilized by most of the previous work, except Liu and Chen (2021), in feature usage?", "answer": " Hand-crafted features", "ref_chunk": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}], "doc_text": "Encoding Afantenos et al. (2015); Perret et al. (2016) use a MaxEnt (Ratnaparkhi, 1997) model to parameter- ize local pairwise scores between utterance pairs. Therefore, global and contextual information are not taken into account during the encoding pro- cess. Liu and Chen (2021) improve upon them by using a hierarchical encoder that models the con- textual information. Shi and Huang (2019) inject more structural information by first predicting all the links, followed by a global structured encoding module. However, the predicted links are discrete, making this two-staged solution not end-to-end trainable. To connect the two stages, Wang et al. (2021) instead use a fully connected graph between all utterances. While being fully end-to-end, useful structured bias is not encoded anymore. Based on the drawbacks of previous parsers, we propose a fully end-to-end encoder while maintaining struc- tured information at the same time. Experimental results demonstrate state-of-the- art discoursing parsing performance on two datasets. 2.2 Decoding 2 Task Background We are given a dialogue session D and the links and relations between pairs of utterances labeled using the 17 discourse relations defined in Asher et al. Shi and Huang (2019); Wang et al. (2021); Liu and Chen (2021) treat the links and relations de- coding tasks as a series of independent multiple- choice problems. In other words, the existence of one link has nothing to do with other links. In contrast, Perret et al. (2016) find the structure by solving an integer linear programming problem, but it needs a set of complicated human-designed decoding constraints. Afantenos et al. (2015) is the closest approach to this work, where they run the maximum spanning tree decoding algorithm on the predicted edges only to find the tree structure (links). However, the relations are not jointly de- coded. Instead, we run the modified spanning tree decoding algorithm on the unified link and relation space. 2.3 Link and Relation Prediction All previous work treat the prediction of links and relations as a two-stage process. That is, they first predict the existence of a link, and the relation is predicted only if the link exists. This decouples the joint learning of links and relations. We mitigate this issue by unifying the prediction space of links and relations, making it a three-dimensional tensor. 2.4 Feature Usage Finally, all previous work execpt (Liu and Chen, 2021) utilize some hand-crafted features. To name a few, they explicitly model if two utterances are spoken by the same speaker, or if they belong to the same turn. These features are useful but also make the baseline parsers deeply coupled with them, which might limit the parsing performance if applied to a new dataset. For example, if the new dataset is a transcript of a teleconference or radio exchange, it is likely that we only have the utter- ances recorded as it is expensive and hard to obtain all the speaker and turn information. In contrast, since our model does not rely on such explicitly modeled feature, the performance drop is less than the ones that use them when the speaker and turn information are removed. 3 Structure Formulation The graph G defined in \u00a7 2 can theoretically be any directed acyclic graph, which is generally difficult to optimize. Fortunately, we find that by discarding only a small fraction of the edges, which is 6% for the STAC corpus and 0% for the molweni corpus, we can recover a spanning tree-like structure that permits efficient learning and structure inference. For the nodes having more than one parent, we keep only the latest one.2 In addition, for dangling utterances that do not have any parents, we connect 2This strategy is adopted by all baselines as well. them to the dummy root utterance, so we are in fact optimizing a multi-root tree during training time. Finally, note that our tree structure allows different links to cross each other (Figure 1) and each edge also has a relation label, G(V, E, R) is a labeled directed multi-root non-projective spanning tree, which is referred to as tree for conciseness hereinafter 3. Several questions naturally arise: \u2022 How to parametrize the tree? We will model the pairwise potential scores by an adjacency matrix, where a cell represents the relevance score of a pair of utterances. See \u00a7 4.1. How to learn the correct tree? We calculate the probability of the correct tree among all possible trees encoded by the adjacency ma- trix, and that probability is maximized. This is similar to softmax attention using trees as basic units instead of tokens. See \u00a7 4.2. How to perform inference? Given the learned three-dimensional adjacency matrix, we can run the modified maximum spanning tree in- duction algorithm to induce the tree structure. See \u00a7 4.4. 4 Proposed Framework 4.1 Model Parameterization Given n utterances (aka EDUs) {Ui}n i=1 in a dia- logue session D, we define a discourse pair in D as a 3-tuple (h, m, r), h < m, r \u2208 [1, 17] where h \u2208 [0 . . . n] is the index of the parent utterance, m \u2208 [1 . . . n] is the index of the children utterance, and r is one of the 17 relations. Note that we add a special root utterance h = 0 to be the shared pseudo parent for the first utterances. Note that this root utterance can be chosen arbitrarily, and we use the utterance \u201cThis is the start of a dialogue\u201d in this work. To parameterize the tree, we first model the d- dimensional pairwise representation between a pair of utterances. This can be expressed compactly by a 3-order tensor, which is an adjacency matrix with each element Vh,m being a d-dimensional fea- ture vector, hence V \u2208 R(n+1)\u00d7(n+1)\u00d7d. Each Vh,m is calculated using a BERT (Devlin et al., 2019) model as the encoder. BERT takes a pair of utterances as input, and a special [CLS] token is prepended before the concatenation of the two 3There are four types of spanning trees investigated in the dependency parsing domain"}