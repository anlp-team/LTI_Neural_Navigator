{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Overview_of_Robust_and_Multilingual_Automatic_Evaluation_Metrics\n\nfor_Open-Domain_Dialogue_Systems_at_DSTC_11_Track_4_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the goal for participants in this task?", "answer": " The goal for participants is to propose effective automatic dialogue evaluation metrics that exhibit specific properties and perform well in a multilingual setup.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " What were participants asked to do in terms of proposing a model?", "answer": " Participants were asked to propose a single multilingual model that could provide high correlations with human-annotations in multilingual dialogues and perform well in the hidden multilingual test set.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " What types of models were participants required to use?", "answer": " Participants were required to use pre-trained multilingual models and train them to predict quality metrics using self-supervised techniques.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " What languages were the models expected to perform well in?", "answer": " The models were expected to perform well in English, Spanish, and Chinese.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " What types of conversations were included in the datasets used during the competition?", "answer": " The datasets included open-domain human-human conversations and human-chatbot interactions.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " How many dialogues were included in the training set?", "answer": " The training set consisted of 393,000 dialogues, totaling approximately 3 million turns.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " What service was used to translate the data back and forth in the training set?", "answer": " The data was automatically translated back and forth using the high-quality MS Azure translation service.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " What was the purpose of the development data provided to participants?", "answer": " The development data was intended to help participants verify the multilingualism and robustness capabilities of their models in terms of correlations with human-annotations.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " How were the human-chatbot interactions dataset annotations obtained?", "answer": " The human-chatbot interactions dataset annotations were obtained through manual annotation and translations using various services.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}, {"question": " What was provided to participants to help estimate metric scores between original and translated sentences?", "answer": " Participants were provided with quality annotations and machine translation Quality Estimation (QE) metric scores using the QE COMET system.", "ref_chunk": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}], "doc_text": "this task, the goal for participants is to pro- pose effective automatic dialogue evaluation met- rics that exhibit the properties mentioned above (Section 1.1) and perform well in a multilingual setup (English, Spanish, and Chinese). In concrete, participants were asked to propose a single multi- lingual model that could provide high correlations with human-annotations when evaluated in multi- lingual dialogues (development set in Section 2.1) and perform well in the hidden multilingual test set. Participants were required to use pre-trained multilingual models and train them to predict mul- tidimensional quality metrics using self-supervised techniques and, optionally, fine-tune their system over a subset of the development data. Finally, participants evaluated their models on the development and test sets, expecting to show similar performance in terms of correlations with human-annotations across three languages: En- glish, Spanish, and Chinese. Only development and test sets have human-annotations, and only the test sets were manually translated or paraphrased/back- translated to guarantee the correlations with the original human-annotations on the English data. 2.1 Datasets Datasets summary Table 1 shows the three clus- ters of datasets we used or created during the com- petition. The table shows information about the number of data used to train, develop, and test the proposed metrics. All these datasets clusters were available in English, Spanish, or Chinese and were back-translated into English. CHANEL and CDIAL include open-domain human-human conversations, while DSTC10 includes human- annotations on human-chatbot interactions. The type of annotations or metadata and how each clus- 2https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics ter was used (training/development/test) are indi- cated in the last three rows. Table 7 (Appendix A) provides a brief sum- mary of all the statistics of the train, development, and test datasets. The datasets statistics includ- ing their number of utterances, avg. number of utterances in each conversation, avg. number of context/response words, type of annotations (turn or dialogue level), number of criteria, number of provided annotations, and type of dialogue systems used for generating responses are shown. Train As training set, we used the data released during the CHANEL@JSALT20203,4 (Rudnicky et al., 2020) workshop organized by Johns Hop- kins University. This cluster consisted of a total of 18 well-known human-human dialogue datasets pre-processed and distributed in a standard format. The total number of dialogues was 393k (approx- imately 3M turns). An additional advantage of the data in this cluster is that they have been auto- matically translated back and forth using the same high-quality MS Azure translation service.5 Development As development set, the organiz- ers provided data from two clusters of datasets: DSTC10 and CDIAL. The first one was collected during DSTC10 Track 5 (Zhang et al., 2022c), consisting of more than 35k turn-level human-annotations, which were automatically translated into Spanish and Chinese, and then back-translated into English using MS Azure services. Second, we used datasets provided by THU- COAI6 group (Conversational AI groups from Ts- inghua University), naming this cluster of datasets CDIAL. It contains open-domain human-human dialogues. They are originally in Chinese and in- clude 3,470 dialogues (approximately 130k turns). Furthermore, we provided Chinese to English trans- lations through the SotA Tencent MT7 system. Furthermore, Tencent AI manually annotated \u223c 3k random H-H turns (\u223c1k dialogues) of CDIAL in Chinese (at turn-and dialogue-level). It is important to note that the development data is intended to help participants verify the multilin- 3https://github.com/CHANEL-JSALT-2020/ datasets 4https://www.clsp.jhu.edu/chaval- cha t-dialogue-modeling-and-evaluation/ 5https://azure.microsoft.com/en-us/pr oducts/cognitive-services/translator/ 6https://github.com/thu-coai 7https://www.tencentcloud.com/product s/tmt gualism and robustness capabilities of their models in terms of correlations with human-annotations. Test Furthermore, in order to check the general- ization capabilities of the proposed metrics from the participant, the test data included new English, Chinese, and Spanish data of human-chatbot inter- actions (Appendix B). A new Human-Chatbot English dataset (HCEnglish) with \u223c2k turns (\u223c60 dialogues) with three different SotA chatbots (ChatGPT (Radford et al., 2018), GPT-3.5 (Brown et al., 2020), and BlenderBot 3 (Shuster et al., 2022) (Giorgi et al., 2023)). This dataset was manually annotated (turn-level and dialogue-level) using Amazon Mechanical Turk (AMT), then translated from English to Chinese and Spanish using MS Azure. In addition, a new Human-Chatbot Chinese dataset (HCChinese) consisting of \u223c5k turns (\u223c500 dialogues) was generated with three differ- ent SotA chatbots (Chinese DialoGPT, Microsoft\u2019s Xiaoice (Zhou et al., 2020b) and Baidu\u2019s Plato- XL (Bao et al., 2022)). This dataset was manually annotated (turn and dialogue level) by Tencent AI, and then translated from Chinese to English using the Tencent MT system. Third, hidden data from the DSTC10 data was used for Spanish with a total of \u223c1500 turns (\u223c700 dialogues). Existing turn-level annotations were used, as well as Spanish translations and English back-translations created using MS Azure, which were subsequently manually reviewed. Table 2 shows the number of turns and dia- logues for each test dataset for each language. The DSTC10 datasets did not have annotations at dialogue-level. Metadata Since the quality of translated sen- tences can play an important role in the estima- tion of metric scores, quality annotations between the original sentence and its respective translation were delivered for each turn of all datasets. Ma- chine translation Quality Estimation (QE) metric scores were given to participants using the QE COMET8 (Rei et al., 2020) system. In addition, for task 1, the cosine similarity between the origi- nal sentence and the translated sentence. Thanks to this information, participants could optionally dis- card dialogues or turns that potentially did not get a high translation quality estimation, therefore reduc- ing potential noise but also allowing the creation of more robust metric systems. 8https://github.com/Unbabel/COMET Dataset Name CHANEL DSTC10 CDIAL #datasets 18 7 3 Language English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation English, Spanish/Chinese, and English back-translation Dialogues Type Human-Human Open-Domain Human-Chatbot Open-Domain Human-Human Open-Domain #dialogues/utterances + 390,000 / + 3,000,000 + 18,000 / + 55,000 + 3,470 / +130,000 Annotations Sentiment analysis and Toxicity Sentiment analysis and Toxicity Turn /dialogue level human scores Turn /dialogue level human scores Task 1 Set Public: Train Public: Dev, Test Hidden: Automatic Translations Public: Train, Dev Task 2 Set Public: Train Public: Dev, Test"}