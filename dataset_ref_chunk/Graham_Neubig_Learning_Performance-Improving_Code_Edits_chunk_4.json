{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Learning_Performance-Improving_Code_Edits_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the models used in the experiments mentioned in the text?,answer: Some of the models used in the experiments are CODELLAMA 7B, CODELLAMA 13B, CODELLAMA 34B, GPT-3.5, and GPT-4.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " What is the purpose of using pretrained checkpoints from CODELLAMA models and OpenAI in the experiments?,answer: The purpose is to evaluate and adapt models from these pretrained checkpoints.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " What is the focus when using the CODELLAMA family of models?,answer: The focus is on using the base set of models that have not been instruction-tuned, as the authors note that instruction-tuning diminished the performance on code generation.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " How is performance measured in the experiments?,answer: Performance is measured using metrics such as Percent Optimized [%OPT], Speedup [SPEEDUP], and Percent Correct [%Correct].", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " What decoding strategy is used for code generation in the experiments?,answer: Sampling multiple candidate outputs for each input and choosing the fastest program that passes all test cases.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " What does BEST@k denote in the context of the study?,answer: BEST@k denotes the strategy of sampling k outputs and choosing the best one with a temperature of 0.7.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " What was the observation regarding the use of generic few-shot prompts in the study?,answer: The observation was that generic few-shot prompts often yield similar results compared to simple instruction-prompting, potentially leading to sub-optimal performance.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " How did the study find the use of COT prompting compared to other prompting approaches?,answer: The study found improvements with COT prompting over instruction-tuned and fixed prompt setups, especially for larger models like CODELLAMA 13B and 34B and GPT-3.5.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " What were the results of the dynamic retrieval-based few-shot prompting strategy in the study?,answer: The results showed that dynamic few-shot prompting outperformed all the baseline variants, indicating effective adaptation of LLMs for program optimization in few-shot settings.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}, {"question": " What was the impact of fine-tuning with PIE on the models discussed in the text?,answer: Fine-tuning with PIE substantially improved all models that were fine-tuned.", "ref_chunk": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}], "doc_text": "Black-Box, Retrieval CODELLAMA 34B GPT4 42.16% 69.03% 2.57 3.56 77.92% 95.90% Open-Source, FineTune CODELLAMA 13B-PC Black Box, FineTune GPT-3.5, SP 66.60% 87.68% 5.65 6.86 71.08% 95.11% 4 EXPERIMENTS Models. We evaluate and adapt models from the CODELLAMA models (Rozi\u00e8re et al., 2023) and models from OpenAI available through their API. We also used pretrained checkpoints of (Rozi\u00e8re et al., 2023): CODELLAMA {7B,13B,34B} obtained via HuggingFace (Wolf et al., 2020). For the CODELLAMA family of models, we use the base set of models that have not been instruction-tuned, as the authors of the paper note that instruction-tuning diminished the performance on code generation. We provide training details in Appendix A.6. We experiment with gpt-3.5-turbo-0613 by prompting the pre-trained model and using the fine-tuning API. We evaluate gpt-4-0613 by prompting the pre-trained model; to date, fine- tuning GPT4 is not available through the API. Metrics. To evaluate performance, we measure the following for functionally correct programs: Percent Optimized [%OPT]: The fraction of programs in the test set (out of 1000 unseen samples) improved by a certain method. A program must be at least 10% faster and correct to contribute. Speedup [SPEEDUP]: the absolute improvement in running time. If o and n are the \u201cold\u201d and \u201cnew\u201d (cid:1). A program must be correct to contribute. n Percent Correct [%Correct]: The proportion of programs in the test set that are at least functionally equivalent to the original program (included as a secondary outcome). As described in Section 2, we count a program as functionally correct if it passes every test case in our dataset. Though correctness is not our primary focus, we include it to help interpret our results. In addition, we report our SPEEDUP as the average speedup across all test set examples. For generations that are either incorrect or slower than the original program, we use a speedup of 1.0 for that example, given that, in the worst case, the original program has a speedup of 1.0. We benchmark performance using our gem5 6 Preprint. Under review. Table 2: Prompting: Results for different prompting strategies and models for Best@1 and Best@8. *Note that for GPT4, we report only with Best@4 due to resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Instruction-Only Instruction-Only Instruction-Only Instruction-Only Instruction-Only CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 GPT-4 0.94% 0.42% 2.93% 16.29% 8.60% 1.01 1.00 1.05 1.20 1.15 5.44% 24.06% 2.51% 10.24% 19.46% 45.40% 80.75% 39.10% 93.45% 16.58%* 1.06 1.03 1.27 1.54 1.23* Few-Shot Few-Shot Few-Shot Few-Shot CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 2.20% 2.30% 2.73% 11.70% 1.02 1.02 1.03 1.13 44.67% 41.27% 45.23% 83.07% 9.73% 14.11% 14.06% 29.68% 1.15 1.21 1.17 1.39 COT COT COT COT CODELLAMA 7B CODELLAMA 13B CODELLAMA 34B GPT-3.5 0.84% 2.30% 4.08% 21.78% 1.01 1.04 1.08 1.26 28.11% 33.75% 31.14% 67.01% 7.63% 11.49% 20.06% 43.78% 1.13 1.21 1.30 1.61 Dynamic Retrieval, K=2 CODELLAMA 7B Dynamic Retrieval, K=2 CODELLAMA 13B Dynamic Retrieval, K=2 CODELLAMA 34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT-3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 80.82 48.16% 69.03%* 2.14 3.56* environment and all test cases mentioned in Section 2. We compile all C++ programs with GCC version 9.3.0 and C++17 as well as the -O3 optimization flag; therefore, any reported improvements would be those on top of the optimizing compiler. Decoding strategy. Code generation is known to benefit from sampling multiple candidate outputs for each input and choosing the best one (Li et al., 2021); in our case, \u201cbest\u201d is the fastest program that passes all test cases. We use BEST@k to denote this strategy with k samples and a temperature of 0.7. 4.1 RESULTS FOR FEW-SHOT PROMPTING Baseline few-shot prompting. Table 2 (top) shows results on standard few-shot prompting techniques (Sec- tion 3.1, prompts are shown in appendix A.8). We find that generic few-shot prompts often yield similar results compared to simple instruction-prompting. For instance, when prompted with instructions alone, both GPT-3.5 and CODELLAMA 34B demonstrated superior %OPT and SPEEDUP metrics. This observation aligns with the findings of Zhao et al. (2021), which highlighted that few-shot examples can sometimes bias the model and lead to an incorrect understanding of the task. In the context of our study, the consistent use of the same fixed prompt might constrain the model to only apply optimization techniques present in the prompt, thereby resulting in sub-optimal performance. Finally, in line with the findings of Wei et al. (2022a) that identified COT prompting as an emergent capability, we observe improvements with this approach over both instruction-tuned and fixed prompt setups, but notably only for the larger CODELLAMA (13B and 34B) and GPT-3.5 models. Retrieval-based few-shot prompting. Table 2 (bottom) shows results using our dynamic retrieval-based few-shot prompting strategy, with the optimal setting at K = 2 retrieved prompts. Extended results for 7 Correct 70.19% 41.38% 87.24% 98.78% 98.46%* 87.45% 85.27% 85.20% 98.43% 75.24% 81.19% 80.88% 93.15% 56.74% 66.25% 77.92% 97.96% 95.90%* Preprint. Under review. K \u2208 {1, 2, 4} are detailed in Appendix A.5. The results show that dynamic few-shot prompting outperforms all the baseline variants, showing that PIE effectively adapts LLMs for program optimization in few-shot settings. We note that increased speedup may, however, come with some cost of correctness. 4.2 RESULTS FOR FINETUNING Table 3: Fine-Tuning: Results for various models and dataset configurations. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 7.33% 12.93% 1.23 1.53 58.04% 55.50% 31.16% 43.79% 2.04 2.71 HQ HQ HQ CODELLAMA 7B CODELLAMA 13B GPT-3.5 10.29% 11.51% 38.49% 1.40 1.43 2.70 76.48% 70.47% 59.16% 45.21% 47.86% 86.66% 3.13 3.43 6.74 All w/Perf-Cond CODELLAMA 7B All w/Perf-Cond CODELLAMA 13B 25.05% 31.87% 2.44 2.95 34.73% 38.70% 56.82% 66.60% 4.84 5.65 HQ + Self-Play HQ + Self-Play HQ + Self-Play CODELLAMA 7B CODELLAMA 13B GPT-3.5 15.27% 14.26% 45.62% 1.58 1.61 3.02 75.87% 76.37% 61.71% 46.13% 49.59% 87.68% 3.31 3.50 6.86 Fine-tuning with PIE substantially improves all models. We fine-tune CODELLAMA and GPT-3.5 models on our"}