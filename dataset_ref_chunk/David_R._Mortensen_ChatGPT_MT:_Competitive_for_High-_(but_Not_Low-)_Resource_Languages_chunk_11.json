{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_ChatGPT_MT:_Competitive_for_High-_(but_Not_Low-)_Resource_Languages_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of Table 11 in the text?", "answer": " Table 11 provides continued data from the previous page regarding various language-related metrics like BLEU scores.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " According to the text, what does Figure 4 display?", "answer": " Figure 4 displays chrF scores across all MT systems and languages.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " What is the estimated cost in USD to translate the FLORES-200 devtest set ENG\u2192X for each target language and MT system?", "answer": " The estimated cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " What does Table 14 in the text provide information about?", "answer": " Table 14 provides estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each target language and MT system.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " How are the hexagons arranged in Figure 8?", "answer": " The hexagons in Figure 8 are arranged in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left and the lowest at the bottom right.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " What is the content of Figure 9 according to the text?", "answer": " Figure 9 shows alternative visualizations compared to those in Figure 8, with groups and languages organized from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " Why were certain macrolanguages used in Table 12?", "answer": " Macrolanguages were used in Table 12 because the corresponding specific languages were not present.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " What does Table 13 indicate about the FLORES-200 languages?", "answer": " Table 13 indicates languages for which the assigned wiki_ct was zero, despite the existence of Wikipedia pages in the corresponding macrolanguage.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " What does the column spBLEU200/cost show in the text?", "answer": " The column spBLEU200/cost shows data related to GPT-4 with values for certain languages and metrics.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}, {"question": " According to the text, what is one reason for assigning wiki_ct as zero in certain cases?", "answer": " One reason for assigning wiki_ct as zero was due to the presence of Wikipedia pages in a corresponding macrolanguage.", "ref_chunk": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}], "doc_text": "43.3 44.7 65.9 58.6 53.7 53.7 23.1 16.7 46.8 55.9 51.2 60.5 42.7 25.8 41.6 48.5 50.0 42.1 35.2 58.3 37.9 32.3 45.3 56.3 26.6 48.9 50.6 51.6 59.5 Language war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 0-shot 24.3 2.1 5.3 10.6 2.5 26.4 36.3 29.3 41.4 6.7 Table 11 \u2013 continued from previous page spBLEU200 5-shot GPT-4 NLLB Google \u2013 \u2013 29.5 16.8 4.9 \u2013 43.6 \u2013 47.5 32.0 25.0 1.2 6.0 18.7 3.3 33.8 36.5 30.4 41.3 7.3 28.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 35.0 9.6 25.4 18.4 10.5 16.6 26.6 12.4 45.5 31.4 0-shot 49.3 10.6 21.9 31.0 11.4 22.3 31.0 24.8 64.5 25.2 chrF2++ 5-shot GPT-4 NLLB Google \u2013 \u2013 52.2 37.7 20.0 \u2013 37.8 \u2013 68.0 53.9 49.5 8.3 23.3 38.1 13.7 27.2 31.9 26.0 64.3 26.3 54.0 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 57.4 29.7 48.6 38.6 25.5 17.9 22.8 14.0 66.5 53.3 Figure 4: chrF scores across all MT systems and languages Figure 5: BLEU scores across all MT systems and languages Figure 6: chrF scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 7: BLEU scores divided by the estimated cost of each MT system, across all MT systems and languages Figure 8: ChatGPT relative improvement over NLLB chrF (color scale), with languages organized by family, script, and number of Wikipedia pages (divided in quartiles). Hexagons (one per language) are displayed in descending order across rows, with the highest ChatGPT relative improvement over NLLB chrF2++ at the top left, and the lowest at the bottom right. Group hexagons at the bottom of each plot display the average color for each group and are organized in like manner. Figure 9: Alternative visualizations to those in Figure 8. Groups and languages are organized the same here: from top left to bottom right in descending order of the ChatGPT relative improvement over NLLB (using averages for the groups). FLORES lang. arb bho dik fuv knc lvs plt khk gaz pes pbt quy als uzn ydd zsm substitution for wiki_ct Used macrolanguage \u2018Arabic\u2019 (ara) because \u2018Standard Arabic\u2019 (arb) not present Used macrolanguage \u2018Bihari\u2019 (bih) because \u2018Bhojpuri\u2019 (bho) not present Used macrolanguage \u2018Dinka\u2019 (din) because \u2018Southwestern Dinka\u2019 (dik) not present Used macrolanguage \u2018Fula\u2019 (ful) because \"Nigerian Fulfulde\" (fuv) not present Used macrolanguage \u2018Kanuri\u2019 (kau) because \u2018Central Kanuri\u2019 (knc) not present Used macrolanguage \u2018Latvian\u2019 (lav) because \u2018Standard Latvian\u2019 (lvs) not present Used macrolanguage \u2018Malagasy\u2019 (mlg) because \u2018Plateau Malagasy\u2019 (plt) not present Used macrolanguage \u2018Mongolian\u2019 (mon) because \u2018Halh Mongolian\u2019 (khk) not present Used macrolanguage \u2018Oromo\u2019 (orm) because \u2018West Central Oromo\u2019 (gaz) not present Used macrolanguage \u2018Persian\u2019 (fas) because \u2018Western Persian\u2019 (pes) not present Used macrolanguage \u2018Pashto\u2019 (pus) because \u2018Southern Pashto\u2019 (pbt) not present Used macrolanguage \u2018Quechua\u2019 (que) because \u2018Ayuacucho Quechua\u2019 (quy) not present Used macrolanguage \u2018Albanian\u2019 (sqi) because \u2018Tosk Albanian\u2019 (als) not present Used macrolanguage \u2018Uzbek\u2019 (uzb) because \u2018Northern Uzbek\u2019 (uzn) not present Used macrolanguage \u2018Yiddish\u2019 (yid) because \u2018Eastern Yiddish\u2019 (ydd) not present Used macrolangauge \u2018Malay\u2019 (msa) because \u2018Standard Malay\u2019 (zsm) not present Table 12: FLORES-200 languages for which we used the Wikipedia page count associated with a macrolanguage of another ISO 639-3 code FLORES lang. acm acq aeb ajp apc ars mag prs reason for assigning wiki_ct = 0 Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Mesopotamian Arabic\u2019 (acm) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tai\u2019izzi Arabic\u2019 (acq) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Tunisian Arabic\u2019 (aeb) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018South Levantine Arabic\u2019 (ajp) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018North Levantine Arabic\u2019 (apc) Macrolanguage \u2018Arabic\u2019 (ara) appears to be in \u2018Standard Arabic\u2019 (arb), not \u2018Najdi Arabic\u2019 (ars) Macrolanguage \u2018Bihari\u2019 (bih) appears to be in \u2018Bhojpuri\u2019 (bho), not \u2018Magahi\u2019 (mag) Macrolanguage \u2018Persian\u2019 (fas) appears to be in \u2018Western Persian\u2019 (pes), not \u2018Dari\u2019 (prs) Table 13: FLORES-200 languages for which we used assigned wiki_ct to be zero, despite the existence of Wikipedia pages in a corresponding macrolanguage Table 14: Estimated costs in USD to translate the FLORES-200 devtest set ENG\u2192X for each targe language and MT system, along with BLEU and chrF scores divided by the cost estimates, where applicable. The cost is roughly $0.09 for NLLB and $2.66 for Google Translate for all target languages. Lang. ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn 0-shot 0.9 4.0 22.4 24.6 19.3 39.5 25.0 2.3 27.6 2.3 24.2 29.9 4.2 28.5 15.3 20.9 5.7 26.4 11.5 0.1 2.7 13.4 4.0 0.3 9.0 15.3 1.1 15.4 8.8 1.5 9.2 0.1 33.3 4.3 spBLEU200/cost GPT-4 \u2013 \u2013 1.2 \u2013 \u2013 \u2013 1.3 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 5-shot 0.9 2.2 13.9 14.9 11.6 25.8 15.3 1.4 17.5 1.1 14.7 17.6 4.2 17.3 9.2 12.5 3.6 18.1 6.4 0.0 1.6 8.7 2.2 0.3 4.8 9.1 0.5 7.7 4.8 1.3 6.8 0.1 21.9 1.4 NLLB 5.1 10.7 10.8 24.8 18.3 41.0 33.4 10.8 36.3 28.8 33.8 39.6 \u2013 33.8 21.5 29.6 20.6 31.9 25.3 7.0 5.0 22.7 27.9 8.6 17.9 25.1 12.5 33.0 21.7 5.3 20.2 7.7 37.6 8.4 Google \u2013 \u2013 \u2013 \u2013 \u2013 13.3 \u2013 \u2013 \u2013 9.3 \u2013 13.3 2.1 \u2013 \u2013 \u2013 6.4 \u2013 \u2013 2.0 \u2013 \u2013 \u2013 2.6 \u2013 8.2 \u2013 10.3 5.7 \u2013 \u2013 \u2013 12.0 \u2013 Continued on next page 0-shot 6.3 15.8 35.6 37.7 32.7 56.2 37.4 9.5 46.0 6.4 36.2 42.7 18.1 41.7 28.8 33.7 16.1 45.3 26.1 2.8 13.6 31.1 14.8 3.7 25.4 30.0 7.3 27.2 21.9 7.7 30.2 6.4 49.9 19.2 5-shot 4.0 9.1 21.6 22.4 19.4 36.8 22.5 6.1 28.9 3.2 21.8 25.0 13.4 24.7 17.2 20.0 8.8"}