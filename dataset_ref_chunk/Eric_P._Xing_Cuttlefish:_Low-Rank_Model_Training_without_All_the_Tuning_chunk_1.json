{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Cuttlefish:_Low-Rank_Model_Training_without_All_the_Tuning_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the automated low-rank training approach introduced in the paper?", "answer": " CUTTLEFISH", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " What is the observation that CUTTLEFISH leverages to switch from full-rank to low-rank training?", "answer": " Stable ranks of all layers converging to a constant value", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " How much smaller can models generated by CUTTLEFISH be compared to full-rank models?", "answer": " Up to 5.6\u00d7 smaller", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " What is the primary goal of training low-rank neural networks?", "answer": " To reduce the total number of trainable parameters without sacrificing predictive accuracy", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " Why is training large neural network models challenging?", "answer": " Due to the exponential growth in the number of parameters which makes training increasingly challenging even with advanced accelerators", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " What is the drawback of starting training low-rank networks from scratch?", "answer": " It may result in significant accuracy loss", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " What can improper tuning of factorization ranks in low-rank models lead to?", "answer": " Either large models or diminished predictive accuracy", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " What is the approach suggested in previous studies to transition to low-rank model training?", "answer": " Starting with full-rank model training for a specific number of epochs before transitioning", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " Why is selecting the appropriate number of full-rank training epochs essential?", "answer": " It influences the final model accuracy", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}, {"question": " How does CUTTLEFISH determine the rank of the factorization in low-rank model training?", "answer": " By using the stable ranks of all layers as the rank of the factorization", "ref_chunk": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}], "doc_text": "3 2 0 2 y a M 5 ] G L . s c [ 2 v 8 3 5 2 0 . 5 0 3 2 : v i X r a CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING Hongyi Wang 1 Saurabh Agarwal 2 Pongsakorn U-chupala 3 Yoshiki Tanaka 3 Eric P. Xing 4 1 5 Dimitris Papailiopoulos 6 ABSTRACT Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacri\ufb01cing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish. 1 As neural network-based models have experienced expo- nential growth in the number of parameters, ranging from 23 million in ResNet-50 (2015) to 175 billion in GPT-3 (2020) and OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training these models has become increasingly challenging, even with the assistance of state-of-the-art accelerators like GPUs and TPUs. This problem is particularly pronounced in resource-limited settings, such as cross-device federated learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b). In response to this challenge, researchers have explored the reduction of trainable parameters during the early stages of training (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy for speeding up the training process. INTRODUCTION Previous research efforts have focused on developing several approaches to reduce the number of trainable parameters during training. One method involves designing compact neural architectures, such as MobileNets (Howard et al., 1Machine Learning Department, Carnegie Mellon Uni- versity 2Department of Computer Sciences, University of Wisconsin-Madison 3Sony Group Corporation 4Mohamed bin Zayed University of Arti\ufb01cial Inc. 6Department of Electrical and Computer Engineering, Univer- sity of Wisconsin-Madison. Correspondence to: Hongyi Wang <hongyiwa@andrew.cmu.edu>. Intelligence 5Petuum, 2017) and Ef\ufb01cientNets (Tan & Le, 2019), which demand fewer FLOPs. However, this may potentially compromise model performance. Another alternative is weight prun- ing, which reduces the number of parameters in neural net- works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda et al., 2020; Sreenivasan et al., 2022b). While unstruc- tured sparsity pruning methods can result in low hardware resource utilization, recent advancements have proposed structured pruning based on low-rank weight matrices to tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b; Vodrahalli et al., 2022). However, training low-rank models necessitates tuning additional hyperparameters for factoriza- tion, such as the width/rank of the factorization per layer, in order to achieve both compact model sizes, as measured by the number of parameters, and high accuracy. Striking the right balance between the size of a low-rank model and its accuracy is crucial, and depends on accurately tuning the rank of the factorized layers. As demonstrated in Figure 1, improper tuning of factorization ranks can lead to either large models or diminished predictive accuracy. Training low-rank networks from scratch may cause signi\ufb01- cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). To address this, previous studies have suggested starting with full-rank model training for a speci\ufb01c num- ber of epochs, E, before transitioning to low-rank model training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a). However, varying the number of full-rank training epochs can in\ufb02uence the \ufb01nal model accuracy, as illustrated in Fig- Proceedings of the 6 th MLSys Conference, Miami Beach, FL, USA, 2023. Copyright 2023 by the author(s). CUTTLEFISH: Low-rank Model Training without All The Tuning ure 1. Thus, selecting the appropriate number of full-rank training epochs, E, is essential (e.g., neither E = 0 nor E = 120 yield the optimal model accuracy). Furthermore, to attain satisfactory accuracy, some earlier work (Wang et al., 2021a) has proposed excluding the factorization from the \ufb01rst K layers, resulting in a \u201chybrid network\u201d that bal- ances model size and accuracy through the choice of K. rapidly during the initial stages of training and then sta- bilizes around a constant value (as depicted in Figure 2). We exploit this observation to develop a simple heuristic for selecting the layer ranks R and the full-rank training duration E: (i) transition from full-rank model training to low-rank model training when all the layer\u2019s stable ranks have converged to a constant, and (ii) use these constants as the rank of the factorization. 93 (E=40, K=1) (E=0, K=1) (E=120, K=1) 95Validation Accuracy 90 1.0Model Parameters1e7 Vanilla 0.2 (E=80, K=1) 0.6 91 0.8 92 Cuttlefish(ours) 94 0.4 Pufferfish Layer-6 Layer-15 0.4Rank Ratio 5.0 0.0 0.0 Layer-12 12.5% of the Total Training Epochs 2.5 7.5 Layer-3 10.0 0.2 Layer-0 Layer-9 94.0 0.4 94.5 0.6 (E=80, K=5) Pufferfish (E=80, K=9) Cuttlefish(ours) (E=80, K=1) 0.8 1.0Model Parameters1e7 Vanilla 93.0 95.0Validation Accuracy 92.0 (E=80, K=7) 0.2 93.5 92.5 Figure 1. Comparison between CUTTLEFISH and grid search tun- ing results: (top): \ufb01xing K = 1 (the very \ufb01rst convolution layer is always not factorized) and varying E \u2208 {0, 40, 80, 120} and varying the selection of R by choosing various \ufb01xed rank ratios. (bottom): \ufb01xing a good choice of E, e.g., E = 80 and varying K and the rank ratio."}