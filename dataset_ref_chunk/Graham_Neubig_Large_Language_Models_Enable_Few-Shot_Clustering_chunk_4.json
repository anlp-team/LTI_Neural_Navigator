{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Large_Language_Models_Enable_Few-Shot_Clustering_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two other methods for text clustering mentioned in the text?", "answer": " SCCL and ClusterLLM", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " How do SCCL and ClusterLLM improve clusters?", "answer": " They use contrastive learning of deep encoders.", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " What does SCCL combine to learn features from text?", "answer": " Deep embedding clustering with unsupervised contrastive learning.", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " How does ClusterLLM use LLMs to improve learned features?", "answer": " It uses triplet feedback from the LLM to decide the cluster granularity and generate clusters.", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " Which encoders are used for Bank77 and CLINC in the comparison with SCCL and ClusterLLM?", "answer": " Instructor (Su et al., 2022) and DistilBERT (finetuned for sentence similarity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019)", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " According to the results, what method is the most effective for expanding textual representations?", "answer": " Using the LLM to expand textual representations.", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " What does the Keyphrase Clustering approach achieve in the comparison?", "answer": " It achieves the best results on 3 of 5 datasets.", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " What is compared in Table 3 regarding the effect of LLM intervention?", "answer": " The effect of LLM intervention without demonstrations or instructions.", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " What is the result of the ablation study regarding instructions and demonstrations for the LLM?", "answer": " Providing both instructions and demonstrations gives the most consistent positive effect.", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}, {"question": " What do the results of the ablation study suggest about providing instructions and demonstrations to the LLM?", "answer": " Providing both instructions and demonstrations enables the LLM to improve cluster quality.", "ref_chunk": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}], "doc_text": "include two other methods for text clus- tering, where previously reported: SCCL (Zhang et al., 2021) and ClusterLLM (Zhang et al., 2023). Both use constrastive learning of deep encoders to improve clusters, making these significantly more complicated and compute-intensive than our pro- posed methods. SCCL combines deep embedding clustering (Xie et al., 2015) with unsupervised con- trastive learning to learn features from text. Clus- terLLM uses LLMs to improve the learned features. After running hierarchical clustering, they also use triplet feedback from the LLM (\u201cis point A more similar to point B or point C?\u201d) to decide the cluster granularity from the cluster hierarchy and gener- ate a flat set of clusters. To compare effectively with these approaches, we use the same encoders reported for SCCL and ClusterLLM in prior works: Instructor (Su et al., 2022) for Bank77 and CLINC and DistilBERT (finetuned for sentence similar- ity classification) (Sanh et al., 2019; Reimers and Gurevych, 2019) for Tweet. 5 Results 5.1 Summary of Results We summarize empirical results for entity canoni- calization in Table 1 and text clustering in Table 2.5 We find that using the LLM to expand textual repre- sentations is the most effective, achieving state-of- the-art results on both canonicalization datasets and significantly outperforming a K-Means baseline for all text clustering datasets. Pairwise constraint K- means, when provided with 20,000 pairwise con- 4This model is distilbert-base-nli-stsb-mean-tokens on HuggingFace. 5As discussed in Section 4, when performing entity canon- icalization, we assign mentions to the same cluster if they con- tain the same entity surface form (e.g. \u201cMarseille\u201d), following prior work (Vashishth et al., 2018; Shen et al., 2022). This ap- proach leads to irreducible errors for polysemous noun phrases (e.g. \u201cMarseille\u201d may refer to the athletic club Olympique de Marseille or the city Marseille). To our knowledge, we are the first to highlight the limita- tions of this \u201csurface form clustering\u201d approach. We present the optimal performance under this assumption in Table 1, finding that the baseline of Shen et al. (2022) is already near- optimal on some metrics, particularly for ReVerb45k. Dataset / Method OPIEC59k ReVerb45k Macro F1 Micro F1 Pair F1 Avg Macro F1 Micro F1 Pair F1 Avg Optimal Clust. 80.3 \u00b10.0 97.0 \u00b10.0 95.5 \u00b10.0 90.9 84.8 \u00b10.0 93.5 \u00b10.0 92.1 \u00b10.0 90.1 CMVC 52.8 \u00b10.0 90.7 \u00b10.0 84.7 \u00b10.0 76.1 66.1 \u00b10.0 87.9 \u00b10.0 89.4 \u00b10.0 81.1 KMeans 53.5 \u00b10.0 91.0 \u00b10.0 85.6 \u00b10.0 76.7 69.6 \u00b10.0 89.1 \u00b10.0 89.3 \u00b10.0 82.7 s r u o PCKMeans LLM Correction Keyphrase Clust. 58.7 \u00b10.0 58.7 \u00b10.0 60.3 \u00b10.0 91.5 \u00b10.0 91.5 \u00b10.0 92.5 \u00b10.0 86.1 \u00b10.0 85.2 \u00b10.0 87.3 \u00b10.0 78.7 78.4 80.0 72.0 \u00b10.0 69.9 \u00b10.0 72.3 \u00b10.0 88.5 \u00b10.0 89.2 \u00b10.0 90.2 \u00b10.0 87.0 \u00b10.0 88.4 \u00b10.0 90.0 \u00b10.0 82.5 82.5 84.2 Table 1: Comparing methods for integrating LLMs into entity canonicalization. \u201cCMVC\u201d refers to the multi-view clustering method of Shen et al. (2022), while \u201cKMeans\u201d refers to our simplified reimplementation of the same method. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. Dataset / Method Bank77 CLINC Tweet Acc NMI Acc NMI Acc NMI \u2013 \u00b10.0 ClusterLLM 71.2 \u00b10.0 SCCL \u2013 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 83.8 \u00b10.0 \u2013 \u00b10.0 \u2013 \u00b10.0 78.2 \u00b10.0 \u2013 \u00b10.0 89.2 \u00b10.0 \u2013 \u00b10.0 KMeans 64.0 \u00b10.0 81.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 57.5 \u00b10.0 80.6 \u00b10.0 s r u o PCKMeans LLM Correction Keyphrase Clustering 59.6 \u00b10.0 64.1 \u00b10.0 65.3 \u00b10.0 79.6 \u00b10.0 81.9 \u00b10.0 82.4 \u00b10.0 79.6 \u00b10.0 77.8 \u00b10.0 79.4 \u00b10.0 92.1 \u00b10.0 91.3 \u00b10.0 92.6 \u00b10.0 65.3 \u00b10.0 59.0 \u00b10.0 62.0 \u00b10.0 85.1 \u00b10.0 81.5 \u00b10.0 83.8 \u00b10.0 Table 2: Comparing methods for integrating LLMs into text clustering. \u201cSCCL\u201d refers to Zhang et al. (2021) while \u201cClusterLLM\u201d refers to Zhang et al. (2023). We use the same base encoders as those methods in our experiments. Where applicable, standard deviations are obtained by running clustering 5 times with different seeds. straints pseudo-labeled by an LLM, achieves strong performance on 3 of 5 datasets (beating the current state-of-the-art on OPIEC59k). Below, we con- duct more in-depth analyses on what makes each method (in-)effective. Dataset / Method Keyphrase Clust. w/o Instructions w/o Demonstrations OPIEC59k Avg F1 80.0 79.1 79.8 CLINC Acc NMI 79.4 \u00b10.0 92.6 \u00b10.0 78.4 \u00b10.0 92.7 \u00b10.0 78.7 \u00b10.0 91.8 \u00b10.0 5.2 LLMs excel at text expansion In Table 1 and Table 2, we see that the \u201cKeyphrase Clustering\u201d approach is our strongest approach, achieving the best results on 3 of 5 datasets (and giving comparable performance to the next strongest method, pseudo-oracle PCKMeans, on the other 2 datasets). This suggests that LLMs are useful for expanding the contents of text to facili- tate clustering. Instructor-base Instructor-large Instructor-XL (Su et al., 2022) - - - - - - - - 74.8 \u00b10.0 90.7 \u00b10.0 77.7 \u00b10.0 91.5 \u00b10.0 77.2 \u00b10.0 91.9 \u00b10.0 Instructor-XL (GPT-3.5 prompt) - - 70.8 \u00b10.0 88.6 \u00b10.0 Table 3: We compare the effect of LLM intervention without demonstrations or without instructions. We see that GPT-3.5-based Keyphrase Clustering outperforms instruction-finetuned encoders of different sizes, even when we provide the same prompt. What makes LLMs useful in this capacity? Is it the ability to specify task-specific modeling instruc- tions, the ability to implicitly specify a similarity function via demonstrations, or do LLMs contain knowledge that smaller neural encoders lack? We answer this question with an ablation study. For OPIEC59k and CLINC, we consider the \u201cKeyphrase Clustering\u201d technique but omit either the instruction or the demonstration examples from the prompt. For CLINC, we also compare with K-Means clustering on features from the Instructor model, which allows us to specify a short instruc- tion to a small encoder. We find empirically that providing either instructions or demonstrations in the prompt to the LLM enables the LLM to im- prove cluster quality, but that providing both gives the most consistent positive effect. Qualitatively, providing instructions but omitting demonstrations Dataset / Method OPIEC59k CLINC Tweet Counts Data Size 2138 # of LLM Reassignmnts 109 Accuracy of Reassignments"}