{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Active_Retrieval_Augmented_Generation_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the hypothesis regarding when to retrieve in active retrieval augmented generation?", "answer": " Linguistic Models (LMs) should retrieve information only when they lack the required knowledge to avoid unnecessary or inappropriate retrieval.", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " According to the text, what does low probability/confidence often indicate for large LMs?", "answer": " Low probability/confidence often indicates a lack of knowledge for large LMs.", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " What is the goal of active retrieval in the generation process?", "answer": " The goal of active retrieval is to benefit future generations.", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " What is the approach proposed by the text to anticipate the future in active retrieval augmented generation?", "answer": " Generating a temporary next sentence, using it as a query to retrieve relevant documents, and then regenerating the next sentence conditioning on the retrieved documents.", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " What is the proposed method to implement the active retrieval augmented generation framework?", "answer": " Forward-Looking Active REtrieval augmented generation (FLARE).", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " What are the four diverse tasks/datasets on which FLARE was evaluated according to the text?", "answer": " Multihop QA (2WikiMultihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summarization (WikiAsp).", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " How does FLARE perform compared to single-time and multi-time retrieval baselines?", "answer": " FLARE achieves superior or competitive performance compared to single-time and multi-time retrieval baselines.", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " What is the main objective of the single-time retrieval augmented generation approach?", "answer": " To directly use the user input as the query for retrieval and generate the complete answer at once.", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " How does active retrieval augmented generation differ from single-time retrieval augmented generation?", "answer": " Active retrieval augmented generation involves actively deciding when and what to retrieve throughout the generation process, while single-time retrieval augmented generation generates the complete answer at once.", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}, {"question": " What is the intuition behind FLAREinstruct in active retrieval augmented generation?", "answer": " To prompt the LM to generate retrieval queries when necessary while generating the answer using retrieval-encouraging instructions.", "ref_chunk": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}], "doc_text": "ac- tively decides when and what to retrieve throughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMul- tihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summa- rization (WikiAsp) (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021). Over all tasks, FLARE achieves superior or com- petitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method. 2 Retrieval Augmented Generation We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation. 2.1 Notations and Definitions Given a user input x and a document corpus D = {di}|D| i=1 (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer y = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing m sentences or n tokens leveraging information retrieved from the corpus. In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents Dq = ret(q) for a query q; the LM conditions on both the user input x and retrieved documents Dq to generate the answer. Since we focus on examining various methods of determin- ing when and what to retrieve, we follow exist- ing methods (Ram et al., 2023; Trivedi et al., 2022) to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: y = LM([Dq, x]), where [\u00b7, \u00b7] is concatenation fol- lowing the specified order. 2.2 Single-time Retrieval Augmented Generation The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once y = LM([Dx, x]). 2.3 Active Retrieval Augmented Generation To aid long-form generation with retrieval, we pro- pose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and genera- tion. Formally, at step t(t \u2265 1), the retrieval query qt is formulated based on both the user input x and previously generated output y<t = [y0, ..., yt\u22121]: qt = qry(x, y<t), where qry(\u00b7) is the query formulation function. At the beginning (t = 1), the previous generation is empty (y<1 = \u2205), and the user input is used as the initial query (q1 = x). Given retrieved documents Dqt, LMs continually generate the answer until the next retrieval is triggered or reaches the end: yt = LM([Dqt, x, y<t]), where yt represents the generated tokens at the cur- rent step t, and the input to LMs is the concatena- tion of the retrieved documents Dqt, the user input x, and the previous generation y<t. We discard previously retrieved documents \u222at\u2032<tDqt\u2032 and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs. 3 FLARE: Forward-Looking Active REtrieval Augmented Generation Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented gener- ation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the an- swer using retrieval-encouraging instructions, de- noted as FLAREinstruct. The second method directly uses the LM\u2019s generation as search queries, denoted as FLAREdirect, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence. 3.1 FLARE with Retrieval Instructions Inspired by Toolformer (Schick et al., 2023), a straightforward way of expressing information needs for retrieval is to generate \u201c[Search(query)]\u201d when additional information is needed (Schick et al., 2023), e.g., \u201cThe colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of mar- tyrs, ...\u201d When working with GPT-3.5 models that Joe Biden attended [Search(Joe Biden degree)] GenerationRetriever$%$%% a law degree.Generate a summary about Joe Biden.Input$&$ Search results: !\"![1]: \u2026[2]: \u2026 Search results: !\"\"[1]: \u2026[2]: \u2026 [Search(Joe Biden University)] the University of Pennsylvania, where he earned Search results: !![1]: \u2026[2]: \u2026 &#%$&%%% Figure 2: An illustration of forward-looking active re- trieval augmented generation"}