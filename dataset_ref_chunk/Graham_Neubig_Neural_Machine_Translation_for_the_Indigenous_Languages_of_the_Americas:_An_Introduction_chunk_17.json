{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Neural_Machine_Translation_for_the_Indigenous_Languages_of_the_Americas:_An_Introduction_chunk_17.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is one way to avoid overfitting when training models for low-resource machine translation?,answer: One way to avoid overfitting is to finetune models on both a HRLs pair and a LRLs pair in a multi-task fashion.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " According to Zoph et al. (2016), how can the vocabulary be represented best?,answer: Zoph et al. (2016) suggest using tied embeddings for representing the vocabulary.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " What type of representations does Edunov et al. (2019) use as pretrained features in the encoder of a transformer model?,answer: Edunov et al. (2019) use ELMO (Peters et al., 2018) representations as pretrained features.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " How do Song et al. (2020) suggest improving performance in machine translation?,answer: Song et al. (2020) suggest combining monolingual texts from linguistically related languages and performing a script mapping.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " What approach does BART (Lewis et al., 2019) use to train the entire transformer model?,answer: BART uses a denoising autoencoder approach to train the entire transformer model.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " How can improvements to BART be achieved according to Li et al. (2020)?,answer: Improvements to BART can be achieved by augmenting the maximum likelihood objective with a data-dependent Gaussian prior distribution.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " What direction does Wang et al. (2019a) take in developing a hybrid architecture for machine translation?,answer: Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " What problem does Serra et al. (2018) focus on in transfer-learning?,answer: Serra et al. (2018) focus on minimizing catastrophic forgetting in transfer-learning.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " According to Vuli\u00b4c et al. (2020), why is isomorphism less likely for bilingual word embeddings when small amounts of monolingual data are used?,answer: Isomorphism is less likely because small amounts of monolingual data can lead to performance decay.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}, {"question": " What ethical concerns are raised when working on machine translation for endangered languages, according to the text?,answer: Ethical concerns include lack of community involvement during language documentation, biased translations, and use of low-quality machine translation systems for endangered languages.", "ref_chunk": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}], "doc_text": "and as pri- ors at training time also improves vanilla models (Baziotis et al., 2020). To avoid overfitting, models can be finetuned on both a HRLs pair and a LRLs pair in a multi-task fashion (Neubig and Hu, 2018). However, how can we represent best the vocab- ulary? Zoph et al. (2016) use separate embeddings for the source and the target language. However, using tied embeddings has been shown to yield better results (Press and Wolf, 2017). Edunov et al. (2019) employs ELMO (Peters et al., 2018) repre- sentations as pretrained features in the encoder of a transformer model. Song et al. (2020) shows that it is possible to improve performance by combin- ing monolingual texts from linguistically related languages, performing a script mapping. It is also possible to extract features from a BERT model in the source language and combining these with an NMT system (Zhu et al., 2020b), but using a BERT model pretrained with a mixed sentences from source and target languages lead to even bet- ter results (Xu et al., 2021). have pretrained models gained popularity in the last years for low- resource MT. Conneau and Lample (2019) proposes training the encoder and the decoder separately in order to get cross-language rep- resentations (XLM). This idea has further been extended by Song et al. (2019, MASS) to masking a sequence of tokens from the input. Training MASS in a multilingual fashion and using monolingual data for pretraining helps to improve NMT for low-resource languages and zero-shot translation (Siddhant et al., 2020). Another approach is to train the entire transformer model as a denoising autoencoder (BART; Lewis et al., 2019). The multilingual version of BART (mBART) is more suitable for NMT tasks and yields important gains (Liu et al., 2020). It is also possible to pretrain a transformer in a multi-task, Encoder-decoder text-to-text fashion, where one of the tasks is MT (T5; Raffel et al., 2020). All four models can be finetuned for MT or used in an unsuper- vised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to im- prove zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) pro- pose to only update the cross attention parameters. A.6 Unsupervised MT The addition of other components such as masked LMs and denoising auto-encoding has also been tried (Stojanovski et al., 2019). Unsupervised methods are vulnerable to adversarial attacks of word substitution and order change in the input. Adversarial training can improve performance in such situations (Sun et al., 2020). Since the ini- tialization step is crucial for UMT, Ren et al. (2020) aligns semantically similar sentences from two monolingual corpora with the help of cross- lingual embeddings. With these, an SMT system is trained to warm up an NMT system. How- ever, UMT still has to overcome a set of chal- lenges. S\u00f8gaard et al. (2018) shows that perfor- mance decays dramatically for languages with dif- ferent typological features, since, in such situa- tions, bilingual word embeddings (Conneau et al., 2017) are far from isomorphic. Vuli\u00b4c et al. (2020) finds that isomorphism is also less likely if small amounts of monolingual data are used for training bilingual word embeddings. Nooralahzadeh et al. (2020) discovers that performance quickly deteri- orates for a mismatch of source and target domain and that the initialization of word embeddings can affect MT performance. All of this makes UMT for LRLs or endangered languages challenging. Some of the described issues have been ad- dressed: Liu et al. (2019) proposes to combine word-level and subword-level embeddings to ac- count for morphological complexity. For the prob- lem of distant language pairs, Leng et al. (2019) proposes pivoting (cf. \u00a76.3). Isomorphism of bilingual word-embeddings can be improved with semi-supervised methods (Vuli\u00b4c et al., 2019). Garcia et al. (2020) introduces multilingual UMT systems. The main idea consists of general- izing UMT by using a multi-way back-translation objective. Recently, pretrained multilingual trans- former networks are used to improve UMT even further (cf. \u00a76.4). B Ethical Considerations Ethical concerns when working on MT for endan- gered languages include a lack of community in- volvement during language documentation, data creation, and development and setup of MT sys- tems. For more information, we refer interested readers to Bird (2020). Finally, we want to men- tion that publicly employing low-quality MT sys- tems for LRLs bears a risk of translating incor- rectly or in biased (e.g., sexist or racist) ways."}