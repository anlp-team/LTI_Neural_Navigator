{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Learning_to_Speak_from_Text:_Zero-Shot_Multilingual_Text-to-Speech_with_Unsupervised_Text_Pretraining_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What did the proposed methods show in terms of MOS for each language?", "answer": " The highest MOS.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " Was there any significant difference observed between the proposed methods and the best baseline method?", "answer": " No significant difference was observed.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " What was the best baseline method in the study?", "answer": " The IPA-based multilingual model with LIDs.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " How did the byte-based and IPA-based proposed models compare in terms of scores?", "answer": " On average, the byte-based proposed model showed 2.89, while the IPA-based proposed model showed 2.84.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " Which proposed model often scored higher than the IPA-based proposed models?", "answer": " The byte-based proposed model.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " What were the MOS values for the oracle methods and the baseline zero-shot method?", "answer": " The oracle methods had the highest MOS of 3.76 and 3.96, while the baseline zero-shot method had the lowest MOS of 3.29.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " How did the byte- and IPA-based models perform compared to the baseline method?", "answer": " The proposed methods outperformed the baseline method, with the byte-based model at 3.44 and the IPA-based model at 3.32.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " What type of test was conducted to compare the naturalness of the byte-based and IPA-based models?", "answer": " A preference AB test.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " What kind of languages have previous studies on multilingual TTS primarily focused on?", "answer": " Resource-rich languages.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}, {"question": " What kind of pretraining technique did the study propose for TTS?", "answer": " Multilingual MLM pretraining using text tokens shared across languages.", "ref_chunk": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}], "doc_text": "For each language, either of the proposed methods showed the high- est MOS, while we did not observe any significant differ- ence between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed meth- ods showed the highest scores in all the languages. On aver- age, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based pro- posed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The ora- cle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the lis- tening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. 4 Related Work Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al., 2012; Li and Zen, 2016], there is growing interest in developing TTS models on low-resource languages. Sev- eral studies have explored the input tokens shared across lan- guages such as bytes [Li et al., 2019a; He et al., 2021], IPA symbols [Gutkin, 2017], and articulatory features [Lux and Vu, 2022], to transfer knowledge from resource-rich to low- resource languages. Grapheme tokens can eliminate the per- 7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al., 2022a]. The relative relationships are more reliable in the AMOS. language G2P knowledge, and previous work has built a byte- based TTS model for around 40 languages [He et al., 2021]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al., 2020]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for low- resource scenarios by leveraging VQ-VAE [Zhang and Lin, 2020] or an ASR model [Ren et al., 2019; Ni et al., 2022]. Other work [Saeki et al., 2022b] has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs text- only training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zero- shot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different lan- guages [Gouws et al., 2015; Ruder et al., 2019]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al., 2019], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lam- ple, 2019]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al., 2021]. Other studies have used phonemes jointly with graphemes [Jia et al., 2021] or sub- phonemes [Zhang et al., 2022] as the inputs of the MLM pre- training. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. 5 Conclusions We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, re- It also improved the sulting in a CER of less than 12%. TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our abla- tion studies provided additional insights, including the effec- tiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of in- telligibility, speech quality, and naturalness, as seen in the evaluation in \u00a7 3.3 and \u00a7 3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limi- tation with language dependency, as the results in \u00a7 3.5 sug- gest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and de- veloping a method that performs better for various languages. Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al., 2015], which is sup- ported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. References [Ba et al., 2016] J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Ba\u02dcn\u00b4on et al., 2020] M. Ba\u02dcn\u00b4on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl`a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Web- scale acquisition of parallel corpora."}