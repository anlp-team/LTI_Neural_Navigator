{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Generating_Images_with_Multimodal_Language_Models_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are multimodal language models capable of processing?", "answer": " Multimodal language models can process image and text inputs to generate text outputs.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " What did the Frozen model demonstrate in terms of visual encoding and text-only language models?", "answer": " The Frozen model showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only language model.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " How does GILL outperform FROMAGe in terms of image handling capabilities?", "answer": " GILL is capable of both image retrieval and image generation, while FROMAGe can only retrieve images in their outputs.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " What properties do large language models exhibit when trained at a large enough scale?", "answer": " Large language models exhibit compelling properties such as the ability to learn from few-shot in-context examples and generate and process long text inputs.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " What is the primary goal of the GILL model architecture?", "answer": " The GILL model architecture is trained with a captioning loss to learn to process images on the left side, and losses for image retrieval and image generation to learn to produce images on the right side.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " What is the task of text-to-image generation?", "answer": " Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " What approach does GILL use to process image and text inputs and produce outputs?", "answer": " GILL adapts a pretrained autoregressive language model of text, keeping most model weights frozen, and finetuning a small number of parameters on image caption data.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " What are some challenges that the model needs to resolve according to the text?", "answer": " The model needs to learn to process image-and-text content, learn to produce images, and determine whether to produce text or images at each step.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " How does the model learn to process images according to the text?", "answer": " The model learns translation parameters that map from image features to text embedding space, using a pretrained visual backbone and a linear mapping.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}, {"question": " What loss function is used to train the linear mapping in the GILL model?", "answer": " The linear mapping in the GILL model is trained using the negative log-likelihood loss of the token sequence.", "ref_chunk": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}], "doc_text": "have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64], improving the visual encoder [4, 33], finetuning on instructions [35], and training unified models on multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to 1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill. 2 InfoNCE Loss Image and Caption Inputs SD Text Encoder Learning to Process Images A grey cat sitting in a basket Image #1 <img> Input Embeddings(seq_len, 4096) GILLMapper Output Embeddings(seq_len, 4096) [IMG1]...[IMG{r}] Cross Entropy Loss [IMG1] Generated Caption(next token prediction) LLM LLM Caption Input Learning to Produce Images Input Caption MSE Loss An European shorthair cat in a woven basket An European shorthair cat in a woven basket Tokenizer Image Generation ... GILLMapper Visual Encoder ... Visual Encoder ... Frozen Model Image Input an an short [IMG{r}] Caption #1 Image-Text Retrieval European European Loss Linear Layer Figure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process images (left), and losses for image retrieval and image generation to learn to produce images (right). generate text interleaved with retrieved images. While FROMAGe can only retrieve images in their outputs, GILL is capable of both image retrieval and image generation, which allows it to outperform retrieval-only models when they are limited by their candidate retrieval set (Fig. 5). Large Language Models Our work leverages recent advances in Transformer-based [57] LLMs. When trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn from few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7]. Our approach also builds upon recent efforts on open sourced LLM weights [69, 55]. Text-to-Image Generation Text-to-image generation is the task of synthesizing a realistic image conditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional GAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha- nisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image generation task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models on discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image quantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion models [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve significant gains in generating relevant images. In contrast with computationally intensive methods that train end-to-end, GILL does not require running the image generation model during training. 3 Method We efficiently adapt a pretrained autoregressive language model of text, to process image and text inputs and produce image and text outputs. Most of the model weights (including those of the base LLM and image generator) are kept frozen, and we finetune a small number of parameters on image caption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that need to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also needs to learn to produce images (either retrieved or generated), and determine whether to produce text or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to decide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3). 3.1 Learning to Process Images Given an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM to enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example, inputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved in a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation parameters that map from image features to text embedding space. We first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and LLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a sequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the 3 [IMG1] I think they look best when they are on a tray with a little bit of space between them. gen <img> ... ... I I [IMG{r}] .. How How should I display these at the farmer's market? should think ret ... (r, 4096) (L, 768) [IMG] ... (L, 512) ... Figure 3: Inference time procedure for GILL. The model takes in image and text inputs, and produces text interleaved with image embeddings. After deciding whether to retrieve or generate for a particular set of tokens, it returns the appropriate image outputs. Figure 4: GILLMapper model architecture. It is conditioned on the hidden [IMG] represen- tations and a sequence of learnt query embedding vectors. LLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by minimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ): lc(x, y) = \u2212 T (cid:88) log p\u03b8(st | v\u03d5(x)T"}