{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_One-for-All:_Generalized_LoRA_for_Parameter-Efficient_Fine-tuning_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the GLoRA method serve as?", "answer": " a superset of all prior solutions", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " How can GLoRA be degraded to any of its predecessor methods?", "answer": " By setting different support tensors to zero", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " How is the GLoRA architecture articulated?", "answer": " as a unified mathematical equation", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " What are the trainable support tensors A, B, C, D, E used for in GLoRA?", "answer": " scaling, input/output shifting, bias scaling, and bias shifting, respectively", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " What is the role of the multi-path supernet in GLoRA?", "answer": " to randomly sample subnets during training for optimization", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " How does the weight entanglement strategy in GLoRA help in increasing the search space?", "answer": " by sharing weights in different subnets", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " What is the fundamental factor enabling model re-parameterization?", "answer": " elimination of non-linearity amidst adjacent transformations", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " How does GLoRA re-parameterize the weight and bias?", "answer": " Wuni = W0 + W0A + B; buni = CW0 + Db0 + E + b0", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " What method is employed to identify the optimal configuration for each layer in GLoRA?", "answer": " evolutionary search method", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}, {"question": " How is model capacity regulated in GLoRA?", "answer": " by selecting an appropriate hypothesis space", "ref_chunk": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}], "doc_text": "tuning in both weight and feature space along with VPT-Deep level prompt design. Additionally, we adopt a re-parameterization strategy to incorporate aux- iliary parameters into the adjacent projection weights during the inference stage. Broadly speaking, our method serves as a superset of all prior solutions, i.e., one-for-all mechanism. By setting different support tensors to zero, our GLoRA can be degraded to any of these prede- cessor methods. Unlike NOAH (Zhang et al., 2022), our architecture can be succinctly artic- ulated as a unified mathematical equation. The consolidated formulation to represent all tunable spaces can be represented as follows: Figure 1: Schematic representation of a linear layer adapted with GLoRA. f (x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0 where A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0 and b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale the input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D and E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1. (10) Module Design. In this subsection, we delineate the methodology for designing layer-wise adap- tors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars, vectors, low-rank decompositions, or none. Based on the role of these trainable sup- port tensors, they can be sampled from the following respective search spaces: A = {LoRA, vector, scalar, none} B = {LoRA, vector, scalar, none} C = {LoRA, vector, none} D = {vector, scalar, none} E = {vector, scalar, none} (11) where none indicates zero, if all the trainable support tensors are zero, the model will be degraded to the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original weight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer, we define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71, D \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and randomly sample a subnet during any given supernet training iteration for optimization. A subnet comprises of a single path network with different layerwise support tensors sampled from Eq. 11. Depending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1 d \u2208 Rd2\u00d7r1, Ar1 u is used as the final tensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed from Ad. A similar strategy is followed for all other support tensors depending upon the current sampled configuration in the subnet. This weight entanglement strategy helps to increase the search space without increasing the number of parameters substantially and also shows faster convergence due to weight sharing in different subnets. u \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1 d \u00d7 Ar1 4 2.3 STRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS The fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is the elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption of supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al., 2023), the removal of such non-linear layers does not detrimentally impact the performance of the networks. The precise concept of GLoRA re-parameterization is explicated as follows: f (x) = Wunix + buni where Wuni and buni are our final unified trained weight and bias in GLoRA. They are re- parameterized according to Eq. 10: Wuni = W0 + W0A + B buni = CW0 + Db0 + E + b0 (14) As a result, the re-parameterization strategy we employ, which integrates learnable parameters into the existing weight matrix offers a distinct advantage as it imposes no additional computational burden during the inference phase. This is further discussed in Section 4 where we provide thorough inference efficiency analysis of GLoRA compared to exisitng works. 2.4 EVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS Our design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity across different layers. To identify the optimal configuration for each layer, we employ the evolu- tionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency and effectiveness. Although the training time may increase due to this search process, it is important to note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such as low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby et al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others), as presented in Appendix. Our unified support tensor design conducts an implicit search that elim- inates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is reasonable and well-justified. More details regarding evolutionary search are in Appendix. 2.5 GLORA WITH HIGHER CAPACITY Model capacity refers to the capability of a model to approximate a diverse range of functions. A method for regulating the capacity of a learning algorithm involves selecting an appropriate hy- pothesis space, essentially a set of functions that the learning algorithm is permitted to consider as potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo- nenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged to provide a formal evidence for this assertion. Theorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni, dvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5 s.t. \u03f5 \u2265 0 In the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni denotes the hypothesis space of the complete supernet. The validity of this theorem stems from the inherent property of our"}