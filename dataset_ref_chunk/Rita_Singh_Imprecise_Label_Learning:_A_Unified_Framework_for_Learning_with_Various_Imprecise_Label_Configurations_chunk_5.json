{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the framework discussed in the text that is much simpler and more concise compared to previous methods?,answer: ILL framework", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " In Semi-supervised learning (SSL), what does the input X consist of?,answer: labeled data X L and unlabeled data X U", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " What are the labels Y U for the unlabeled data X U considered as?,answer: latent variable", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " What is the loss function for Semi-supervised learning (SSL) using ILL?,answer: LSSL ILL = - P(Y U|X U, X L, Y L; \u03b8t) log P(Y U|X U, X L; \u03b8) - log P(Y L|X L; \u03b8)", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " What is one advantage of Eq. (7) discussed in the text?,answer: It adopts the prediction as soft targets of all possible labeling on unlabeled data", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " What does the noise model in Noisy label learning (NLL) involve?,answer: The noisy labels \u02c6Y do not directly reveal the true information about Y", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " What is the loss function for Noisy label learning (NLL) using ILL?,answer: LNLL ILL = - P(Y|X, \u02c6Y ; \u03b8t, \u03c9t) log P(Y|X, \u02c6Y ; \u03b8, \u03c9t) - log P(\u02c6Y|X; \u03b8, \u03c9)", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " What kind of labels are considered in the partial label learning setup discussed in the text?,answer: Partial labels with negative labels flipped to false positive labels", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " How many random seeds were used in the experiments conducted in the text?,answer: Three random seeds", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}, {"question": " What were the datasets used in the partial label learning setup?,answer: CIFAR-10, CIFAR-100, and CUB-200", "ref_chunk": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}], "doc_text": "similar insights as (Wu et al., 2022) which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in ??. Semi-supervised learning (SSL) In SSL, the input X consists of the labeled data X L and the unlabeled data X U. The imprecise label for SSL is realized as the limited number of full labels Y L for X L. The labels Y U for unlabeled X U are unknown and become the latent variable. Inter- estingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior P (Y U|X L, X U, Y L; \u03b8), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since Y L is conditionally independent with Y U given X, the second term of Eq. (5): P (Y L|X L, X U, Y U; \u03b8), is reduced to P (Y L|X L; \u03b8), which corresponds to the supervised objective on labeled data. The loss function for SSL using ILL thus becomes: LSSL ILL = \u2212 (cid:88) P (Y U|X U, X L, Y L; \u03b8t) log P (Y U|X U, X L; \u03b8) \u2212 log P (Y L|X L; \u03b8) Y \u2208[C] = LCE (cid:0)p(y|As(xu); \u03b8), p(y|Aw(xu); \u03b8t)(cid:1) + LCE (cid:0)p(y|Aw(xL); \u03b8), yL(cid:1) The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL (Sohn et al., 2020; Chen et al., 2023). It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \u03c4 . Noisy label learning (NLL). Things become more complicated here since the noisy labels \u02c6Y do not directly reveal the true information about Y , thus P ( \u02c6Y |Y, X; \u03b8) inherently involves a noise model that needs to be learned. We define a simplified instance-independent5 noise transition model T ( \u02c6Y |Y ; \u03c9) with parameters \u03c9, and take a slightly different way to formulate the loss function for NLL: LNLL ILL = \u2212 (cid:88) P (Y |X, \u02c6Y ; \u03b8t, \u03c9t) log P (Y |X, \u02c6Y ; \u03b8, \u03c9t) \u2212 log P ( \u02c6Y |X; \u03b8, \u03c9) Y \u2208[C] = LCE (cid:0)p(y|As(x), \u02c6y; \u03b8, \u03c9t), p(y|Aw(x), \u02c6y; \u03b8t, \u03c9t)(cid:1) + LCE (p(\u02c6y|Aw(x); \u03b8, \u03c9), \u02c6y) , 4To formulate the loss function, we convert the problem to minimization of the negative log-likelihood. 5A more complicated instance-dependent noise model T ( \u02c6Y |Y, X; \u03c9) can also be formulated under our unified framework, but not considered in this work. 6 (6) (7) (8) Preprint Table 1: Accuracy of different partial ratio q on CIFAR-10, CIFAR-100, and CUB-200 for partial label learning. The best and the second best results are indicated in bold and underline respectively. Dataset CIFAR-10 CIFAR-100 CUB-200 Partial Ratio q 0.1 0.3 0.5 0.01 0.05 0.1 0.05 Fully-Supervised 94.91\u00b10.07 73.56\u00b10.10 LWS (Wen et al., 2021) PRODEN (Lv et al., 2020) CC (Feng et al., 2020b) MSE (Feng et al., 2020a) EXP (Feng et al., 2020a) PiCO (Wang et al., 2022a) 90.30\u00b10.60 90.24\u00b10.32 82.30\u00b10.21 79.97\u00b10.45 79.23\u00b10.10 94.39\u00b10.18 88.99\u00b11.43 89.38\u00b10.31 79.08\u00b10.07 75.65\u00b10.28 75.79\u00b10.21 94.18\u00b10.12 86.16\u00b10.85 87.78\u00b10.07 74.05\u00b10.35 67.09\u00b10.66 70.34\u00b11.32 93.58\u00b10.06 65.78\u00b10.02 62.60\u00b10.02 49.76\u00b10.45 49.17\u00b10.05 44.45\u00b11.50 73.09\u00b10.34 59.56\u00b10.33 60.73\u00b10.03 47.62\u00b10.08 46.02\u00b11.82 41.05\u00b11.40 72.74\u00b10.30 53.53\u00b10.08 56.80\u00b10.29 35.72\u00b10.47 43.81\u00b10.49 29.27\u00b12.81 69.91\u00b10.24 39.74\u00b10.47 62.56\u00b10.10 55.61\u00b10.02 22.07\u00b12.36 9.44\u00b12.32 72.17\u00b10.72 Ours 96.37\u00b10.08 96.26\u00b10.03 95.91\u00b10.05 75.31\u00b10.19 74.58\u00b10.03 74.00\u00b10.02 70.77\u00b10.29 where the parameters \u03c9 and \u03b8 are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \u02c6y: p(y|x, \u02c6y; \u03b8, \u03c9t) \u221d p(y|x; \u03b8)T (\u02c6y|y; \u03c9t), and p(\u02c6y|x; \u03b8, \u03c9) = (cid:88) p(y|x; \u03b8)T (\u02c6y|y; \u03c9). y\u2208[C] 4 EXPERIMENTS In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes (John Bridle, 1991; Joulin & Bach, 2012), similarly as (Liu et al., 2022; Wang et al., 2023). All experiments are conducted with three random seeds using NVIDIA V100 GPUs. 4.1 PARTIAL LABEL LEARNING Setup. Following (Wang et al., 2022a), we evaluate our method on partial label learning setting using CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and CUB-200 (Welinder et al., 2010). We generate partially labeled datasets by flipping negative labels to false positive labels with a probability q, denoted as a partial ratio. The C \u22121 negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, q \u2208 {0.01, 0.05, 0.1} for CIFAR-100, and q = 0.05 for CUB-200. We choose six baselines for PLL using ResNet-18 (He et al., 2016): LWS (Wen et al., 2021), PRODEN (Lv et al., 2020), CC (Feng et al., 2020b), MSE and EXP (Feng et al., 2020a), and PiCO (Wang et al., 2022a). The detailed hyper-parameters and comparison with the more recent method R-CR (Wu et al., 2022) that utilizes a different training recipe and model (Zagoruyko & Komodakis, 2016) are shown in Appendix D.2.2. Results. The results for PLL are shown in Table 1. Our method achieves the best performance"}