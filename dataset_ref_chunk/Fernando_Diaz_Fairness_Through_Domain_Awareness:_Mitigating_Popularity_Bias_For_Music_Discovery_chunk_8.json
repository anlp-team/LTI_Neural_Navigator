{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Fairness_Through_Domain_Awareness:_Mitigating_Popularity_Bias_For_Music_Discovery_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the Recall@100 value for the Features model?", "answer": " 0.041", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What is the NDCG@100 value for the PinSage model?", "answer": " 0.001", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What is the Flow value for the ZeroSum model?", "answer": " 0.931", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What is the Diversity value for the Boost model?", "answer": " 0.831", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What is the %LT value for the LightGCN model?", "answer": " 0.002 \u00b1 0.000", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What is the LT Cvg value for the LightGCN model?", "answer": " 0.022", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What is the Artist Cvg value for the ZeroSum model?", "answer": " 0.105 \u00b1 0.000", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What is the visibility defined as in Figure 6?", "answer": " The number of times an item from a group appears in the recommendations normalized by the total number of items in the recommendation lists", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What does hyperparameter sensitivity refer to?", "answer": " The ability of each method to perform under different hyperparameter values", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}, {"question": " What is the main problem this work addresses?", "answer": " Mitigating popularity bias in music recommendation", "ref_chunk": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}], "doc_text": "Features 0.033 MostPop 0.015 0.026 \u00b1 0.001 LightGCN 0.064 \u00b1 0.001 PinSage 0.001 \u00b1 0.003 ZeroSum 0.055 \u00b1 0.001 xQuAD 0.014 \u00b1 0.001 MACR 0.038 \u00b1 0.002 REDRESS 0.005 \u00b1 0.001 BOOST p values 5.696989e-08 Recall@100 0.041 0.044 NDCG@100 0.073 0.048 0.119 \u00b1 0.004 0.144 \u00b1 0.003 0.043 \u00b1 0.002 0.104 \u00b1 0.006 0.030 \u00b1 0.015 0.100 \u00b1 0.003 0.047 \u00b1 0.003 1.768725e-19 0.037 0.011 0.023 \u00b1 0.001 0.095 \u00b1 0.002 0.001 \u00b1 0.001 0.064 \u00b1 0.001 0.014 \u00b1 0.001 0.053 \u00b1 0.004 0.007 \u00b1 0.001 1.179627e-15 Artist Recall@100 0.073 0.141 0.272 \u00b1 0.011 0.139 \u00b1 0.003 0.220 \u00b1 0.008 0.135 \u00b1 0.013 0.149 \u00b1 0.022 0.162 \u00b1 0.004 0.137 \u00b1 0.002 0.727897 0.041 0.046 0.068 \u00b1 0.001 0.077 \u00b1 0.002 0.045 \u00b1 0.004 0.068 \u00b1 0.002 0.049 \u00b1 0.007 0.057 \u00b1 0.001 0.029 \u00b1 0.002 1.914129e-07 Flow 0.900 0.908 0.905 \u00b1 0.000 0.931 \u00b1 0.001 0.904 \u00b1 0.001 0.927 \u00b1 0.004 0.902 \u00b1 0.002 0.969 \u00b1 0.032 0.979 \u00b1 0.000 3.751961e-61 0.996 0.926 0.998\u00b1 0.000 0.969 \u00b1 0.000 0.996 \u00b1 0.008 0.998 \u00b1 0.000 0.996 \u00b1 0.003 0.998 \u00b1 0.002 0.999 \u00b1 0.000 0.001408 Diversity 0.841 0.680 0.672 \u00b1 0.025 0.707 \u00b1 0.003 0.765 \u00b1 0.013 0.703 \u00b1 0.059 0.831 \u00b1 0.034 0.829 \u00b1 0.001 0.899 \u00b1 0.002 1.168816e-29 0.919 0.600 0.505 \u00b1 0.012 0.775 \u00b1 0.003 0.866 \u00b1 0.000 0.801 \u00b1 0.008 0.777 \u00b1 0.050 0.862 \u00b1 0.004 0.941 \u00b1 0.003 1.112495e-34 %LT 0.588 0.0 0.002 \u00b1 0.000 0.476 \u00b1 0.002 0.000 \u00b1 0.003 0.226 \u00b1 0.001 0.019 \u00b1 0.006 0.504 \u00b1 0.003 0.522 \u00b1 0.001 0.000596 0.486 0.000 0.000 \u00b1 0.000 0.437 \u00b1 0.001 0.007 \u00b1 0.000 0.212 \u00b1 0.000 0.002 \u00b1 0.004 0.451 \u00b1 0.000 0.498 \u00b1 0.006 2.477700e-11 LT Cvg 0.022 0.0 0.000 \u00b1 0.000 0.032 \u00b1 0.000 0.000 \u00b1 0.000 0.017 \u00b1 0.000 0.000 \u00b1 0.001 0.036 \u00b1 0.004 0.037 \u00b10.003 - 0.005 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.000 0.000 \u00b1 0.000 0.004 \u00b1 0.000 0.000 \u00b1 0.000 0.008 \u00b1 0.002 0.010 \u00b1 0.000 - Artist Cvg 0.073 0.001 0.025 \u00b1 0.001 0.105 \u00b1 0.000 0.048 \u00b1 0.002 0.098 \u00b1 0.004 0.011 \u00b1 0.003 0.117 \u00b1 0.000 0.125 \u00b10.000 - 0.034 0.001 0.003 \u00b1 0.001 0.053 \u00b1 0.001 0.032 \u00b1 0.001 0.053 \u00b1 0.001 0.001 \u00b1 0.000 0.056 \u00b1 0.000 0.068 \u00b1 0.001 - Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs. Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. items, but also musically relevant ones for recommendation. Finally, our method\u2019s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method\u2019s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular definition for popularity bins can synthetically inflate the perfor- mance of %\ud835\udc3f\ud835\udc47 and \ud835\udc3f\ud835\udc47\ud835\udc36\ud835\udc63\ud835\udc54. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines\u2019 fairness. 6 Conclusion In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users\u2019 ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options. Conference\u201917, July 2017, Washington, DC, USA References [1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/ Retrieved Oct 31, 2021 from https: [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys \u201917. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking. [4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness"}