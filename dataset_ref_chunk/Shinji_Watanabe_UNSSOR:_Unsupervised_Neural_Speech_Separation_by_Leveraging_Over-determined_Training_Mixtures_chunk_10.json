{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_UNSSOR:_Unsupervised_Neural_Speech_Separation_by_Leveraging_Over-determined_Training_Mixtures_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the SMS-WSJ dataset used for?,answer: The SMS-WSJ dataset is used for evaluating two-speaker separation algorithms in reverberant conditions.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " How many microphones are there in the simulated microphone array for the SMS-WSJ dataset?,answer: There are six microphones arranged uniformly on a circle with a diameter of 20 cm.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " What is the range for the speaker-to-array distance in the SMS-WSJ dataset?,answer: The speaker-to-array distance is drawn from the range [1.0, 2.0] m.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " What is the range for the reverberation time (T60) in the SMS-WSJ dataset?,answer: The reverberation time (T60) is sampled from [0.2, 0.5] s.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " What issue does the visualization in Fig. 3 show in the separated speech?,answer: The visualization shows the frequency permutation problem in the separated speech.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " Why are \u00b5 and \u03bd used in the DNN estimates?,answer: \u00b5 and \u03bd are used to mimic cases of good separation (\u00b5 \u2248 1, \u03bd \u2248 0) and bad separation (\u00b5, \u03bd are away from 0 and 1).", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " What does minimizing the proposed loss function encourage?,answer: Minimizing the proposed loss function can promote unsupervised separation of speakers.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " What is the purpose of the appendix that visualizes the surface of the mixture-constraint loss?,answer: The purpose is to show that minimizing the loss can promote unsupervised separation of speakers.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " What does Appendix K experiment with in relation to UNSSOR?,answer: Appendix K experiments with DNN architectures with lower modelling capabilities in UNSSOR.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}, {"question": " What is the corpus content of the SMS-WSJ dataset?,answer: The clean speech in the SMS-WSJ dataset is sampled from the WSJ0 and WSJ1 datasets.", "ref_chunk": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}], "doc_text": "in Proc. Interspeech, 2020, pp. 2472\u20132476. [81] Y. Liu and D. Wang, \u201cCausal Deep CASA for Monaural Talker-Independent Speaker Separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1270\u20131279, 2020. [82] A. Pandey and D. Wang, \u201cDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain,\u201d in Proc. ICASSP, 2020, pp. 6629\u20136633. [83] H. Taherian, K. Tan, and D. Wang, \u201cMulti-Channel Talker-Independent Speaker Separation Through Location-Based Training,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2791\u20132800, 2022. [84] J. Zhang, C. Zorila, R. Doddipatla, and J. Barker, \u201cOn End-to-End Multi-Channel Time Domain Speech Separation in Reverberant Environments,\u201d in Proc. ICASSP, 2020, pp. 6389\u20136393. 14 Appendix This appendix is organized as follows: Appendix A describes the SMS-WSJ dataset. \u2022 Appendix B visualizes the surface of the proposed mixture-constraint loss to show that minimizing the loss can promote unsupervised separation of speakers. Appendix C provides the derivation that FCP can be used to approximate speaker images. \u2022 Appendix D illustrates the frequency permutation problem. \u2022 Appendix E describes differences between the MC loss and the mixture consistency concept. \u2022 Appendix F illustrates the cases on when to use causal and non-causal filtering. \u2022 Appendix G provides interpretations of the physical meanings of intermediate DNN estimates \u02c6Z. \u2022 Appendix H discusses differences between UNSSOR and RAS. \u2022 Appendix I presents miscellaneous system and DNN configurations. \u2022 Appendix J experiments with alternative filters taps for iRAS and UNSSOR. \u2022 Appendix K experiments UNSSOR with DNN architectures with lower modelling capabilities. \u2022 Appendix L reports the results of MixIT. A SMS-WSJ dataset SMS-WSJ [67] is a popular corpus for evaluating two-speaker separation algorithms in reverberant conditions. The clean speech is sampled from the WSJ0 and WSJ1 datasets. The corpus contains 33, 561 (\u223c87.4 h), 982 (\u223c2.5 h), and 1, 332 (\u223c3.4 h) two-speaker mixtures respectively for training, validation, and testing. The simulated microphone array has six microphones arranged uniformly on a circle with a diameter of 20 cm. For each mixture, the speaker-to-array distance is drawn from the range [1.0, 2.0] m, and the reverberation time (T60) is sampled from [0.2, 0.5] s. A weak white noise is added to simulate microphone self-noises, and the energy level between the sum of the reverberant speech signals and the noise is sampled from the range [20, 30] dB. The sampling rate is 8 kHz. B Visualization of loss surface Fig. 2 visualizes the values of LMC in (4) based on a six-channel noisy-reverberant two-speaker mixture sampled from the SMS-WSJ dataset (see Appendix A for the dataset details). Let C = 2 and suppose that the DNN estimates are \u02c6Z(1) = \u00b5 \u00d7 X1(1) + \u03bd \u00d7 X1(2) + \u03b51/2 \u02c6Z(2) = (1 \u2212 \u00b5) \u00d7 X1(1) + (1 \u2212 \u03bd) \u00d7 X1(2) + \u03b51/2, (14) where \u00b5 and \u03bd \u2208 R are bounded in the range [0, 1]. Essentially, we use \u00b5 and \u03bd to mimic the cases that the DNN produces (1) good separation (i.e., when \u00b5 \u2248 1 and \u03bd \u2248 0, or \u00b5 \u2248 0 and \u03bd \u2248 1); and (2) bad separation (i.e., when \u00b5 and \u03bd are both away from 0 and 1, meaning that each esti- mate contains multiple speakers, and when \u00b5 \u2248 0 and \u03bd \u2248 0, or \u00b5 \u2248 1 and \u03bd \u2248 1, meaning that the two speakers are merged into one output and the other output does not contain any speakers). Fig. 2 enumer- ates \u00b5 and \u03bd and plots the resulting separa- tion results against the loss value of (4). We can see the loss values are smallest when \u00b5 \u2248 1 and \u03bd \u2248 0 or when \u00b5 \u2248 0 and \u03bd \u2248 1 (i.e., when the speakers are success- fully separated), and clearly larger other- wise. This indicates that minimizing the proposed loss function can encourage sep- aration. 0.66 0.66 0.33 0.33 0.0 0.0 0.35 0.50 1.0 0.30 0.55 0.45 0.40 1.0 Figure 2: Loss surface of LMC in (4) with P = 6 and C = 2 (i.e., over-determined conditions) against hypothesized separa- tion outputs generated by using various \u00b5 and \u03bd. Best viewed in color. 15 C Effectiveness of FCP at approximating speaker images Following the derivation in [29], let us define the mixture as Yp = Xp(c) + Vp(c), where Vp(c) consists of the signals of all the sources but c. We can formulate (6) as (cid:12) (cid:12) (cid:12)Xp(c, t, f ) + Vp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) 1 \u02c6\u03bbp(c, t, f ) (cid:88) t argmin gp(c,f ) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f )|2+|Vp(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:88) \u2248 argmin gp(c,f ) t 2 (cid:88) , = argmin gp(c,f ) t 2 (cid:12) (cid:12) (cid:12) 2 where the derivation from the first row to the second is based on the assumption that, as the DNN training continues, \u02c6Z(c) would become more and more accurate so that, after some epochs, it can become uncorrelated (or little correlated) with Vp(c), meaning that the cross-term would be small: (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:16) Xp(c, t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:17)H Vp(c, t, f ) \u2248 0. From the third row of (15), the resulting \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f ) would approximate Xp(c, t, f ). D Illustration of frequency permutation problem We use three-channel input and loss for DNN training. Fig. 3(a) and (b) show an example separation result of the model trained with LMC in (9). Comparing the separated speech in Fig. 3(a) and (b) with the clean speech in (c) and (d), we can see that the separated speech suffers from the frequency permutation problem approximately in the range [1.6, 2.9] kHz. Fig. 3(e) and (f)"}