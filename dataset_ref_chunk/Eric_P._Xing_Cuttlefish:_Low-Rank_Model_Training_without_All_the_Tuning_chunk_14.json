{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Cuttlefish:_Low-Rank_Model_Training_without_All_the_Tuning_chunk_14.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many colored training images are there in the ImageNet ILSVRC 2012 dataset?", "answer": " 1,281,167", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " What are the augmentation techniques used for the ImageNet dataset?", "answer": " Normalization, random rotation, random horizontal flip", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " What is the resolution to which the training images in the ImageNet dataset are randomly resized and cropped?", "answer": " 224x224", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " What are the mean values used to normalize each color channel in the ImageNet dataset?", "answer": " \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " What are the standard deviation values used to normalize each color channel in the ImageNet dataset?", "answer": " \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " What are the architectural modifications made to ResNet-18 and ResNet-50?", "answer": " Derived from original design with minor modifications", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " What is special about WideResNet-50-2?", "answer": " It adheres to the original wide residual network design", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " How are the bias terms treated for ResNet-18, ResNet-50, and WideResNet-50-2 networks?", "answer": " They are deactivated for all layers except the final FC layer", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " What are the modifications made to VGG-19 to create VGG-19-BN?", "answer": " Remove first two FC layers, replace max pooling layer with average pooling layer", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}, {"question": " Where are the implementations of DeiT-base and ResMLP-S36 sourced from?", "answer": " Pytorch Image Models (timm) library", "ref_chunk": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}], "doc_text": "and normalization techniques used for CIFAR-10 and CIFAR-100, as described above. ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012 dataset consists of 1,281,167 colored training images spanning 1,000 classes and 50,000 colored validation images, also cov- ering 1,000 classes (Deng et al., 2009). Augmentation tech- niques include normalization, random rotation, and random hor- izontal \ufb02ip. The training images are randomly resized and cropped to a resolution of 224\u00d7224 using the torchvision API torchvision.transforms.RandomResizedCrop. The validation images are \ufb01rst resized to a resolution of 256\u00d7256 and then center cropped to a resolution of 224\u00d7224. Each color channel is normalized with the following mean and stan- dard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. Each channel pixel is normalized by subtracting the corresponding channel\u2019s mean value and then dividing by the color channel\u2019s standard deviation. ResNet-18, ResNet-50, and WideResNet-50-2. The ResNet-18 and ResNet-50 architectures are derived from the orig- inal design with minor modi\ufb01cations (He et al., 2016). The WideResNet-50-2 adheres to the original wide residual network design (Zagoruyko & Komodakis, 2016). As we employed ResNet- 18 for CIFAR-10 classi\ufb01cation, we adjusted the initial convolution layer to use a 3 \u00d7 3 convolution with padding at 1 and stride at 1. Our ResNet-18 implementation follows the GitHub repository 3. For all ResNet-18, ResNet-50, and WideResNet-50-2 networks, the strides used for the four convolution layer stacks are respec- tively 1, 2, 2, 2. Bias terms for all layers are deactivated (owing to the BatchNorm layers), except for the \ufb01nal FC layer. VGG-19-BN. In our experiments, we employ the VGG-19-BN network architecture, which is a modi\ufb01ed version of the original VGG-19 (Simonyan & Zisserman, 2014). The original VGG- 19 network consists of 16 convolution layers and 3 FC layers, including the \ufb01nal linear classi\ufb01cation layer. We adopt the VGG- 19 architectures from (Frankle & Carbin, 2018; Khodak et al., 2020), which remove the \ufb01rst two FC layers following the last convolution layer while retaining the \ufb01nal linear classi\ufb01cation layer. This results in a 17-layer architecture, but we continue to refer to it as VGG-19-BN since it stems from the original VGG-19 design. Another modi\ufb01cation is replacing the max pooling layer after the last convolution layer (conv16) with an average pooling layer. The detailed architecture is displayed in Table 7. We follow the implementation of the pytorch-cifar GitHub repository mentioned above. Due to the BatchNorm layers, bias terms for all layers are deactivated, except for the \ufb01nal FC layer. DeiT and ResMLP. Our implementations of DeiT-base and ResMLP-S36 are sourced directly from the model implementa- tions provided by the Pytorch Image Models (i.e., timm) library 4. For DeiT-base, we do not use the scaled ImageNet resolution ver- sion and we deactivate the distillation options. More speci\ufb01cally, we initiate the training of a deit base patch16 224 model from scratch, as provided by the timm library. For training, we employ the training method and hyperparameters speci\ufb01ed in the original GitHub repository 5. For ResMLP-S36, we adhere to the same training methodology used for DeiT-base, utilizing the resmlp 36 224 provided by the timm library. BERTBASE, DistillBERT, and TinyBERT. The imple- mentations of BERTBASE, DistillBERT, and TinyBERT6 are directly provided by Hugging Face. For BERTBASE, we use the model named bert-base-cased. For DistillBERT, we 2https://github.com/huggingface/ transformers/tree/main/examples/pytorch/ text-classification 3https://github.com/kuangliu/ pytorch-cifar 4https://github.com/rwightman/ GLUE benchmark. For the GLUE benchmark, we utilize the data preparation and pre-processing pipeline implemented by pytorch-image-models 5https://github.com/facebookresearch/deit CUTTLEFISH: Low-rank Model Training without All The Tuning Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is followed by a BatchNorm layer. In the notation used in this table, \u201c7 \u00d7 7, 64\u201d signi\ufb01es that the convolution layer contains 64 convolution kernels, i.e., each kernel has a dimension of 7 \u00d7 7 and the output dimension is 64. Model ResNet-18 ResNet-50 WideResNet-50-2 Conv 1 Layer stack 1 Layer stack 2 Layer stack 3 Layer stack 4 3\u00d73, 64 padding 1 stride 1 - (cid:20) 3\u00d73, 64 3\u00d73, 64 (cid:21) \u00d72 (cid:20) 3\u00d73, 128 3\u00d73, 128 (cid:21) \u00d72 (cid:20) 3\u00d73, 256 3\u00d73, 256 (cid:21) \u00d72 (cid:20) 3\u00d73, 512 3\u00d73, 512 (cid:21) \u00d72 7\u00d77, 64 padding 3 stride 2 7\u00d77, 64 padding 3 stride 2 Max Pool, kernel size 3, stride 2, padding 1 (cid:34) 1\u00d71, 64 3\u00d73, 64 1\u00d71, 256 (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 512 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 1024 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 2048 (cid:35) (cid:35) (cid:34) 1\u00d71, 128 3\u00d73, 128 1\u00d71, 256 (cid:34) 1\u00d71, 256 3\u00d73, 256 1\u00d71, 512 (cid:34) 1\u00d71, 512 3\u00d73, 512 1\u00d71, 1024 (cid:34) 1\u00d71, 1024 3\u00d73, 1024 1\u00d71, 2048 \u00d73 \u00d73 (cid:35) (cid:35) \u00d74 \u00d74 (cid:35) (cid:35) \u00d76 \u00d76 (cid:35) (cid:35) \u00d73 \u00d73 FC Avg Pool, kernel size 4 512 \u00d7 10 Adaptive Avg Pool, output size (1, 1) 2048 \u00d7 1000 2048 \u00d7 1000 employ the model named distilbert-base-cased. For BERTBASE, we use the model named bert-base-cased again. For TinyBERT6, we utilize the model named huawei-noah/TinyBERT General 6L 768D. All model names are --model name or path in Hugging Face. supplied through the API of B.3 Software details For the experiments on CIFAR-10, CIFAR-100, and SVHN, which include CUTTLEFISH and all considered baseline meth- ods, our software setup is built on the NVIDIA NGC Docker container for PyTorch. We use the docker image nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex- periment environment on p3.2xlarge EC2 instances. The CUDA version we used is 11.6. For the BERTBASE \ufb01ne-tuning experiment on the GLUE benchmark, we employ the docker im- age, nvcr.io/nvidia/pytorch:22.01-py3. We install Hugging Face with version 4.17.0.dev0. \ufb01ne-tune the num workers and enable pin memory for all ex- periments to achieve faster end-to-end runtimes. For the DeiT and ResMLP experiments on ImageNet, we enable mixed-precision training using PyTorch AMP. Examples of factorized low-rank layers. As discussed, a full-rank layer W can be factorized to obtain U and V(cid:62). For"}