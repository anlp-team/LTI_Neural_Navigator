{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Grounding_Language_Models_to_Images_for_Multimodal_Generation_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of results are presented in Table 2?", "answer": " Zero-shot results on Visual Dialog for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " What is the key capability of FROMAGe mentioned in the text?", "answer": " Generating free-form text interleaved with image outputs through text-to-image retrieval.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " Who evaluated the story generation in the text?", "answer": " Human annotators.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " What is highlighted as valuable in the text regarding multimodal context?", "answer": " A single image can provide more information than multiple text descriptions.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " What improvement percentage was observed when providing 2 captions + 1 image compared to a single caption case?", "answer": " 30.1% relative improvement.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " How does the model FROMAGe perform compared to CLIP in all settings?", "answer": " FROMAGe outperforms CLIP.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " What does the text mention about the ability of FROMAGe when provided with the full multimodal context?", "answer": " The model is able to learn in-context to synthesize plausible story-like text.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " According to the text, what future directions are suggested for extending FROMAGe?", "answer": " Scaling with larger and more capable language models, training on larger image-text datasets, and generating novel images from scratch.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " What is the conclusion drawn about FROMAGe in the text?", "answer": " FROMAGe is effective in visual grounding pretrained frozen language models and producing coherent interleaved image-text outputs.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}, {"question": " How does the model FROMAGe perform in terms of zero-shot performance?", "answer": " It shows strong zero-shot performance on tasks involving image-text inputs and outputs.", "ref_chunk": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}], "doc_text": "- 25.7 22.0 2.6 3.1 - 14.6 17.6 7.2 8.7 - - 20.1 11.3 15.9 - - 25.1 17.7 20.8 38.9 Incapable Incapable 44.9 50.2 56.0 Table 2. Zero-shot results on Visual Dialog (Das et al., 2017), for image-and-text-to-text (IT2T) and text-to-image (T2I) retrieval. Unlike previous methods, FROMAGe is capable of generating free-form text interleaved with image outputs through text-to-image retrieval. 1 cap0 img 2 caps1 img 3 caps2 imgs 10 VIST Image Retrieval with Increasing Context 5 caps4 imgs 0 4 caps3 imgs 15R@1 5 caps0 img 5 When we saw the lighthouse we knew we were there. We saw emus near the road. ? \u201cthe view from the top of the world\u201d Captions Only 1 5 2 5th Image Only Model Model \u201cthe view from the lighthouse was amazing.\u201d Model \u201cthe water was so clear you could see the bottom.\u201d The cliffs had eroded over time into wonderful arches. You could walk right under them. 3 4 All Images + Captions A road trip to the coast. What would we see? 2 5 8 20R@1 4 VisDial Image Retrieval with Increasing Context 15 10 Ours CLIP 10# Rounds of Dialogue 6 Figure 5. Increasing input context generally improves performance. Shown are results for image retrieval for VIST (Huang et al., 2016) (top) and image retrieval on VisDial (Das et al., 2017) (bottom). Figure 6. More coherent and relevant text is generated when in- context examples are provided to FROMAGe. When multimodal context is provided, the outputs for VIST are more story-like, while the outputs for a single image input are more caption-like. 0.00.10.20.30.40.5More coherent storyMore relevant to image 4 captions 5 images + 4 captionsHuman Preference (Visual Storytelling) 1 image to the single caption case (11.3 to 11.9). However, when we provide an additional image and text example (2 captions + 1 image), we observe an even greater improvement of 30.1% relative to the single caption case (11.3 to 14.7). This high- lights the value of multimodal context: a single image can provide more information than multiple text descriptions. Figure 7. Human evaluations on VIST story generation. Including both images and captions improves story coherence over using just images, and improves image relevance compared to just captions. where correctly parsing long language descriptions is crucial to performance. More context helps. Performance also steadily improves on image retrieval for VIST as more image and caption context is provided (Fig. 5, top). The highest R@1 of 18.2 is achieved with 5 captions and 4 images (i.e., the full story context excluding the image to be retrieved), rep- resenting a 61.1% relative improvement over the single caption case. Similar trends are observed for image retrieval using VisDial (Das et al., 2017) dialogue rounds (Fig. 5, bottom), with performance improving as more rounds of dialogue are provided. Additionally, the results show that FROMAGe outperforms CLIP in all settings, and signifi- cantly outperforms CLIP when the full set of dialogue is provided, achieving an improvement of 17.5% relative over CLIP. These findings suggest that FROMAGe is more sen- sitive to context, enabling it to perform better in situations 5.3. In-context Learning and Text Generation As FROMAGe uses a frozen LLM as its backbone, it is also capable of in-context learning (Brown et al., 2020; Chan et al., 2022), where it generalizes rapidly from a few input examples. We observe this qualitatively from generating new stories for VIST, as shown in Fig. 6. When a single in- put image is provided, the model generally produces simple caption-like descriptions. However, when prompted with the full multimodal context (i.e., 5 images and 4 stories), the model is able to learn in-context to synthesize plausible story-like text for the 5th image (Fig. 6). As evaluating generated text is difficult, especially for sub- jective outputs such as stories, we run human evaluations to study the effect of multimodal context on model generated 8 Grounding Language Models to Images for Multimodal Inputs and Outputs stories. We request human annotators to select the output (from 3 anonymized models) which (1) forms the most co- herent story when viewed in relation with the context, and (2) is most relevant to the image. We sample 100 random examples and collect 5 independent ratings each. The re- sults are aggregated from pairwise comparisons (details in appendix) and summarized in Fig. 7. When only the last im- age or the text description is provided as input, the generated stories are rated as less coherent than FROMAGe with the full multimodal context (5 images and 4 descriptions). The model generated story is also rated as significantly more rel- evant to the image inputs compared to the text-only setting, which highlights the ability of the model to condition on both image and text inputs. We observe that the generated output of the single image input case is rated as more rele- vant compared to the full multimodal context case, which we attribute to the fact that the model produces more factual (albeit less story-like) descriptions (Fig. 6). These results showcase the ability of FROMAGe to learn in-context to synthesize coherent and consistent multimodal outputs. 7. Conclusion We propose a method to visually ground pretrained frozen language models through efficient finetuning of several lin- ear layers. Our model, FROMAGe, is capable of producing coherent interleaved image-text outputs. We show strong zero-shot performance on a variety of tasks involving image- text inputs and outputs, and qualitatively showcase interac- tive abilities such as multimodal dialogue. These results demonstrate the effectiveness of our approach for bootstrap- ping general purpose vision-and-language models, capable of consuming and producing arbitrarily interleaved images and text. Scaling FROMAGe with larger and more capable language models, training on larger image-text datasets, and extending our approach for generation of novel images from scratch are promising directions for future work. Acknowledgements 6. Future Work FROMAGe is one of the first models capable of parsing image-text inputs, and producing text interleaved with re- trieved images. There are several promising directions that are worth exploring in future work. Extending FROMAGe"}