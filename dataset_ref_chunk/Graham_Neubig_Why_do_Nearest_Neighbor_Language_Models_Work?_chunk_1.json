{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Why_do_Nearest_Neighbor_Language_Models_Work?_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What task do language models (LMs) perform?", "answer": " Language models compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " How do most LMs calculate representations of contexts?", "answer": " Most LMs calculate representations of contexts through a neural network consuming the immediate previous context.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " What is the main difference between retrieval-augmented LMs and standard neural LMs?", "answer": " Retrieval-augmented LMs improve over standard neural LMs by accessing information retrieved from a large datastore, in addition to their standard next-word prediction.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " What is a k-nearest neighbor language model (kNN-LM) notable for?", "answer": " A k-nearest neighbor language model extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved based on the distances between the current context embedding of the base LM and all the context embeddings in the datastore.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " Why is it surprising that kNN-LM reduces the perplexity of the base LM?", "answer": " It is surprising because kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " What are the three main reasons identified for why kNN-LM performs better than standard LMs?", "answer": " 1. Using a different input representation for predicting the next tokens. 2. Approximate kNN search. 3. The importance of softmax temperature for the kNN distribution.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " What percentage of the performance gain of kNN-LM is attributed to ensembling the output of softmax using two representations from different layers of the transformer?", "answer": " 55% of the performance gain of kNN-LM is attributed to ensembling the output of softmax using two representations from different layers of the transformer.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " What effect does the lack of preciseness in approximate nearest neighbor search have on kNN-LM?", "answer": " The lack of preciseness actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " Why is adding a temperature term to the kNN non-parametric component crucial to the success of modeling?", "answer": " Adding a temperature term can be crucial to the success of modeling in kNN-LM as it impacts the design decisions chosen for modeling and contributes significantly to the success.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}, {"question": " Where is the code available for the kNNLM-why model?", "answer": " The code is available at https://github.com/frankxu2004/knnlm-why.", "ref_chunk": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}], "doc_text": "3 2 0 2 n a J 7 1 ] L C . s c [ 2 v 8 2 8 2 0 . 1 0 3 2 : v i X r a Why do Nearest Neighbor Language Models Work? Frank F. Xu Uri Alon Graham Neubig Language Technologies Institute Carnegie Mellon University {fangzhex,ualon,gneubig}@cs.cmu.edu Abstract Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and speci\ufb01cally why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why. 1 Introduction Language modeling is the task of predicting the probability of a text (often conditioned on context), with broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018; Baevski and Auli, 2018; Brown et al., 2020). This modeling is usually done by sequentially encoding a context ct using a trained neural network function f , and computing the probability of the next word wt according to f (ct) and a vector representation of wt. Recently, retrieval-augmented LMs have shown a series of impressive results (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020b; Borgeaud et al., 2022; Alon et al., 2022). Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context ct and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM\u2019s prediction. One retrieval-augmented model that is notable for both its simplicity and ef\ufb01cacy is the k-nearest neighbor language model (kNN-LM; Khandelwal et al., 2020b). It extends a trained base LM by linearly interpolating the output word distribution with a kNN model. The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore. The datastore is created by encoding all contexts from any text collection, including the original LM training data. One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is Preprint. Under review. \ud835\udc49 \ud835\udc37\ud835\udc41\ud835\udc51\ud835\udc60+ In \ud835\udc58NN-LM: top-\ud835\udc58() FFN In \ud835\udc58NN-LM:\ud835\udc41\ud835\udc51\ud835\udc60: up to 5000\ud835\udc49 \u210e\ud835\udc51\ud835\udc60 softmax() softmax() \ud835\udc4a\ud835\udc51\ud835\udc60 Layer Norm ATT \ud835\udc43\ud835\udc3f\ud835\udc40parametric component Multi Headed Attention Feed Forward Network \u210e\ud835\udc60\ud835\udc5a mask-to-k() \ud835\udc37 \ud835\udc37 \ud835\udc43\ud835\udc58\ud835\udc41\ud835\udc41non-parametric component \ud835\udc4a\ud835\udc60\ud835\udc5a\ud835\udc37 Figure 1: An illustration of the generalized formulation of kNN-LM in Equation 5. not simply bene\ufb01ting from access to more data. Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM? In this paper, we set out to understand why kNN-LMs work even in this setting. In the following sections, we \ufb01rst elucidate connections between the added kNN component and the standard LM component. Speci\ufb01cally, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words. With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs. We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsi\ufb01cation implementations in the kNN search. This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions. We proceed to propose multiple hypotheses for why kNN-LM works, which are testable by adjusting the various parameters exposed by our generalized formulation. Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of PkN N . As the answer to our question, \u201cwhy kNN-LMs work\u201d, we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0"}