{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Decoder-only_Architecture_for_Speech_Recognition_with_CTC_Prompts_and_Text_Data_Augmentation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the study described in the text?", "answer": " The main focus is on using a decoder-only architecture for speech recognition with CTC prompts and text data augmentation.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " What problem does the study aim to address in the field of automatic speech recognition (ASR)?", "answer": " The study aims to address the challenge of obtaining a large amount of audio-text pair data for ASR by proposing a new training scheme using text-only data.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " How does the proposed model use encoder features compressed by CTC prediction in the decoder-only architecture?", "answer": " The encoder features compressed by CTC prediction are used as prompts for the decoder to refine CTC prediction using the decoder-only model.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " What advantage does the decoder-only architecture offer in terms of leveraging external text data for training?", "answer": " The decoder-only architecture simplifies enhancing the model by leveraging external text data for training, as the decoder architecture is the same as an autoregressive LM.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " How did the proposed model perform in experimental comparisons using LibriSpeech and Switchboard datasets?", "answer": " The proposed model with text augmentation training reduced word error rates by 0.3% and 1.4% on LibriSpeech test-clean and test-other sets, and by 2.9% and 5.0% on Switchboard and CallHome.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " What is the distinction between the proposed decoder-only architecture and conventional encoder-decoder ASR models?", "answer": " The proposed decoder-only architecture outperformed conventional encoder-decoder models with a similar parameter setup and had an advantage in computational efficiency.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " What are the main contributions of the study according to the text?", "answer": " The main contributions include proposing a new training scheme to build a decoder-only model for ASR tasks simultaneously augmented by text-only data and achieving lower word error rates with reduced computational cost.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " How is audio information provided in the decoder-only model for ASR with CTC prompts?", "answer": " Audio information is provided as prompts directly in the embedding space of the decoder.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " What approach is used to efficiently generate prompts for the decoder in the proposed model?", "answer": " The proposed model uses CTC prediction to efficiently generate prompts, known as CTC prompts.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}, {"question": " What is the main difference between the proposed decoder-only architecture and an autoregressive LM?", "answer": " The proposed decoder-only architecture does not have source-target attention layers like an autoregressive LM, making it more suitable for ASR tasks.", "ref_chunk": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}], "doc_text": "4 2 0 2 n a J 9 ] S A . s s e e [ 2 v 6 7 8 8 0 . 9 0 3 2 : v i X r a DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi Siddhant Arora, Shinji Watanabe Sony Group Corporation, Japan Carnegie Mellon University, USA ABSTRACT Collecting audio\u2013text pairs is expensive; however, it is much eas- ier to access text-only data. Unless using shallow fusion, end-to- end automatic speech recognition (ASR) models require architec- ture modi\ufb01cations or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language mod- els (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as re\ufb01ning CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leverag- ing external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed mod- els with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and test- other set, respectively, and 2.9% and 5.0% on Switchboard and Call- Home. The proposed model had advantage on computational ef\ufb01- ciency compared with conventional encoder\u2013decoder ASR models with a similar parameter setup, and outperformed them on the Lib- riSpeech 100h and Switchboard training scenarios. introduced in speech translation (ST) [18] and RNN transducer (RNN-T) decoding [19, 20]. Owing to the strong potential of LLMs, these methods perform well in multi-task scenarios of speech-to-text processing, such as ASR, speech synthesis, and ST. Inspired by this, we adopt a decoder-only architecture for ASR tasks that can be effectively enhanced with external text-only data. This study aims to build a decoder-only architecture for ASR tasks and enhance its performance with text augmentation using external text-only data. We provide audio information as prompts compressed by CTC prediction, which can also be regarded as re- \ufb01ning CTC prediction using a decoder-only model. Although Wu et al. attempted to train a decoder-only from scratch, they obtained a slightly degraded ST model compared to conventional encoder\u2013 decoder models. However, we train the decoder for ASR tasks using not only the audio\u2013text pair but also external text-only data as aug- mentation. Thus, the model is trained from scratch for ASR and LM tasks simultaneously. Experimentally, we con\ufb01rmed that the pro- posed model successfully re\ufb01ned the CTC results while achieving faster inference owing to the compression mechanism. Using Lib- riSpeech 100h subset and Switchboard with text augmentation, the decoder-only model outperformed conventional encoder\u2013decoder models with a similar parameter setup. The main contributions of this study are as follows: Index Terms\u2014 Speech recognition, Decoder-only ASR, CTC, Prompt We propose a new training scheme to build a decoder-only model from scratch for ASR tasks simultaneously augmented by text- only data. 1. INTRODUCTION End-to-end automatic speech recognition (ASR) requires a large amount of audio\u2013text pair data, which is dif\ufb01cult to obtain, whereas a large amount of text-only data can be obtained much more easily. How to exploit unpaired text data is not a trivial problem for ASR tasks. It is common to perform score interpolation with an external language model (LM) trained with text data with an ASR model [1, 2]. Some studies have revealed that estimating the linguistic bias in ASR models, an internal LM, is more suitable for further effective fusion [3\u20135]. To exploit the accessible text-only data directly to the models, some studies modi\ufb01ed the model architecture to accept both audio\u2013text pairs and text-only data as the input for training in a multitask learning manner [6\u20139]. To the best of our knowledge, this is the \ufb01rst study to success- fully outperform conventional encoder\u2013decoder models with the decoder-only model having similar parameter sizes by exploiting external text-only data effectively. We experimentally show that our proposed approach achieves lower word error rates (WERs) by 0.3% and 1.4% for Lib- riSpeech test-clean and test-other, respectively, than the encoder\u2013 decoder model, with approximately half the computational cost, when we used paired data of 100h with text augmentation of 960h transcription data. 2. CTC PROMPTS FOR DECODER-ONLY ASR Recently, such text resources have been effectively used by introducing pretrained large LMs (LLMs) including decoder-only LMs such as GPT-3 [10] and PaLM [11] into the ASR formulation. Studies have successfully adapted decoder-only LMs for speech- processing tasks [12\u201317]. To bridge the audio\u2013text modalities, audio information is injected into the LLMs as a prompt, which are discrete audio units [12\u201314] or continuous representations injected directly into the linguistic embedding space [15, 16]. In the latter approach, encoded audio features are compressed by convolution layers [15, 16] or by CTC predictions [16], which have also been 2.1. Decoder-only architecture We follow the encoder\u2013decoder conformer ASR model [21], except that there are no source\u2013target attention layers in the decoder trans- former. A transformer decoder that does not have source\u2013target at- tention layers is considered an autoregressive decoder-only LM as GPT-3 [10] or PaLM [11]. ASR is a task to predict the most probable I-length token se- quence Y I given a T -length input audio X T , i.e., Y I with the high- est probability p(Y I |X T ). Instead of directly using audio input X T Linguistic domain <eos> Next token prediction Decoder embedding space tokens <aud> <sos> Speech domain CTC CTC prediction Encoder audio signals Fig. 1. Model architecture of decoder-only model for ASR with CTC prompts. in the decoder, it is generally approximated by compact audio rep- resentation. The audio information is provided as \u03c4 -length prompts \u02c6H\u03c4 = {\u02c6ht|1 \u2264 t \u2264 \u03c4 } directly in the embedding space of the de- coder. We propose using CTC prediction to ef\ufb01ciently generate the prompts, namely, CTC prompts, which are"}