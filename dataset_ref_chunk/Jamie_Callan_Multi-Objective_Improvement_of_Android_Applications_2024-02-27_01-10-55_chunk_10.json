{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_Multi-Objective_Improvement_of_Android_Applications_2024-02-27_01-10-55_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the Pareto fronts and where can they be found?", "answer": " The Pareto fronts are found in the online repository of GIDroid (2023).", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " What is the significance of the trade-offs between properties mentioned in the text?", "answer": " The trade-offs between properties must be considered in the search process, showing the natural tension between them.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " Which algorithm performed the best according to the text?", "answer": " SPEA2 performed the best, finding the best fronts in 11 cases.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " What was the effectiveness of the caching operators introduced?", "answer": " The caching operators introduced were highly effective, appearing in 26% of improving patches.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " What is the Vargha and Delaney A measure used for?", "answer": " The Vargha and Delaney A measure is used to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " How did single-objective genetic improvement (SO-GI) perform in terms of improvements?", "answer": " SO-GI found improvements to execution time of up to 33% and memory consumption of up to 72%.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " What are some differences between multi-objective search and single-objective search?", "answer": " Multi-objective search found improvements to both execution time and memory, while single-objective search generally performs better when improving individual properties.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " What is the main finding related to the cost of GI?", "answer": " The median time taken by MO-GI across the benchmarks was 2.6 hours, with a range between 0.1 hours and 20.6 hours.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " What is the significance of the variation in time taken for experiments across different benchmarks?", "answer": " The time taken varied a lot between different benchmarks due to differences in test suite execution time and the number of patches that compiled.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}, {"question": " How does the effectiveness of multi-objective search compare to single-objective search?", "answer": " While multi-objective search found improvements in both execution time and memory, single-objective search produced results that improved one property in 753 of 1260 cases.", "ref_chunk": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}], "doc_text": "though also present raw ones in our online repository, including all Pareto fronts (GIDroid (2023)). Normalised hypervol- ume values are presented in Table 4. The Pareto fronts from all of our multi-objective 20 Fig. 6 Memory consumption improvements (%) achieved by GIDroid using three MO algorithms on 21 versions of 7 Android apps. experiments can be found in our repository (GIDroid (2023)). We find that across our experiment we find patches spread across the Pareto front (see Figure 7), showing that trade-offs between properties must be considered in the search process, due to the natural tension between them. We find that NSGA-II performs similarly to NSGA-III, with the biggest hypervol- ume in 5 cases for both algorithms. We find that SPEA2 performs best, finding the best fronts in 11 cases. In general, the different algorithms seem to perform similarly in terms of the best improvements found, as shown in Figure 5 and Figure 6. We find that the caching operators we introduced turned out to be highly effective, appearing in 26% of improving patches. We also evaluate the effect size of the improvements found by each of the MO algorithms, as show in Table 5. We use the Vargha and Delaney A measure (Vargha and Delaney (2000)) to calculate the magnitude of the differences between the observations of the NFPs of original applications and the improved versions. This measure is non- parametric so does assume data is normally distributed. We find that in all but 8 cases we find large effect sizes, and only find negligible differences in 2 cases. 7.4 RQ4: Comparison to SO-GI Next, we run single-objective genetic improvement on each of our benchmarks. We measure the effects of the changes found by SO-GI on our other properties. The results of this evaluation can be found in Table 6. We found improvements to execution time of up to 33% and memory consumption of up to 72%. 21 Table 5 A effect size for each algorithm on each benchmark. Effect sizes larger than 0.5 show positive improvement. differences: N=negligible, S=small, M=medium, L=large Benchmark PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority 6 PortAuthority Current Exec. Time NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.97 (L) 0.99 (L) 0.81 (L) 0.99 (L) 1.0 (L) 1.0 (L) 0.98 (L) 0.97 (L) 0.99 (L) 0.67 (M) 0.88 (L) 1.0 (L) Mem. Con. SPEA2 NSGA-II NSGA-III 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.97 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.82 (L) 0.18 (L) 1.0 (L) 0.91 (L) 0.71 (M) 1.0 (L) 1.0 (L) 0.67 (M) Tower Collector 1 Tower Collector 2 Tower Collector Current 1.0 (L) 1.0 (L) 0.92 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.89 (L) 1.0 (L) 0.85 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 0.67 (M) Gadgetbridge 1 Gadgetbridge Current 1 0.87 (L) 1.0 (L) 0.96 (L) 1.0 (L) 0.53 (N) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) 1.0 (L) FosdemComp. 1 FosdemComp. Current 1.0 (L) 1.0 (L) 0.95 (L) 0.95 (L) 0.67(M) 0.67(M) 1.0 (L) 1.0 (L) 1.0 (L) 0.83 (L) Fdroid 1 Fdroid 2 Fdroid Current 0.77 (L) 0.99 (L) 0.74 (L) 0.92 (L) 0.93 (L) 1.0 (L) 0.73 (L) 0.92 (L) 0.99 (L) 0.82 (L) 1.0 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) LightningBro. LightningBro. Current 0.79 (L) 0.9 (L) 1.0 (L) 0.83 (L) 1.0 (L) 0.59 (S) 1.0 (L) 1.0 (L) 0.95 (L) 0.9 (L) FrozenBubble 1 FrozenBubble Current 0.98 (L) 1.0 (L) 1.0 (L) 0.93 (L) 0.97 (L) 0.88 (L) 0.98 (L) 1.0 (L) 1.0 (L) 1.0 (L) Fig. 7 Pareto Front from NSGA-II experiments on the FB1 Benchmark. 22 SPEA2 1.0 (L) 1.0 (L) 0.93 (L) 1.0 (L) 0.79 (M) 0.9 (L) 1.0 (L) 0.92 (L) 1.0 (L) 0.98 (L) 0.54 (N) 1.0 (L) 0.83 (L) 1.0 (L) 0.76 (L) 1.0 (L) 0.99 (L) 1.0 (L) 0.92 (L) 0.97 (L) 1.0 (L) Table 6 Maximum improvements to execution time and memory use found by GIDroid using SO-GI (no bandwidth improvements were found). Application Version Exec. Time (%) Mem. Con. (%) PortAuthority 1 PortAuthority 2 PortAuthority 3 PortAuthority 4 PortAuthority 5 PortAuthority6 PortAuthority Current 23.39 21.2 23.13 26.32 28.03 24.44 29.9 71.69 53.05 33.76 60.59 59.13 24.43 9.32 Tower Collector 1 Tower Collector 2 Tower Collector Current 16.01 26.92 20.9 30.82 34.61 32.43 Gadgetbridge 1 Gadgetbridge Current 29.52 26.73 31.29 5.89 FosdemComp. 1 FosdemComp. Current 32.8 10.31 36.81 13.62 Fdroid 1 Fdroid 2 Fdroid Current 21.82 27.94 14.14 17.06 33.01 32.18 LightningBrow. 1 LightningBro. Current 28.45 23.71 8.96 32.43 FrozenBubble 1 FrozenBubble Current 16.67 19.88 36.11 4.09 We find that SO search generally performs better when improving individual prop- erties than multi-objective search. However, a multi-objective search was capable of finding improvements to both execution time and memory in a similar time as a single- objective search could find improvements to individual properties. Single-objective search produces results that improve one property in 753 of 1260 cases (21 bench- marks \u2217 20 runs \u2217 3 properties) but in 47% of these cases, patches are detrimental to another property. 7.5 RQ5: Cost of GI In order to evaluate the applicability of our approach, we analyze its cost. Figure 8 shows a boxplot of the time taken in hours for our experiments. We find that the time taken varies a lot between different benchmarks and in some cases even across different runs on the same benchmark. We find that MO-GI takes between 0.1 hours and 20.6 hours, with a median time across the benchmarks of 2.6 hours. The main source of variation across the benchmarks is the difference in time taken by the test suites. In the slowest benchmark, the test suite takes 8 seconds to execute, whereas the quickest one takes 2 seconds. In the slowest experiments, there were more patches that compiled, rather than instantly failing, further slowing down the experiments. 23 Fig. 8 Time taken by GIDroid using different MO algorithms to"}