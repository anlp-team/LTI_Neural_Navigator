{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Data-efficient_Active_Learning_for_Structured_Prediction_with_Partial_Annotation_and_Self-Training_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the proposed method in the text?", "answer": " Reducing the annotation cost for structured label spaces using active learning", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " How does the approach in the text leverage partial annotation?", "answer": " By selecting only the most informative sub-structures for annotation", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " What is the purpose of utilizing self-training in the proposed method?", "answer": " To incorporate the current model\u2019s automatic predictions as pseudo-labels for unannotated sub-structures", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " What is the key challenge in combining partial annotation with self-training according to the text?", "answer": " Determining which sub-structures to select to label", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " What is the benefit of using an error estimator in the proposed method in the text?", "answer": " To adaptively decide the partial selection ratio according to the current model\u2019s capability", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " What is structured prediction in NLP as described in the text?", "answer": " A fundamental problem where the label space consists of complex structured outputs with groups of interdependent variables", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " How are modern structured predictors developed according to the text?", "answer": " In a data-driven way, by training statistical models with suitable annotated data", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " What is active learning (AL) as mentioned in the text?", "answer": " A method that allows the model to select the most informative data instances to annotate", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " What is the difference between full annotation (FA) and partial annotation (PA) in structured prediction?", "answer": " FA annotates the full output structures while PA selects only a portion of informative sub-structures to annotate", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}, {"question": " What is the purpose of self-training in the context of AL according to the text?", "answer": " To adopt the model\u2019s automatic predictions as extra training signals and complement active learning", "ref_chunk": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}], "doc_text": "3 2 0 2 t c O 9 1 ] L C . s c [ 2 v 4 3 6 2 1 . 5 0 3 2 : v i X r a Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Zhisong Zhang, Emma Strubell, Eduard Hovy Language Technologies Institute, Carnegie Mellon University zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu Abstract In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our ap- proach leverages partial annotation, which re- duces labeling costs for structured outputs by selecting only the most informative sub- structures for annotation. We also utilize self- training to incorporate the current model\u2019s au- tomatic predictions as pseudo-labels for un- annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is deter- mining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selec- tion ratio according to the current model\u2019s capa- bility. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration. 1 Introduction Structured prediction (Smith, 2011) is a fundamen- tal problem in NLP, wherein the label space con- sists of complex structured outputs with groups of interdependent variables. It covers a wide range of NLP tasks, including sequence labeling, syntactic parsing and information extraction (IE). Modern structured predictors are developed in a data-driven way, by training statistical models with suitable annotated data. Recent developments in neural models and especially pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have greatly improved sys- tem performance on these tasks. Nevertheless, the success of these models still relies on the availabil- ity of sufficient manually annotated data, which is often expensive and time-consuming to obtain. \u2714\u2718 He saw the man with a backpack Annotation for Ambigious PartsModel Prediction for Highly-confident Parts Figure 1: Example partial annotations of a dependency tree. Manual annotation is requested only for the uncer- tain sub-structures (red), whereas model predictions can be used to annotate the highly-confident edges (blue). To mitigate such data bottlenecks, active learn- ing (AL), which allows the model to select the most informative data instances to annotate, has been demonstrated to achieve good model accu- racy while requiring fewer labels (Settles, 2009). When applying AL to structured prediction, one natural strategy is to perform full annotation (FA) for the output structures, for example, annotating a full sequence of labels or a full syntax tree. Due to its simplicity, FA has been widely adopted in AL approaches for structured prediction tasks (Hwa, 2004; Settles and Craven, 2008; Shen et al., 2018). Nevertheless, a structured object can usually be de- composed into smaller sub-structures having non- uniform difficulty and informativeness. For exam- ple, as shown in Figure 1, in a dependency tree, edges such as functional relations are relatively easy to learn, requiring fewer manual annotations, while prepositional attachment links may be more informative and thus more worthwhile to annotate. The non-uniform distribution of informative sub- structures naturally suggests AL with partial an- notation (PA), where the annotation budget can be preserved by only choosing a portion of informative sub-structures to annotate rather than laboriously labeling entire sentence structures. This idea has been explored in previous work, covering typical structured prediction tasks such as sequence label- ing (Shen et al., 2004; Marcheggiani and Arti\u00e8res, 2014; Chaudhary et al., 2019; Radmard et al., 2021) Algorithm 1 AL Procedure. Input: Seed dataset L0, dev dataset D, unlabeled pool U, to- tal budget t, batch selection size b, annotation strategy. Output: Final labeled dataset L, trained model M. 1: L \u2190 L0 2: while t > 0 do 3: M \u2190 train(L, U) # Model training 4: 5: 6: 7: 8: 9: 10: 11: M \u2190 train(L, U) # Final model training 12: return L, M # Initialize # Until out of budget S \u2190 sentence-query(M, U) # Sentence selection if strategy == \u201cpartial\u201d then r \u2190 auto-ratio(S, D) # Decide adaptive ratio partial-annotate(S, r) # Partial annotation else full-annotate(S) # Full annotation U \u2190 U \u2212 S; L \u2190 L \u222a S; t \u2190 t \u2212 b and dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Flannery and Mori, 2015; Li et al., 2016). Our work follows this direction and investigates the central question in AL with PA of how to decide which sub-structures to select. Most previous work uses a pre-defined fixed selection criterion, such as a threshold or ra- tio, which may be hard to decide in practice. In this work, we adopt a performance predictor to estimate the error rate of the queried instances and decide the ratio of partial selection accordingly. In this way, our approach can automatically and adaptively adjust the amount of partial selection throughout the AL process. Another interesting question for AL is how In this work, to better leverage unlabeled data. we investigate a simple semi-supervised method, self-training (Yarowsky, 1995), which adopts the model\u2019s automatic predictions on the unlabeled data as extra training signals. Self-training natu- rally complements AL in the typical pool-based setting where we assume access to a pool of unla- beled data (Settles, 2009). It is particularly compat- ible with PA-based AL since the un-selected sub- structures are typically also highly-confident under the current model and likely to be predicted cor- rectly without requiring additional annotation. We revisit this idea from previous work (Tomanek and Hahn, 2009; Majidi and Crane, 2013) and investi- gate its applicability with modern neural models and our adaptive partial selection approach. We perform a comprehensive empirical investi- gation on the effectiveness of different AL strate- gies for typical structured prediction tasks. We perform fair comparisons that account for the hid- den cost of reading time by keeping the context"}