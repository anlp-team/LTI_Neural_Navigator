{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_Conversational_Search_with_Random_Walks_over_Entity_Graphs_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the motivation for combining the passage centrality score and the full-text retrieval ranker score?", "answer": " To retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities.", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " What is the scoring system named that builds on the entity centrality methods and calculates the passage score as the linear interpolation between the BERT query-passage score and centrality score?", "answer": " EClinear", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " What datasets are used for the conversational search evaluation in the TREC CAsT benchmark?", "answer": " MSMarco and TREC CAR Wikipedia datasets", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " What is the goal of conversational search evaluation metrics focused on in the text?", "answer": " Precision at ranks 1 and 3, MRR, and nDCG at 1 and 3", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " What is Entity Linking and how is it used in the context of corpora and queries?", "answer": " Entity Linking is a preprocessing step that can be performed offline for corpora and at runtime for the queries. TagMe with a confidence score of 0.1 is used as the entity linker to maximize entity recall.", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " How are the proposed methods compared against in the text?", "answer": " The proposed methods are compared against classical retrieval models and transformer models.", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " Which model outperforms the traditional rankers across all metrics in the experimental results discussed in the text?", "answer": " BERT", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " What do the neural entity-based approaches fall behind in the experimental results and why?", "answer": " The neural entity-based approaches fall behind BERT due to the additional contextual entity representation diminishing the ranking capabilities of the pre-trained language models.", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " What was found to show gains in combining the Entity Centrality information with the original BERT ranking in the experiments?", "answer": " Using centrality-based approaches with a graph built with the top 20 passages showed gains in combining Entity Centrality with the original BERT ranking.", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}, {"question": " Which experiments showed a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020?", "answer": " Experiments for the 2020 dataset showed a higher dissociation between the presence of query entities in relevant passages compared to the 2019 dataset.", "ref_chunk": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}], "doc_text": "0.7783 0.7750 0.6840 0.7598 0.7713 0.7730 The balance between the passage centrality score, S\ud835\udc58 , and the full- text retrieval ranker, \ud835\udc45\ud835\udc46\ud835\udc58 , score is tuned with the hyperparameter \ud835\udeff. The motivation for combining S\ud835\udc58 and the ranking provided by the full-text retrieval ranker is to retain the full-text retrieval score since it captures complementary relevance signals, including interactions among query and passage terms that do not correspond to entities. We name this scoring system as \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . positions of the matrix with the respective BERT passage score for all entities contained in that passage, before calculating the centrality scores. In both \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 and \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , the score of each passage is the sum of all entity centrality values. \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f builds on \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and calculates the passage score as the linear interpolation between, the BERT query-passage score and centrality score. The \ud835\udc38\ud835\udc36 variations are reported with a 5-fold cross-validation over the CAsT 2019 and 2020 datasets. 5 EXPERIMENTAL METHODOLOGY Datasets: The TREC CAsT [6, 7] benchmark provides evaluation datasets for conversational search. It is composed of the MSMarco [27] and the TREC CAR Wikipedia datasets [26]. The CAsT datasets follow a dialog construction, where the last utterances of a dialog combine information needs that have occurred during the conver- sation. We use the set of manual queries for the 2019 and 2020 editions of the dataset to maximize entity recall. Evaluation Metrics: The goal of conversational search is to an- swer a question with the top passage, thus we focused on Precision at ranks 1 and 3. We also measured results with MRR, and nDCG at 1 and 3 to account for the multi-level relevance judgments. 6 RESULTS AND DISCUSSION This section discusses experimental results and the impact of the system components on the conversational search task. Entity Linking: Entity Linking is a preprocessing step that can be performed offline for corpora, and at runtime for the queries. We opted to use TagMe [11] as the entity linker for its superior F-measure on the CAsT 2019 and 2020 datasets [17]. TagMe used a Wikipedia dump from November 2019 as its knowledge base, and we linked entities with a confidence score of 0.1 to maximize entity recall on both queries and passages. Baselines: We compare the proposed methods with three clas- sical retrieval models and three transformer models. The classical retrieval models are BM25 (\ud835\udc58 = 1.1, \ud835\udc4f = 0.3), LMD (\ud835\udf07 = 1000), RM3 over the previous LMD baseline (5 terms, 15 docs, query weight of 0.8). A BERT reranker is the main baseline, and the starting ranking for entity centrality methods. The BERT reranker was obtained from the LMD run listed in Table 1 and was finetuned [28] on the MS- Marco dataset [27], (sample size=100k steps; learning rate=3 \u00d7 10\u22126; warm-up=10%; ADAM [18] \ud835\udefd1=0.9, \ud835\udefd2=0.999; L2 decay=0.01). We ap- plied the same fine-tuning process to train two other entity-aware transformer models, ERNIE [47] and E-BERT [29]. 6.1 Analysis of Top Retrieved Passages Table 1 shows the retrieval results for all methods. As expected, BERT outperforms the traditional rankers across all metrics. In- terestingly, the neural entity-based approaches fall behind BERT, despite being trained in the same way. These neural entity-based architectures learn a deep contextual representation by fusing en- tity embeddings in the case of ERNIE [47] or transposing entity embeddings to a BERT-compatible embedding space as in the case of E-BERT [29]. However, our experiments show that the additional contextual entity representation diminishes the ranking capabilities of the pre-trained language models. The centrality-based approaches using a graph built with the top 20 passages show the benefit of using the entities of lower-ranked passages to improve the quality of the top positions of the ranking. Our experiments show gains in combining the Entity Centrality (EC) information with the original BERT ranking. Finally, the three experimental systems are \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 , and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f . \ud835\udc38\ud835\udc36\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc5f \ud835\udc66 uses the binary co-occurrence matrix to calculate the entities\u2019 centrality. \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 replaces the non-zero Statistical significance was determined using two-sided paired t-tests, and non-inferiority with one-sided paired t-tests, follow- ing Sakai [34]. The multiple tests were adjusted with the Holm- Bonferroni correction. For both datasets, the \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f Conversational Search with Random Walks over Entity Graphs ICTIR \u201923, July 23, 2023, Taipei, Taiwan Figure 2: nDCG@3, 10, 20, and 40 after reranking the top K passages on CAsT 2019. Graph-* shows the graph size with the entities from the specified number of passages. the 2020 dataset contains a noisier set of entities, which is directly linked to the quality of the contextual entity graphs as discussed in Section 6.2. We found a higher dissociation between the presence of query entities in relevant passages from 2019 to 2020, with 78% of relevant passages containing at least 1 query entity in 2019, and 66% in 2020. Furthermore, for the 2019 edition, only 3.41% of turns do not contain any query entity, when compared to the 7.41% of turns without entities for 2020. These results indicate that the method has fewer connections available to reach relevant passages for the 2020 dataset. Figure 3: Entity Graph size vs. Number of passages methods show improvements over the baseline. For 2019 we can ob- serve that nDCG@3 statistically outperforms BERT with a p-value inferior to 0.05, and a relative improvement of 8.1%. nDCG@1 and P@1 are statistically superior to BERT with a p-value inferior to 0.05, and with relative improvements of 11.3% and 8.2% respectively. For the 2020 dataset, \ud835\udc38\ud835\udc36\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 and \ud835\udc38\ud835\udc36\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5f are statistically equal or superior to BERT with a p-value inferior to 0.05 for metrics nDCG@3 and P@3, with relative improvements of 2.3% and 3.7%. The results show a more modest improvement from 2019 to 2020. Our experiments showed that for the 2020 dataset, on average lower \ud835\udeff provided the best results thus giving less emphasis to the query entities, which means that the best 2020 results were achieved with a lower contribution of the query entities. This behavior is surprising as query entities"}