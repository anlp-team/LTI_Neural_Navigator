{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_DataFinder:_Scientific_Dataset_Recommendation_from_Natural_Language_Descriptions_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of this text?", "answer": " The text focuses on evaluating popular methods for dataset recommendation and comparing them with existing search engines.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " How are communities defined in the text?", "answer": " Communities are defined based on specific publication venues related to NLP, Vision, Robotics, and Machine Learning.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " What are the three ways of defining the similarity function in the text?", "answer": " The three ways are term-based retrieval, nearest-neighbor retrieval, and neural retrieval.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " What is BM25 and what package is used to implement it?", "answer": " BM25 is a standard baseline algorithm for information retrieval. It is implemented using Pyserini.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " Describe the approach used in Nearest-Neighbor Retrieval for dataset recommendation.", "answer": " Nearest-Neighbor Retrieval finds the most similar datasets to a query in a vector space. It uses Term Frequency-Inverse Document Frequency (TF-IDF) and SciBERT for feature extraction.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " What is a bi-encoder retriever and how is it implemented in the text?", "answer": " A bi-encoder retriever encodes each query and document into a shared vector space and estimates similarity via the inner product between their vectors. It is implemented using the Tevatron package with BERT embeddings.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " How does the performance of existing methods compare with the performance of the neural bi-encoder according to the text?", "answer": " The neural bi-encoder performs significantly better than existing methods in dataset recommendation.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " Which third-party dataset search engines are compared in the text?", "answer": " Google Dataset Search and Papers with Code search.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " What is the focus of the evaluation mentioned in the text?", "answer": " The evaluation focuses on comparing methods on full-sentence and keyword search for dataset recommendation.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}, {"question": " How are time filtering and dataset ranking handled according to the text?", "answer": " Time filtering is applied by ranking only the subset of datasets introduced in 2018 or later for queries made from papers published between 2012 and 2020.", "ref_chunk": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}], "doc_text": "on DataFinder How do popular methods perform on our new task and new dataset? How does our new paradigm differ from existing commercial search engines? In this section, we describe a set of standard methods which we benchmark, and we consider which third- party search engines to use for comparison. 7We define \u201ccommunities\u201d by publication venues: ACL, EMNLP, NAACL, TACL, COLING for NLP, CVPR, ICCV, WACV for Vision, IROS, ICRA, IJRR for Robotics, and NeurIPS, ICML ICLR for Machine Learning. We include proceedings from associated workshops in each community. 4.1 Task Framing We formulate dataset recommendation as a rank- ing task. Given a query q and a search corpus of datasets D, rank the datasets d \u2208 D based on a query-dataset similarity function sim(q, d) and re- turn the top k datasets. We compare three ways of defining sim(q, d): term-based retrieval, nearest- neighbor retrieval, and neural retrieval. 4.2 Models to Benchmark To retrieve datasets for a query, we find the nearest datasets to that query in a vector space. We repre- sent each query and dataset in a vector space using three different approaches: Term-Based Retrieval We evaluated a BM25 retriever for this task, since this is a standard base- line algorithm for information retrieval. We imple- ment BM25 (Robertson and Walker, 1999) using Pyserini (Lin et al., 2021).8 Nearest-Neighbor Retrieval To understand the extent to which this task requires generalization to new queries unseen at training time, we experiment with direct k-nearest-neighbor retrieval against the training set. For a new query, we identify the most similar queries in the training set and return the rel- evant datasets from these training set examples. In other words, each dataset is represented by vectors corresponding to all training set queries attached to that dataset. In practice we investigate two types of feature extractors: TF-IDF (Jones, 2004) and SciBERT (Beltagy et al., 2019). Neural Retrieval We implement a bi-encoder re- triever using the Tevatron package.9 In this frame- work, we encode each query and document into a shared vector space and estimate similarity via the inner product between query and document vec- tors. We represent each document with the BERT embedding (Devlin et al., 2019) of its [CLS] token: sim(q, d) = cls(BERT(q))T cls(BERT(d)) where cls(\u00b7) denotes the operation of accessing the [CLS] token representation from the contextual encoding (Gao et al., 2021). For retrieval, we sep- arately encode all queries and documents and re- trieve using efficient similarity search. Following recent work (Karpukhin et al., 2020), we minimize a contrastive loss and select hard negatives using 8We run BM25 with k1 = 0.8 and b = 0.4. 9https://github.com/texttron/tevatron s r e p a P f o % 60% 40% 20% 0% 16.3 12.7 8.8 S Q u A D C O C O S N LI 7.5 SS T 5.9 4.4 G L U E M ultiN LI NLP 4.1 U D 3.3 SIC K 20.2 13.9 Im ageN et C O C O 8.4 6.5 5.2 KIT TI CIF A R-10 U C F101 CV 5 4.1 Celeb A Cityscapes 2.8 M PII s r e p a P f o % 60% 40% 20% 0% 44.9 Robotics 11.8 8.8 7.4 4.4 4 3.3 2.9 KIT TI M uJo C o Im ageN et C O C O Cityscapes Scan N et ShapeN et C A R L A 28.9 18.4 11.5 7.9 C O C O Im ageN et CIF A R-10 Celeb A ML 7.2 2.4 2.3 M uJo C o S Q u A D M ovieLens 2 S N LI Figure 6: We analyze the distribution of datasets used in NLP, robotics, vision, and machine learning research. BM25 for training. We initialize the bi-encoder with SciBERT (Beltagy et al., 2019) and finetune it on our training set. This model takes 20 minutes to finetune on one 11GB Nvidia GPU. Model P@5 R@5 MAP MRR Full-Sentence Queries BM25 4.7 \u00b10.1 11.6 \u00b11.7 8.0 \u00b1 1.3 14.5 \u00b12.0 7.8 \u00b11.1 15.5 \u00b12.0 9.7 \u00b11.2 21.3 \u00b12.3 Bi-Encoder 16.0 \u00b11.1 31.2 \u00b12.2 23.4 \u00b11.9 42.6 \u00b12.7 kNN (TF-IDF) kNN (BERT) 5.5 \u00b10.6 12.3 \u00b11.6 7.1 \u00b10.7 14.2 \u00b11.5 4.3 Comparison with Search Engines Keyphrase Queries Besides benchmarking existing methods, we also compare the methods enabled by our new data rec- ommendation task against the standard paradigm for dataset search \u2014 to use a conventional search engine with short queries (Kacprzak et al., 2019). We measured the performance of third-party dataset search engines taking as input either key- word queries or full-sentence method descriptions. BM25 kNN (TF-IDF) kNN (BERT) 6.6 \u00b10.5 15.3 \u00b11.1 11.4 \u00b10.8 19.9 \u00b11.5 8.2 \u00b11.6 2.7 \u00b10.4 7.3 \u00b11.3 2.8 \u00b10.4 Bi-Encoder 16.5 \u00b11.0 32.4 \u00b12.2 23.3 \u00b11.8 42.3 \u00b12.6 5.9 \u00b11.1 5.8 \u00b11.1 3.3 \u00b10.7 3.3 \u00b11.1 Table 1: A comparison of methods on full-sentence and keyword search shows that the neural bi-encoder per- forms best by a significant margin. Standard deviations are obtained via bootstrap sampling on the test set. We compare on our test set with two third-party systems\u2013 Google Dataset Search10 (Brickley et al., 2019) and Papers with Code11 search. Google Dataset Search supports a large dataset collection, so we limit results to those from Papers with Code to allow comparison with the ground truth. Our test set annotators frequently entered multi- ple keyphrases for each keyphrase type (e.g. \u201cques- tion answering, recognizing textual entailment\u201d for the Task field). We constructed multiple queries by taking the Cartesian product of each set of keyphrases from each field, deduplicating tokens that occurred multiple times in each query. After running each query against a commercial search engine, results were combined using balanced in- terleaving (Joachims, 2002). 5 Evaluation 5.1 Time Filtering The queries in our test set were made from pa- pers published between 2012 and 202012, with me- dian year 2017. In contrast, half the datasets in our search corpus were introduced in 2018 or later. To account for this discrepancy, for each query q, we only rank the subset of datasets D\u2032"}