{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_ML-SUPERB:_Multilingual_Speech_Universal_PERformance_Benchmark_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the benchmark introduced to evaluate the performance of Self-Supervised Learning (SSL) models on speech processing tasks?", "answer": " Speech processing Universal PERformance Benchmark (SUPERB)", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " How many languages are covered by the multilingual SUPERB (ML-SUPERB) benchmark?", "answer": " 143 languages", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " What two tasks are primarily focused on in the ML-SUPERB benchmark?", "answer": " Automatic speech recognition (ASR) and language identification (LID)", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " What is the main difference between ML-SUPERB and XTREME-S benchmarks?", "answer": " ML-SUPERB covers a wider range of languages and focuses on ASR and LID, while XTREME-S covers four different tasks.", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " What do SSL models capture through training on large amounts of unlabeled speech data?", "answer": " Important speech features, such as phonemes and other acoustic units.", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " Why is using a small 10-minute/1-hour training set important in the ML-SUPERB benchmark?", "answer": " Using a smaller training set size presents a more challenging design for the SSL models.", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " What are the two tracks included in the ML-SUPERB benchmark?", "answer": " Monolingual track (monolingual ASR) and multilingual track (multilingual ASR, LID, joint multilingual ASR/LID)", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " What models have shown promising results in capturing important speech features through training on large amounts of unlabeled speech data?", "answer": " Self-supervised learning (SSL) models", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " What is the purpose of ML-SUPERB in the context of existing benchmarks?", "answer": " To be a valuable complement to existing benchmarks by focusing on multilingual speech processing tasks.", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}, {"question": " Who introduced the Speech processing Universal PERformance Benchmark (SUPERB)?", "answer": " Yang et al.", "ref_chunk": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}], "doc_text": "3 2 0 2 g u A 1 1 ] D S . s c [ 2 v 5 1 6 0 1 . 5 0 3 2 : v i X r a ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Jiatong Shi1, Dan Berrebbi1\u2217, William Chen1\u2217, Ho-Lam Chung2\u2217, En-Pei Hu2\u2217, Wei Ping Huang2\u2217, Xuankai Chang1, Shang-Wen Li3, Abdelrahman Mohamed4, Hung-yi Lee2, Shinji Watanabe1 1Carnegie Mellon University 2National Taiwan University 3Meta AI 4Rembrand {jiatongs, dberrebbi, wc4, swatanab}@cs.cmu.edu, shangwenl@meta.com, abdo@rembrand.com hungyilee@ntu.edu.tw Abstract Speech processing Universal PERformance Benchmark (SU- PERB) is a leaderboard to benchmark the performance of Self- Supervised Learning (SSL) models on various speech process- ing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high- resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB bench- mark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research. Index Terms: speech self-supervised learning, multilingual speech recognition, language identification 1. Introduction Self-supervised learning (SSL) has been a popular method in the speech community. SSL models have shown promising re- sults by capturing important speech features, such as phonemes and other acoustic units, through training on large amounts of unlabeled speech data [1]. These models have led to signifi- cant improvements in downstream tasks, such as speech recog- nition, speaker identification, and emotion recognition [2]. Over the past few years, researchers have proposed a variety of SSL models with different training objectives, operating under vari- ous data conditions, model architectures, and modalities [3, 4]. A major challenge in evaluating SSL models for speech is the difficulty of comparison since most models have been eval- uated using different experimental setups. To address this is- sue, Yang et al. introduced the Speech processing Universal PERformance Benchmark (SUPERB) [2]. Recently, an exten- sion of SUPERB called SUPERB-SG [5] has been introduced. SUPERB provides a comprehensive speech SSL benchmark in- cluding tasks such as recognition, detection, semantics, speaker identification, paralinguistics, and generation. With SUPERB, researchers can more easily compare the performance of differ- ent SSL models on various speech-related tasks, universally. While SUPERB covers a wide range of speech tasks, it was designed primarily for English speech. However, there has been growing interest in applying SSL models to multilingual sce- narios, such as training multilingual SSL models [6\u20138] or using SSL models in a cross-lingual manner [9\u201312]. To support fu- ture research in these areas, we propose a new benchmark called multilingual SUPERB (ML-SUPERB). ML-SUPERB is designed to cover a wide range of lan- guages, including both high-resource languages like English and endangered languages such as Totonac. The benchmark pri- marily focuses on evaluating SSL models for automatic speech recognition (ASR) and language identification (LID). To ac- commodate different use cases for SSL models, ML-SUPERB includes two tracks with four different tasks: the monolingual track (monolingual ASR), and the multilingual track (multilin- gual ASR, LID, joint multilingual ASR/LID). Similar to SU- PERB, ML-SUPERB employs frozen SSL models as feature extractors and a lightweight downstream model that can be fine- tuned for different tracks to achieve high training efficiency. Several existing benchmarks also include multilingual SSL models [13\u201315]. Lebenchmark primarily evaluates speech tasks in French [13]; IndicSUPERB focuses mostly on Indian lan- guages [14]. XTREME-S focuses on multilingual speech rep- resentation benchmarks, including ASR, speech translation, speech classification, and speech retrieval [15]. There are three main differences between XTREME-S and ML-SUPERB. Firstly, ML-SUPERB covers a wider range of languages, with 143 languages compared to XTREME-S\u2019s 102. Secondly, ML- SUPERB focuses on ASR and LID, while XTREME-S covers four different tasks. However, ML-SUPERB expands the tasks by evaluating them in four common multilingual research sce- narios, while XTREME-S considers multilingual training only. Finally, ML-SUPERB is designed for efficiency, using smaller benchmark datasets and downstream models, and does not in- clude fine-tuning. This lightweight setup allows us to conduct experiments for a dozen of popular speech SSL models, trained with various sizes and pre-training sets, and compare their per- formances across the proposed tracks. We expect ML-SUPERB would be a valuable complement to existing benchmarks. 2. Benchmark Details 2.1. Data Collection ML-SUPERB gathers data from a wide range of multilingual speech corpora, including Multilingual Librispeech [16], Com- monvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n open-source project [20\u201322], Nordic Language Technology ASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken Wikipedia corpus [26], Mexican endangered languages [10, 27, 28], M-AILab multilingual corpora [29], Living Audio dataset [30], ALFFA corpus [31]. All corpora are with either Creative Commons, MIT, GNU, or Free-BSD licenses, which are avail- able for both industrial and academic research, permissively. \u2217Equal contribution, sorted in alphabetical order. For each language-corpus pair denoted as (lang, data), Table 1: Statistics of the data used for training, development, and testing in ML-SUPERB. Detailed discussed in Sec. 2.1. Dataset Hours Normal Langs (123) Few-shot Langs (20) 10-minute 1-hour Dev Test 37.43 222.46 41.82 44.97 \u223c10min \u00d7 240 (lang, data) \u223c1h \u00d7 240 (lang, data) 5 utt. \u00d7 20 lang 5 utt. \u00d7 20 lang \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) \u223c10min \u00d7 240 (lang, data) \u223c10min \u00d7 31 (lang, data) three 10-minute subsets are randomly extracted for training, de- velopment, and testing, along with an additional 1-hour training set that includes the 10-minute training set.1 The reasons for using a small 10-minute/1-hour training set are: (1) Challeng- ing design: using a large training data size could lead to high performance easily and may result in a saturated benchmark in evaluation metrics [3, 4]. Therefore, using a smaller training set size presents a more challenging design for the SSL mod- els,"}