{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of making the all-in-one recipes and pre-trained model publicly available?", "answer": " To facilitate reproducibility.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " How do the standardized data preprocessing pipeline and open-source recipes enable researchers?", "answer": " They enable researchers to compare results directly, promoting progress in disordered speech processing.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " Why is monitoring antiviral susceptibilities of influenza viruses important?", "answer": " To minimize the public health risk.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " What did the researchers find regarding the antiviral susceptibilities of influenza C and D viruses to RNA polymerase inhibitors?", "answer": " All viruses tested were susceptible to both drugs.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " What did the researchers find when checking for amino acid substitutions associated with baloxavir and favipiravir resistance?", "answer": " None of the viruses tested possessed these substitutions.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " What is the proposed learning objective in the speech enhancement models?", "answer": " To formalize differences in perceptual quality using domain knowledge of acoustic-phonetics.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " Why is data augmentation important in the SIGMORPHON 2023 Shared Task on interlinear glossing?", "answer": " To explore approaches to data augmentation and modeling across low-resource languages.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " What did the researchers find about the token classification models in the SIGMORPHON Shared Task?", "answer": " They were the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " What does the work on speech separation and recognition with self-supervised learning representation focus on?", "answer": " Exploring multi-channel separation methods and the best features to use in the ASR back-end model.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}, {"question": " What is the purpose of Dynamic-SUPERB benchmark in speech processing?", "answer": " To build universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion.", "ref_chunk": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}], "doc_text": "applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']] ['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']] ['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']] ['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Sp\u00e9c Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']] ['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']] ['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning,"}