{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_16.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What role do Language Models (LMs) play in providing a prior probability distribution?", "answer": " LMs provide a prior probability distribution, P(C), for a given sentence C.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " How is the probability distribution P(C) computed based on the chain rule for a sequence of tokens?", "answer": " The probability distribution P(C) is computed as P(C) = \u03a0(i=1 to L+1) P(ci|c0:i\u22121) where ci represents the i-th token of C.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " What is the Markov assumption that the N-gram Language Models (LMs) rely on?", "answer": " N-gram LMs rely on the Markov assumption that the probability distribution of the next token depends only on the previous N-1 tokens.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " What technique is used to avoid data sparsity problems in N-gram models?", "answer": " Smoothing techniques are applied to avoid estimating probabilities based on zero or very small counts for rare token sequences in N-gram models.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " How does the FNN-LM differ from classical N-gram LMs in terms of performance?", "answer": " The FNN-LM outperforms classical N-gram LMs due to its distributed representation of each token and history, leading to better generalization.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " What advantage does the RNN-LM have over other N-gram-based models?", "answer": " The RNN-LM can exploit longer contextual information over N-1 previous tokens using recurrent connections, allowing it to outperform other N-gram-based models in many tasks.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " What is the role of convolutional neural networks in ConvLMs?", "answer": " Convolutional neural networks in ConvLMs replace the recurrent connections used in RNN-LMs with gated temporal convolutions for computing hidden vectors.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " What is the primary purpose of the feed-forward neural network (FNN) LM?", "answer": " The FNN-LM estimates N-gram probabilities using a neural network and provides a better generalization than classical N-gram LMs.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " How does the RNN-LM compute the hidden vector for each token?", "answer": " The RNN-LM computes the hidden vector as a recurrent function of the input token and the previous hidden vector, allowing it to hold longer contextual information.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}, {"question": " What is the main limitation of N-gram Language Models (LMs) despite their simplicity?", "answer": " N-gram LMs suffer from data sparsity problems and poor generalization due to their reliance on discrete representations of tokens and history.", "ref_chunk": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}], "doc_text": "techniques to integrate LMs into E2E models. A. Language Models The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as: P (C) = L+1 (cid:89) P (ci|c0:i\u22121) i=1 where ci denotes the i-th token of C, and c0:i\u22121 represents token sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and cL+1 = \u27e8eos\u27e9. Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i\u22121), i.e., they are modeled to predict the next token given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i\u22121. 1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N \u22121 tokens, i.e., P (ci|c0:i\u22121) \u2248 P (ci|ci\u2212N +1:i\u22121), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci where, K(\u00b7) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N \u2212 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization. The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N \u2212 1 tokens, and predicts the next token as: P (ci|ci\u2212N +1:i\u22121) = softmax(Wohi + bo) hi = tanh(Whei + bh) ei = concat(E(ci\u2212N +1), . . . , E(ci\u22121)) where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(\u00b7) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N \u22121 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than 14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context. 15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation. 3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N \u2212 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi\u22121) ei = E(ci\u22121) where, recurrence(ei, hi\u22121) represents a recursive function, which accepts previous hidden vector hi\u22121 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as recurrence(e, h) = tanh(Whe + Wrh + bh) where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N \u2212 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks. 4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h\u2032 i \u2297 \u03c3(gi) h\u2032 i =ei\u2212k+1:i \u2217 W + b gi =ei\u2212k+1:i \u2217 V + c where \u2297 is element-wise multiplication, \u2217 is a temporal convolution operation, and k is the patch size. \u03c3(gi) represents a gating function of convoluted activation h\u2032 i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution"}