{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Best-Case_Retrieval_Evaluation:_Improving_the_Sensitivity_of_Reciprocal_Rank_with_Lexicographic_Precision_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the trend observed in the number of ties for RL1 as \ud835\udc5a increases?", "answer": " The number of ties for RL1 increases as \ud835\udc5a increases.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " What happens to the number of ties for lexiprecision as \ud835\udc5a increases?", "answer": " The number of ties for lexiprecision shrinks as \ud835\udc5a increases.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " What does the drop in ties for recommender systems benchmarks suggest?", "answer": " It suggests that there are very few relevant items in the rankings, resulting in tied rankings and no relevant items present.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " How do rrLP and sgnLP perform in detecting statistically significant differences compared to reciprocal rank?", "answer": " Both rrLP and sgnLP outperform reciprocal rank in detecting statistically significant differences.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " According to the results, what do the lexiprecision variants capture while increasing the ability to distinguish systems?", "answer": " The lexiprecision variants capture the properties of RL1 while increasing the ability to distinguish systems.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " What is the recommendation regarding the use of RL1 metrics in retrieval settings?", "answer": " It is recommended to assess whether the assumptions behind RL1 metrics are aligned with the use case, considering the uncertainty over recall requirements and psychological relevance.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " What dataset is mentioned as an example where using reciprocal rank might obscure unjudged relevant items?", "answer": " The MSMARCO dataset is mentioned as an example.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " What is the call regarding labeling practices across various domains?", "answer": " The call is to make labeling practices more robust across all domains.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " What is the key contribution highlighted in the conclusion of the study?", "answer": " The study highlights the importance of considering tie-breaking in the evaluation process and provides a method for conducting more reliable best-case retrieval evaluation.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}, {"question": " What is the main finding regarding the use of lexiprecision in addressing the limitations of reciprocal rank?", "answer": " Lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation.", "ref_chunk": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}], "doc_text": "see trends with respect to \ud835\udc5a. Across our three retrieval benchmark sets, we see the growth in number of ties for RL1 as \ud835\udc5a increases; meanwhile, they shrink for lexiprecision. The drop in ties for recommender systems benchmarks suggests that, as described in Section 3.6, rankings contain very few relevant items and, as a result, removing labels will result in no relevant items present and increasingly tied rankings. Fernando Diaz 00.20.40.6 web 00.20.40.6 newsranking recsys 00.20.40.6 deep 0.20.40.60.8label fractionfraction tied 00.20.40.6 Figure 9: Number of ties as labels are removed randomly. Results are averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete information. Dashed red lines: rrLP with incomplete information. Shaded areas: one standard deviation across samples. Number of ties with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. While the number of ties indicates that RL1 might not be able to distinguish systems, for a large enough sample of requests, a met- ric might still be good enough to distinguish systems. A different approach to measuring the discriminative power of an evaluation method is to count the number of differences that are statistically significant [16]. When we compare the percentage of pairs regis- tering a statistically significant difference (Table 4), both rrLP and sgnLP outperform reciprocal rank, often by a very large margin. This indicates that the number of ties indeed hurts the ability of reciprocal rank to detect significant differences, while both variants of lexiprecision are much more sensitive. 6 DISCUSSION Our results demonstrate that our lexiprecision variants capture the properties of RL1 while substantially increasing the ability to dis- tinguish systems under the same best-case evaluation assumptions. Practitioners and evaluators need to assess whether the assump- tions behind RL1 metrics, including reciprocal rank, or lexiprecision or any other evaluation scheme are aligned with the use case. If a retrieval environment supports the assumptions behind RL1 met- rics, including ties, then, by all means, they should be used to assess performance. However, in Section 3.1, we raised several reasons why uncertainty over recall requirements and psychological rel- evance suggest that RL1 metrics make quite strong assumptions not realized in most retrieval settings. We designed lexiprecision Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 4: Percentage of run differences detected at \ud835\udc5d < 0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting. (a) Tukey\u2019s HSD test (b) Paired test with Bonferroni correction rrLP sgnLP RR rrLP sgnLP RR news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 27.42 17.41 28.60 23.85 18.95 14.70 13.39 5.85 20.00 8.25 5.26 6.39 16.52 37.46 24.07 81.43 93.81 92.38 27.34 14.67 31.42 28.28 13.51 10.22 11.61 5.79 11.72 19.20 3.47 11.19 18.47 40.91 24.78 90.95 96.67 96.19 23.55 15.03 27.39 24.11 18.35 13.83 13.21 6.07 18.85 6.97 2.88 4.48 13.21 28.35 20.38 80.00 93.81 90.95 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 26.22 16.22 29.30 23.76 18.55 12.30 11.97 4.75 15.86 11.66 2.33 3.73 15.02 39.04 23.55 90.00 97.14 94.76 27.36 11.35 31.73 25.18 9.27 6.94 10.11 4.64 7.13 16.36 1.79 9.14 17.42 39.45 20.99 92.38 97.62 96.67 21.45 11.53 27.03 23.49 17.74 9.73 11.35 4.32 14.02 5.69 0.60 3.03 10.36 28.00 16.79 90.48 96.67 94.76 to operate as conservatively as possible, preserving any preference from RL1 metrics and only acting to break ties. Although RL1 metrics and lexiprecision agree perfectly when there is only one relevant item, this does not mean that all situations where we have a single judged relevant item should adopt a metric like reciprocal rank. For example, the MSMARCO dataset [14] in- cludes requests and very sparse labels; the majority of requests have one judged relevant item. One might be tempted to use reciprocal rank but Arabzadeh et al. [1] demonstrate that this would obscure the multitude of unjudged relevant items (of which there are many). This hurts efficacy of best-case retrieval evaluation including recip- rocal rank, as shown in Figures 8a and 9. Recommendation tasks have similar issues with sparsity due in part to it being more diffi- cult for a third party to assess the relevance of personalized content and to the difficulty in gathering explicit feedback. Labels derived from behavioral feedback in general suffer from similar sparsity [2]. In this respect, we echo the call from Arabzadeh et al. [1] to make labeling practices across all of these domains much more robust. Given the observation of Voorhees et al. [21] that better labeling can result in less informative evaluation, we need to also develop more sensitive evaluation schemes such as lexiprecision. 7 CONCLUSION Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case re- trieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics\u2014including reciprocal rank\u2014outside of in- formation retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community. REFERENCES [1] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365\u2013385. https://doi.org/10.1007/s10791-022-09411-0 [2] Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018. Learning with Sparse and Biased Feedback for Personal Search. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization, 5219\u20135223. https://doi.org/10.24963/ijcai.2018/725 Finally, this study has introduced a new"}