{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Data-efficient_Active_Learning_for_Structured_Prediction_with_Partial_Annotation_and_Self-Training_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the benefit of self-training in the context of the strategies discussed?", "answer": " Self-training is helpful for all the strategies, boosting performance without the need for extra manual annotations.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " How does the PA strategy compare to the FA strategy in terms of annotation costs?", "answer": " The PA strategy can roughly match the performance of FA with the same amount of reading cost while labeling fewer sub-structures.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " What does the adaptive ratio scheme aim to analyze?", "answer": " The adaptive ratio scheme aims to analyze the effectiveness of the ratio scheme with DPAR as a case study.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " What issue arises when the value of a fixed-ratio scheme is too small?", "answer": " If the value of a fixed-ratio scheme is too small, its performance lags behind others with the same reading contexts due to fewer sub-structures annotated.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " How does the adaptive ratio scheme decide the ratio according to the model performance?", "answer": " The adaptive scheme provides a reasonable solution by automatically deciding the ratio according to the model performance.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " What do the error rates on selected sub-structures indicate?", "answer": " The error rates on selected sub-structures indicate the need for manual corrections.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " What type of data is assumed in the domain-transfer experiments?", "answer": " In the domain-transfer experiments, in addition to unlabeled in-domain data, abundant out-of-domain annotated data is assumed.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " How does the source domain knowledge benefit the AL process in NER tasks?", "answer": " In NER tasks, the source domain knowledge can provide extra information to boost the results, especially in early AL stages.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " What approach is adopted for event extraction and relation extraction in the complex IE tasks?", "answer": " A classical pipelined approach is adopted, splitting the full task into two sub-tasks: mention extraction and relation prediction.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}, {"question": " What do the results indicate about the benefits of combining active learning and transfer learning?", "answer": " The results indicate that the benefits of active learning and transfer learning can be orthogonal, and combining them can lead to promising results.", "ref_chunk": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}], "doc_text": "CoNLL-2003 and 30% for UD-EWT). Moreover, self-training (+ST) is helpful for all the strategies, boosting per- formance without the need for extra manual annota- tions. Finally, with the help of self-training, the PA strategy can roughly match the performance of FA with the same amount of reading cost (according to the left figures) while labeling fewer sub-structures (according to the right figures). This indicates that PA can help to further reduce annotation costs over the strong FA baselines. Ratio Analysis. We further analyze the effec- tiveness of our adaptive ratio scheme with DPAR as the case study. We compare the adaptive scheme to schemes with fixed ratio r, and the results9 are shown in Figure 3. For the fixed-ratio schemes, if the value is too small (such as 0.1), although its improving speed is the fastest at the beginning, its performance lags behind others with the same reading contexts due to fewer sub-structures anno- tated. If the value is too large (such as 0.5), it grows slowly, probably because too many uninformative sub-structures are annotated. The fixed scheme with r = 0.3 seems a good choice; however, it is unclear how to find this sweet spot in realistic AL processes. The adaptive scheme provides a reason- able solution by automatically deciding the ratio according to the model performance. Error and Uncertainty Analysis. We further an- alyze the error rates and uncertainties of the queried sub-structures. We still take DPAR as a case study and Figure 4 shows the results along the AL cycles in PA mode. First, though adopting a simple model, the performance predictor can give reasonable es- timations for the overall error rates. Moreover, by further breaking down the error rates into selected 8The POS tags are assigned by Stanza (Qi et al., 2020). For CoNLL-2003, we filter by PROPN and ADJ, which cover more than 95% of the entity tokens. 9Here, we use self-training (+ST) for all the strategies. \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000$\u0000G\u0000D\u0000S\u0000W\u0000L\u0000Y\u0000H \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0019 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018 \u0000\u001b\u0000\u001b \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000U\u0000 \u0000\u0013\u0000\u0011\u0000\u0016 Figure 3: Comparisons of different strategies to decide the partial ratio. The first three utilize fixed ratio r, while \u201cAdaptive\u201d adopts the dynamic scheme. The grey curve (corresponding to the right y-axis) denotes the actual selection ratios with the adaptive scheme. (S) and non-selected (N) groups, we can see that the selected ones contain many errors, indicating the need for manual corrections. On the other hand, the error rates on the non-selected sub-structures are much lower, verifying the effectiveness of us- ing model-predicted pseudo labels on them in self- training. Finally, the overall margin of the selected sentences keeps increasing towards 1, indicating that there are many non-ambiguous sub-structures even in highly-uncertain sentences. The margins of the selected sub-structures are much lower, sug- gesting that annotating them could provide more informative signals for model training. Domain-transfer Experiments. We further in- vestigate a domain-transfer scenario: in addition to unlabeled in-domain data, we assume abundant out- of-domain annotated data and perform AL on the target domain. We adopt tweet texts as the target domain, using Broad Twitter Corpus (BTC; Der- czynski et al., 2016) for NER and Tweebank (Liu et al., 2018) for DPAR. We assume we have models trained from a richly-annotated source domain and continue performing AL on the target domain. The source domains are the datasets that we utilize in our main experiments: CoNLL03 for NER and UD- EWT for DPAR. We adopt a simple model-transfer \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0016 \u0000\u0016 \u0000 \u0000\u001a \u0000\u001a \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000H\u0000U\u0000U\u0000R\u0000U \u0000\u0013\u0000\u0011\u0000\u0015 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000P\u0000D\u0000U\u0000J\u0000L\u0000Q\u0000 \u00006\u0000 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0014 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00001\u0000 \u0000\u0018 \u0000H\u0000U\u0000U\u0000R\u0000U\u0000 \u00006\u0000 \u0000S\u0000U\u0000H\u0000G \u0000\u0014\u0000\u0016 \u0000\u0014 \u0000\u0014\u0000\u0016 Figure 4: Analyses of error rates and uncertainties (mar- gins) of the DPAR sub-structures in the queried sen- tences along the AL cycles (x-axis). Here, \u2018pred\u2019 de- notes the predicted error rate, \u2018error\u2019 denotes the actual error rate and \u2018margin\u2019 denotes the uncertainty (margin) scores. For the suffixes, \u2018(S)\u2019 indicates partially selected sub-structures, and \u2018(N)\u2019 indicates non-selected ones. \u2018Margin(N)\u2019 is omitted since it is always close to 1. approach by initializing the model from the one trained with the source data and further fine-tuning it with the target data. Since the target data size is small, we reduce the AL batch sizes for BTC and Tweebank to 2000 and 1000 tokens, respec- tively. The results for these experiments are shown in Figure 5. In these experiments, we also include the no-transfer results, adopting the \u201cFA+ST\u201d but without model transfer. For NER, without transfer learning, the results are generally worse, especially in early AL stages, where there is a small amount of annotated data to provide training signals. In these cases, knowledge learned from the source domain can provide extra information to boost the results. For DPAR, we can see even larger benefits of using transfer learning; there are still clear gaps between transfer and no-transfer strategies when the former already reaches the supervised performance. These results indicate that the benefits of AL and transfer learning can be orthogonal, and combining them can lead to promising results. 3.4 Information Extraction We further explore more complex IE tasks that in- volve multiple types of output. Specifically, we investigate event extraction and relation extraction. We adopt a classical pipelined approach,10 which splits the full task into two sub-tasks: the first per- forms mention extraction, while the second exam- ines mention pairs and predicts relations. While 10Please refer to Appendix A for more task-specific details. \u0000\u0014\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001a\u0000\u0019 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000)\u0000$ \u0000\u0015\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u00003\u0000$ \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u0015\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u0017 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0015\u0000)\u0000\u0014\u0000\b \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u001b \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00001\u0000R\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000H\u0000U \u0000\u001a\u0000\u0019 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u0017 \u0000\u001b\u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001a\u0000\u001b \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001a\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0015 \u0000 \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019\u0000/\u0000$\u00006\u0000\b \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013"}