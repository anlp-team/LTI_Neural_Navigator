{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_A_Vector_Quantized_Approach_for_Text_to_Speech_Synthesis_on_Real-World_Spontaneous_Speech_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is one possible example given to explain the lower MCD of VITS compared to MQTTS?,answer: The duration of internal pauses.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " Which synthesis system, MQTTS or VITS, is noted to perform better in terms of intelligibility and speaker transferability?,answer: MQTTS.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " What properties does MQTTS capture better at the utterance level compared to VITS?,answer: Emotion and speaking rate.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " When comparing VITS and MQTTS with a model parameter size of 100M, which system performs better in all metrics except MCD?,answer: MQTTS.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " What phenomenon did the researchers observe for both 100M and 200M versions of MQTTS?,answer: Overfitting, with a higher severity for the 200M version.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " What is considered more prevalent in MQTTS compared to VITS, contributing to a higher Word Error Rate?,answer: Deletion errors.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " Why do deletion errors in MQTTS occur more frequently according to the study?,answer: As intermittent pauses are not annotated explicitly, it encourages MQTTS to produce silence even if attending to a certain phoneme.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " What effect does an increase in \u03c3 value have on the signal-to-noise ratio (SNR) in the synthesis with audio prompts?,answer: The SNR drops as \u03c3 increases.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " How does VITS differ from MQTTS in terms of modeling and sampling from environmental noise?,answer: VITS tries to model and sample from the environmental noise, which is unsuccessful and makes it synthesize only a single type of noise.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}, {"question": " What does the research suggest is crucial for the development of human-level communication for AI?,answer: Bridging the gap between the best-performing syntheses and human speech.", "ref_chunk": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}], "doc_text": "comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We \ufb01nd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed over\ufb01tting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement. Error Analysis. Despite the better average performance of MQTTS in Table 2, we \ufb01nd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually recti\ufb01ed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues. 5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation \u03c3 (then encoded by QE). Figure 5 presents the relationship between \u03c3 and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as \u03c3 increases, con\ufb01rm- ing that the model is guided by the prompt. Using \u03c3 smaller than 10\u22124 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using \u03c3 = 10\u22125. We also noticed that VITS has a lower (a) VITS. (b) MQTTS. Figure 4: Pitch contour for the utterance: \u201cHow much varia- tion is there?\u201d from two models within the same speaker. Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses. SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely dif\ufb01cult. The unsuccessful modeling makes it synthesize only a single type of noise. 6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks. Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback. References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877\u20131901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net."}