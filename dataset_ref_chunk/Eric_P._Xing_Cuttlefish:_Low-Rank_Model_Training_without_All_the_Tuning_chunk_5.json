{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Cuttlefish:_Low-Rank_Model_Training_without_All_the_Tuning_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What heuristic does CUTTLEFISH use to switch from full-rank training to low-rank training?", "answer": " CUTTLEFISH switches from full-rank training to low-rank training when the estimated ranks for all NN layers stabilize.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " How does CUTTLEFISH determine if the curves of the estimated ranks have stabilized?", "answer": " CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch and measures the derivative of the estimated rank sequences for all layers using a rank stabilization threshold.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " What is the role of parameter K in hybrid architectures according to CUTTLEFISH?", "answer": " Parameter K in hybrid architectures balances the final accuracy and model compression rate.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " Why is discerning the relationship between K and final accuracy challenging without fully training the model?", "answer": " It is challenging because it is impractical to achieve faster training speeds without fully training the model to convergence.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " Under what condition does CUTTLEFISH proceed with factorization for low-rank model training?", "answer": " CUTTLEFISH proceeds with factorization if the full-rank time is more than 1.5 times factorized time for a given threshold when pro\ufb01ling rank ratio candidates.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " What is the concept of arithmetic intensity and how does it influence factorization in neural networks?", "answer": " Arithmetic intensity is the ratio of FLOPS to the bytes of data accessed. It affects factorization as layers with low arithmetic intensity may not yield a significant speedup even if FLOPs are reduced.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " How does the arithmetic intensity differ between initial layers and later layers in a convolutional network?", "answer": " Initial layers have small mn but large HW values, resulting in higher arithmetic intensity, while later layers have small H, W values but large mnk2 values.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " Why does factorizing the initial layers in neural networks not result in a significant speedup?", "answer": " Factorizing the initial layers does not yield a significant speedup due to their high arithmetic intensity, which affects the overall performance even if FLOPs are reduced.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " What heuristic does CUTTLEFISH use to switch from full-rank training to low-rank training?", "answer": " CUTTLEFISH switches from full-rank training to low-rank training when the estimated ranks for all NN layers stabilize.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}, {"question": " How does CUTTLEFISH determine if the curves of the estimated ranks have stabilized?", "answer": " CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch and measures the derivative of the estimated rank sequences for all layers using a rank stabilization threshold.", "ref_chunk": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}], "doc_text": "stable rank(\u03a30) , \u2200t \u2208 {1, 2, . . . , T }. 0.05 11 3 9 65 0.20 49 1 1 81 41 13 13 21 17 37 53 45 73 77 Epochs 17 7 9 61 29 5 89 0.10 5 85 25 15 57 33 69 Layer Index 0.15 Figure 3. The rank ratios (\u03c1s) of stable ranks for ResNet-18 trained on CIFAR-10, where darker colors indicate higher \u03c1 values (results for other datasets can be found in the appendix). 3.4 Determining full-rank training epochs E As discussed in Section 1, neither too small nor too large E values result in optimal accuracy. Furthermore, larger E values also lead to slower training time, as full-rank models have higher computational complexity. CUTTLEFISH is in- spired by the observation that the estimated ranks for all NN layers, R, change rapidly during the early training phase but stabilize in later training epochs. A reasonable heuristic, therefore, is to switch from full-rank training to low-rank training when the estimated ranks no longer vary signi\ufb01- cantly. The question now is, how can we determine if the curves of the estimated ranks have stabilized? CUTTLEFISH tracks the sequences of stable ranks for each layer at each epoch, i.e., (cid:37) = {r0, r1, . . . , rt}. CUTTLEFISH measures the derivative of the estimated rank sequences for all layer d(cid:37)l dt d(cid:37)l cantly, using a condition: dt where (cid:15) is a close-to-zero rank stabilization threshold. weights ( ) to detect when they cease to change signi\ufb01- \u2264 (cid:15), \u2200l \u2208 {K+1, . . . , L\u22121}, 3.5 Determining K for hybrid architectures K balances the \ufb01nal accuracy and model compression rate. However, discerning the relationship between K and \ufb01- nal accuracy without fully training the model to conver- gence is challenging and impractical for achieving faster training speeds. For each task, CUTTLEFISH conducts lightweight pro\ufb01ling to measure the runtime of the low- rank NN when factorizing each layer stack, as layers within the same stack have identical weights and input sizes, and assesses whether it results in a signi\ufb01cant speedup. CUTTLEFISH only performs factorization (with pro\ufb01ling rank ratio candidates: \u00af\u03c1) if it leads to meaningful accel- eration (determined by a threshold \u03c5). For example, if CUTTLEFISH: Low-rank Model Training without All The Tuning full-rank time > 1.5 \u00d7 factorized time for \u03c5 = 1.5 when \u00af\u03c1 = 1 4 , then CUTTLEFISH proceeds with factorization. Fig- ure 4 illustrates one example benchmark, where factorizing the \ufb01rst convolution stack (i.e., layer 2 to layer 5) does not yield a substantial speedup. Consequently, CUTTLEFISH does not factorize these layers and returns \u02c6K = 6. 1.1x1.7x1.9x2.6x trained until it reaches full convergence. In our experiments, we set (cid:15) = 0.1 and \u03c5 = 1.5. Algorithm 1 CUTTLEFISH Input: The dataset D, initial full-rank neural network weights W 0 = {W0 L}, the training algorithm A(\u00b7), such as SGD, Adam, etc., the total number of epochs T , and a rank stabilization threshold (cid:15). Output: The trained low-rank factorized NN. Initialize the following: \u02c6E = T ; H = {}; (cid:37)K+1, . . . , (cid:37)L\u22121 = {}, . . . , {}; \u02c6K = Pro\ufb01ling(D, W, \u03c4, \u00af\u03c1) (Algorithm 2), \u03bel = rank(W0 l )/stable rank(\u03a30 for t \u2208 {0, 1, 2, . . . , T \u2212 1} do 1, . . . , W0 l ), \u2200l \u2208 {1, . . . , L}. if t \u2264 \u02c6E then W t+1 \u2190 A(W t, D) for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = stable rank(\u03a3l), (cid:37)l = (cid:37)l \u222a {rl}; l = SVD(Wl); Figure 4. The per-iteration forward time in milliseconds, bench- marked using ResNet-18 on CIFAR-10 with a batch size of 1,024 on an EC2 p3.2xlarge instance. Why does not factorizing the initial layers result in a sig- ni\ufb01cant speedup? The reason for this can be attributed to the concept of arithmetic intensity, which is de\ufb01ned as the ratio of FLOPS to the bytes of data that must be accessed for a speci\ufb01c computation (Jeffers et al., 2016). When a certain layer has low arithmetic intensity, the GPU cannot operate at peak performance, and therefore, even if the FLOPs are substantially reduced, the actual speedup will not be sig- ni\ufb01cant. For a convolution layer, its arithmetic intensity is proportional to O( Bmnk2HW mnk2+BmHW ) where B, H, W stand for the batch size, height, and width of the input image. In con- volution networks, it is generally assumed that the initial lay- ers have small mn but large HW , leading to BmHW (cid:29) mnk2 and O( Bmnk2HW mnk2+BmHW ) \u2192 O(nk2). For later lay- ers, where H, W are small and mnk2 are large, and thus mnk2 (cid:29) BmHW , O( Bmnk2HW mnk2+BmHW ) \u2192 O(BHW ). In the example shown in Figure 4, nk2 = 64 \u00d7 9 = 576 for the \ufb01rst layer stack, while for the last layer stack, BHW = 1024 \u00d7 8 \u00d7 8 = 65, 536 (cid:29) 576. Consequently, the bottom layers exhibit much higher arithmetic intensity, and as a result, factorization leads to signi\ufb01cant speed im- provements. For Transformers, where each layer has identi- cal weight and input sizes (i.e., the same arithmetic inten- sity), we consistently factorize all Transformer layers except for the word/image sequence embedding layers. end if d(cid:37)l dx \u02c6E = t + 1; \u2264 (cid:15), \u2200l \u2208 {K + 1, . . . , L \u2212 1} then else if t = \u02c6E + 1 then for Wl \u2208 W t do if K + 1 \u2264 l < L then \u02dcUl\u03a3l \u02dcV(cid:62) rl = scaled stable rank(\u03a3l, \u03bel); Ul = \u02dcUl\u03a3 \u02dcV(cid:62) l , V(cid:62) l = \u03a3 l ; H = H \u222a {Ul[:, 1 : rl], V(cid:62) l [1 : rl, :]} (with necessary NN weights reshaping); l = SVD(Wl); 1 2 1 2 l else H = H \u222a {Wl}; end end Ht = H; Ht+1"}