{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Speech_collage:_code-switched_audio_generation_by_collaging_monolingual_corpora_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What optimizer is used in the training configuration?", "answer": " Adam optimizer", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " How many blocks does the transformer decoder have?", "answer": " 6 blocks", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " What is the learning rate used in the training?", "answer": " 0.001", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " How many epochs are used for training the long short term memory (LSTM)?", "answer": " 20 epochs", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " What is the beam size used during inference?", "answer": " 10", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " What is the dropout rate in the training configuration?", "answer": " 0.1", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " What is the shared Arabic-English vocabulary size for the Zero-shot scenario?", "answer": " 5000 BPE", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " What is the CTC weight (\u03b1) set to during joint training?", "answer": " 0.3", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " How many attention heads are there in the transformer encoder?", "answer": " 4", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}, {"question": " What type of model is trained with 4 layers and 2048 dimensions in the language model (LM)?", "answer": " long short term memory (LSTM)", "ref_chunk": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}], "doc_text": "and 4 at- tention heads. The transformer decoder has 6 blocks with configura- tions similar to the encoder. We combine 2622 Mandarin characters with 3000 English BPE units for In domain scenario. As for the Zero-shot scenario we use a shared Arabic-English vocabulary of size 5000 BPE. Our training configuration utilizes Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight \u03b1, Eq 2, to 0.3. During inference, we use a beam size of 10 with no length penalty. For the language model (LM), we train a long short term memory (LSTM) with 4 layers, each of 2048 dimensions, over 20 epochs. When integrating LM with E2E-ASR, we apply an LM weight of 0.2. 4. RESULTS AND ANALYSIS 4.1. In-domain CS text We examine the impact of augmenting data with generated CS speech from monolingual data, particularly by integrating in-domain CS text. The results, presented in Table 2, are based on the SEAME evaluation. The results from Mono, obtained by training on mono- lingual Chinese and English data, act as our baseline. A shallow fusion with a SEAME-LM, trained on SEAME text data, results in a marginal relative reduction: up to 2% in MER. However, simple CS augmentation using unigram units yields up to 15.3% relative reductions in MER, compared to Mono. By further enhancing the audio quality of the generated data, we achieve an overall relative improvement of up to 34.4% in MER compared to the Mono. Fi- nally comparing our best results to ASR trained on SEAME, the Table 1. Comparison of the CER/WER/MER results on SEAME. CS: generated CS using in-domain SEAME text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (unigram, bigram) units, SE: signal enhancement from \u00a72, SEAME-ASR: topline model trained on SEAME. Model DevMan DevSge CER-MAN WER-EN MER CER-MAN WER-EN MER Mono + SEAME-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 37.2 36.4 31.5 29.7 27.2 67.4 65.9 55.3 53.7 47.9 32.9 32.2 28.4 27.2 25.4 56.7 55.2 47.5 44.0 39.7 47.5 46.5 42.2 40.9 38.1 SEAME-ASR (topline) 15.1 28.8 16.5 21.7 28.7 38.4 37.6 34.4 33.0 31.4 23.5 Table 2. Comparison of the CER/WER results on ESCWA. CS: data generated using synthetic CS text. Mono: baseline trained on monolingual data, (Unigram, Bigram): generated CS using (uni- gram, bigram) units, SE: signal enhancement from \u00a72 Model MGB-2 TED3 ESCWA CER WER CER WER CER WER Mono + CS-LM + CS-Unigram + CS-Unigram-SE + CS-Bigram-SE 6.1 6.3 6.9 7.0 7.0 12.9 12.5 14.6 14.7 14.7 4.4 4.6 5.2 10.4 10.2 8.5 8.7 10.1 5.4 5.2 31.1 38.0 24.0 23.1 22.5 48.7 57.0 42.7 42.0 40.8 absolute gap is up to 8.9% MER. Given that we utilize SEAME text for data generation, this gap can be attributed to audio mismatches. Thus, we anticipate that further enhancements in audio quality to align with SEAME will bridge this gap. 4.2. Zero-shot CS We investigate the effects of augmenting the dataset with CS speech, generated from monolingual data and synthetic CS text. This syn- thetic CS text is produced from the monolingual Arabic MGB-2 and English Tedlium3 datasets, as described in \u00a72.4. Our evaluations, detailed in Table 2, utilize the ESCWA dataset. Operating under our assumption that we do not have access to real CS data, we use the merged evaluation sets from MGB-2 and Tedlium3 to select the best epochs for the model. The observations align with those from \u00a74.1: the CS-Unigram yields relative reductions of 12.3% in WER and 22.8% in CER. Interestingly, the results from shallow fusion with M ono + CS-LM consistently underperform when compared to M ono. Moreover, enhancing the quality of the generated audio further improves results, leading to an overall relative improvement of 16.2% in WER and 27.6% in CER compared to Mono. It\u2019s note- worthy that, on monolingual data, performance deteriorates with CS augmentation. This suggests model bias towards code-switching and a reduced inclination for monolingual data. We further analyze this observation in \u00a74.4. 45WER/MER 80 escwa dev_sge 0 30 60 40 35 dev_man 25 20 100CS Data Size (%) 40 Fig. 2. WER/MER at different percentages of generated CS data where 0%: represents Monolingual, 100%: represents Monolingual with all generated CS. Table 3. Comparison of the average CMI. Mono: baseline trained on monolingual data, SE: Signal enhancement from \u00a72, Ref: reference, (Uni, Bi): generated CS using (unigram, bigram) units. Dataset Ref Mono CS-Uni CS-Uni-SE Bi-SE ESCWA 15.6 10.4 SEAME 8.7 3.3 10.6 5.4 11.6 6.2 10.5 7.3 4.3. Generated CS data size We explore the impact of the amount of generated CS data size on ASR system performance. Figure 2 illustrates the WER at different In this experiment, we gener- percentages of generated CS data. ated CS data with bigrams at 10%, 50%, and 100%. The 0% rep- resents monolingual condition, while 100% corresponds to 80 hours for Arabic-English and 62.2 hours for Chinese-English. It can be observed that there is a substantial improvement when using 10% of generated CS data. However, as the percentage of generated CS data increases, the rate of improvement decreases. This suggests that with more data, further gains can be expected, albeit at a diminishing rate. 4.4. Analysis To understand the effect of our proposed CS augmentation, we mea- sure the average CMI. Notably, the conventional CMI doesn\u2019t ac- count for the accuracy of the sentence. To address this, we select predictions that closely align with the reference using a WER with heuristic threshold set at \u2264 20%. It can be observed from Table 3, that employing CS data augmentation consistently elevates the CMI. This affirms our assumption that CS augmentation enhances the model\u2019s aptitude for code-switching. 5. CONCLUSION We introduced a framework that generates synthetic code-switched data from monolingual corpora. Our findings demonstrate that inte- grating this CS data augmentation yields substantial improvements that surpass results from training exclusively on monolingual sources or simply combining with a code-switched language model. The en- hancement"}