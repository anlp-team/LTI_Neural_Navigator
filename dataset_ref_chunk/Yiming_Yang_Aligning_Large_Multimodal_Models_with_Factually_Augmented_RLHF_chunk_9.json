{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Aligning_Large_Multimodal_Models_with_Factually_Augmented_RLHF_chunk_9.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the title of the paper by Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan published in 2020?", "answer": " Christiano. Learning to summarize with human feedback.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " What is the title of the personal communication by Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan in 2023?", "answer": " Self-alignment with principle-following reward models.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " Which arXiv preprint was authored by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. in 2023?", "answer": " LLaMA: Open and efficient foundation language models.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " What is one of the sources of hallucination in Supervised Fine-Tuning (SFT) according to the text?", "answer": " GPT-4 synthesized data contains hallucinations.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " What does SFT stand for in the context of hallucination in the text?", "answer": " Supervised Fine-Tuning.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " According to the detailed evaluation results, what attribute has an Overall Hallucination Score of 2.43?", "answer": " Adversarial Comparison.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " What kind of evaluation benchmark does Li et al., 2023d provide in the text?", "answer": " POPE evaluation benchmark.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " Which model achieved 86.1% accuracy in the Shikra InstructBLIP\u2217 7B category according to the results in the text?", "answer": " LLaVA-SFT+ 7B.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " What is the Yes (%) probability for the MiniGPT-4\u2217 7B model in the Popular category based on the results presented?", "answer": " 39.6%.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}, {"question": " What accuracy did the LLaVA-RLHF7B model achieve in the Adversarial category according to the results in the text?", "answer": " 84.0%.", "ref_chunk": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}], "doc_text": "Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Self-alignment with principle-following reward models. personal com- munication, 2023a. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 15 Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014a. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 Preprint A SOURCE OF MULTIMODAL HALLUCINATION MLMM Input + Output Image Caption This image shows the menu of a coffee chop called Roly\u2019s Caf\u00e9. Q: What is the name of the shop?A: Roly\u2019s Caf\u00e9. (LMM can only learn to guess)Supervise Fine-Tuning (SFT)of LMM Agents Human Annotators Source of Hallucination in Behavior Cloning Hallucination can occur even for high-quality vision instruction tuning data when human-labeled vision instruction tuning data does not align with the vision cognition of the MLMM agent itself. Hallucination can occur when synthetic data itself contains hallucinations (e.g., by GPT-4).Clear to Human LabelerVague to LMM Figure 3: Two sources of hallucination in Supervised Fine-Tuning (SFT): GPT-4 synthesized data contains hallucinations; Instruction data labelers have no insights about what LMMs know or see, which essentially teaches them to speculate on uncertain content (i.e. hallucinate). B DETAILED EVALUATION RESULTS ON MMHAL-BENCH We include Table 6 for the full evaluation results on MMHAL-BENCH. Table 6: Detailed evaluation results for different LMMs on MMHAL-BENCH. LLM Overall Hallucination Score \u2191 Rate \u2193 Score in Each Question Type \u2191 Attribute Adversarial Comparison Counting Relation Environment Holistic Other Kosmos-2 IDEFIC9B IDEFIC80B InstructBLIP7B InstructBLIP13B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B LLaVA13BX336 LLaVA-SFT+ LLaVA-RLHF13B 13BX336 1.69 1.89 2.05 2.1 2.14 1.55 1.76 2.05 1.11 2.43 2.53 0.68 0.64 0.61 0.58 0.58 0.76 0.67 0.68 0.84 0.55 0.57 2 1.58 2.33 3.42 2.75 1.33 2.75 2.92 0.67 3.08 3.33 0.25 0.75 1.25 2.08 1.75 0 2.08 1.83 0 1.75 2.67 1.42 2.75 2 1.33 1.25 1.83 1.42 2.42 1.75 2.0 1.75 1.67 1.83 2.5 1.92 2.08 1.17 1.83 1.92 1.58 3.25 2.25 1.67 1.83 1.5 2.17 2.5 2 2.17 2.25 1.5 2.25 2.33 2.67 2.5 3.33 3.67 4.08 2.58 2.17 2.25 1.25 3.83 3.25 2.5 2.17 2.33 1.17 1.5 1.67 1.17 1.75 1.5 1.5 2.25 C DETAILED EVALUATION RESULTS ON POPE We include Table 7 for the full evaluation results on POPE. D AMAZON MECHANICAL TURK DESIGN FOR HUMAN FEEDBACK DATA COLLECTION Data Collection Template The instruction we gave to the crowdworkers is shown in Table 2. Here, we demonstrate the few-shot examples we provided to the crowdworkers. 17 1.33 1.67 1.17 1.08 1.17 1.83 0.5 1.08 0.67 1.75 2.42 Preprint Table 7: POPE evaluation benchmark (Li et al., 2023d). Accuracy denotes the accuracy of predic- tions. \u201cYes\u201d represents the probability of the model outputting a positive answer. Results with \u201c*\u201d are obtained from Li et al., 2023d Model Random Acc\u2191 F1\u2191 Yes (%) Popular Acc\u2191 F1\u2191 Yes (%) Adversarial Acc\u2191 F1\u2191 Yes (%) Overall F1\u2191 Yes (%) Shikra InstructBLIP\u2217 7B MiniGPT-4\u2217 7B mPLUG-Owl\u2217 7B LLaVA\u2217 7B LLaVA7B LLaVA-SFT+ 7B LLaVA-RLHF7B 86.9 88.6 79.7 54.0 50.4 76.3 86.1 84.8 86.2 89.3 80.2 68.4 66.6 80.7 85.5 83.3 43.3 56.6 52.5 95.6 98.8 70.9 44.5 39.6 84.0 79.7 69.7 50.9 49.9 68.4"}