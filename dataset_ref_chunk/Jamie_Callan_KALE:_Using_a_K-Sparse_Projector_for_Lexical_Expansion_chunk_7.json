{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_KALE:_Using_a_K-Sparse_Projector_for_Lexical_Expansion_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What types of approaches are mentioned in the text for enhancing lexical vocabularies?", "answer": " performing lexical expansion, or re-weighting the existing lexical terms", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " What do the experimental results in Table 3 compare?", "answer": " retrieval accuracy and query latency of retrievers alone vs. retrievers augmented with KALE terms", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " What was observed in terms of MRR@10 when using KALE with EfficientSPLADE retriever over MSMARCO?", "answer": " statistically significant boost", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " How does KALE complement existing sparse representations, according to the text?", "answer": " provides accuracy boosts at relatively small efficiency costs", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " What is the purpose of employing a regularization term in KALE?", "answer": " to ensure a balanced index and avoid searching query terms with excessively high posting list sizes", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " What impact did removing the regularization term have on query latency?", "answer": " increase in query latency", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " What does the text suggest about the domain-specificity of the KALE vocabulary?", "answer": " generated terms may not retain performance when used in unrelated domains", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " How did removing equipartitioning from KALE affect effectiveness and query latency?", "answer": " resulted in an effectiveness drop and an increase in query latency", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " What effect did KALE have on the size of the index in disk?", "answer": " increased the size of the index", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}, {"question": " What were the results of combining KALE terms with BM25 in the BEIR benchmark?", "answer": " KALE performed poorly", "ref_chunk": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}], "doc_text": "approaches, e.g., performing lexical ex- pansion, or re-weighting the existing lexical terms. If KALE terms indeed capture different information from that of existing lexical vo- cabularies, one would expect the KALE vocabulary to be compatible with other types of learned sparse representations. Table 3 presents these results. Each block compares the retrieval accuracy and query latency of the retriever alone (i.e., with the learned lexical vocabulary only), and the same retriever augmented with KALE terms. For every retriever, an effectiveness gain is vis- ible, at a small latency cost. Even with EfficientSPLADE, which already relies heavily on re-weighting and expanding with lexical terms, a statistically significant MRR@10 boost was observed over MSMARCO. This reinforces the claim that the generated terms are able to capture concepts beyond the existing English vocabulary. Overall, KALE terms were able to complement already strong learned sparse representations, providing accuracy boosts at rel- atively small efficiency costs. This supports the claim that KALE terms are able to capture semantic information in the corpus that existing sparse representations do not capture as accurately, or do not capture at all. 5.4 Assessing Posting List Size Distribution Natural vocabularies are typically skewed, and KALE, following pre- vious work such as EfficientSPLADE [17], employs a regularization term in the loss function to ensure a balanced index. Enforcing an equal distribution of document frequencies across all the artificial terms, and the consequent balancing of posting list size, is helpful from an efficiency perspective, avoiding the search of query terms with excessively high posting list sizes. The latencies reported in previous experiments did not hint at any serious unbalance in the posting list sizes, which may be the effect of the regularization term. Still, it is relevant to quantify the effect of regularization, and assess how the KALE vocabulary would be distributed without it. The next experiments examined whether the terms generated by KALE could complement more advanced lexical representa- tions. Several sparse retrievers, of increasing retrieval performance, were chosen to be augmented with the KALE generated vocabu- lary. Besides BM25, KALE terms were also tested with DeepCT [6], DocT5Query [28], DeepImpact [24], and EfficientSPLADE [17]. In this subsection, experiments were conducted with a modified version of KALE, and posting list sizes for the generated vocabulary were plotted. Instead of leveraging both the M3SE loss and the equipartitioning loss, a setting was tested where KALE is trained solely with M3SE. After training KALE in this setting, posting list sizes were plotted, both with and without regularization. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 3: Experimental results when complementing other sparse retrievers with the KALE vocabulary. KALE terms were added with TF=1, with the exception of impact indexes. The lexical terms were either untouched (i.e., BM25), or reweighted/expanded by the other retrievers. QL denotes MSMARCO query latency, measured in milliseconds. The symbol \u2020 denotes statistically significant improvements over the base retrievers, for a paired t-test with a p-value of 0.05 . MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 BM25+KALE DeepCT DeepCT+KALE DocT5Query DocT5Query+KALE DeepImpact DeepImpact+KALE EfficientSPLADE EfficientSPLADE+KALE MRR@10 0.184 0.319\u2020 0.245 0.326\u2020 0.274 0.323\u2020 0.326 0.359\u2020 0.386 0.389\u2020 Recall@10 0.379 0.564\u2020 0.481 0.590\u2020 0.539 0.574\u2020 0.582 0.625\u2020 0.671 0.667 NDCG@10 0.506 0.646\u2020 0.576 0.681\u2020 0.629 0.658 0.662 0.704\u2020 0.715 0.720 Recall@10 0.129 0.142 0.156 0.166 0.159 0.145 0.152 0.160 0.168 0.168 NDCG@10 0.480 0.639\u2020 0.550 0.650\u2020 0.611 0.641 0.602 0.667\u2020 0.718 0.713 Recall@10 0.164 0.210\u2020 0.178 0.219 0.218 0.211 0.198 0.222 0.242 0.239 QL 17 26 17 30 21 26 55 84 43 89 Table 4: Ablation tests with the equipartitioning component. Index size is the disk size of the inverted index, measured in GB. QL denotes query latency, and is measured in ms/query. Method BM25 KALE KALE w/o Equipartitioning MRR@10 0.184 0.319 0.315 Index Size QL 18 26 31 0.72 3.80 3.80 The left violin plot from Figure 4 illustrates the distribution of the term Document Frequency (DF) for the 98,304 KALE terms, while the right plot presents the same data, without equipartitioning. Ide- ally, all terms should have the DF obtained by dividing the number of documents in the collection with the KALE vocabulary size of 98,304. The average DF for both distributions matched this ideal DF. Removing equipartitioning increased the standard deviation of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning com- ponent was useful in balancing the posting list sizes of the gener- ated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency. 5.5 Experiments with Out-of-Domain Data This paper claims that the KALE vocabulary captures important concepts in its training corpus. This makes it seem unreasonable to expect the generated terms to retain performance when porting to unrelated domains, since the vocabulary is likely domain-specific. For example, a vocabulary term related to gardening may indeed be useful in MSMARCOv1, but still useless in a physics dataset. Figure 4: Document frequencies for KALE terms when en- abling or disabling the equipartitioning component. Table 4 compares KALE with and without equipartitioning, in terms of effectiveness, query latency, and index size. The results show a statistically significant effect (paired t-test with a p-value of 0.05) when removing equipartitioning. As expected, KALE terms increased the size of the index in disk, and removing equiparti- tioning did not influence index size (i.e., equipartitioning changes the size distribution within the same 98,304 posting lists, which combine to the same total size). Taking regularization out resulted in an effectiveness drop, together with an increase in query latency. This indicates that the regularization term was useful in enforc- ing a fairly balanced distribution of posting list sizes, which also contributed to an accuracy increase. Table 5 displays the results of combining KALE terms with BM25, as well as using KALE in isolation, over the BEIR benchmark. KALE performed poorly when"}