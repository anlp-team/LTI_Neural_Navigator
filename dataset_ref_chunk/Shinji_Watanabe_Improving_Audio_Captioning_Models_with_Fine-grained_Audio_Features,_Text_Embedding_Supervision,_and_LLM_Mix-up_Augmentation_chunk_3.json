{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Audio_Captioning_Models_with_Fine-grained_Audio_Features,_Text_Embedding_Supervision,_and_LLM_Mix-up_Augmentation_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the multitask loss L used to train the model?", "answer": " L = LNLL + \u03b1LInfoNCE", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " What does \u03b1 represent in the multitask loss equation?", "answer": " \u03b1 is a hyperparameter with a value of 1 that works well.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " How does ChatGPT Mix-up Augmentation create more complex and diverse training data?", "answer": " It mixes up pairs of captions with different corresponding audio clips in the Clotho dataset.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " What is the purpose of the encoder reranking process?", "answer": " To compute the cosine similarity between the text and audio embeddings for candidate caption ranking.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " What is the impact of including 50K ChatGPT mix-ups according to the experiments?", "answer": " It yields better performance compared to 100K mix-ups.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " Why is there a tradeoff between downsampling rate and batch size in the model?", "answer": " Using less downsampling gives finer-grained audio features but a smaller batch size leads to less reliable gradients.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " How is the right sample picked with nucleus sampling in the model?", "answer": " Using a hybrid reranking algorithm that considers both learned audio encoder stack and text decoder.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " What is the purpose of tuning the weights of reranking metrics on the Clotho validation split?", "answer": " To achieve a balance between decoder and encoder reranking metrics.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " What is the purpose of the FENSE evaluator in the model?", "answer": " To filter out generations with fluency issues during evaluation.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}, {"question": " What is the effect of a higher temperature in the contrastive objective?", "answer": " It makes the distribution less peaky and more challenging.", "ref_chunk": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}], "doc_text": "index samples in the mini-batch. The multitask loss L used to train our model can hence be written as: L = LNLL + \u03b1LInfoNCE , where \u03b1 is a hyperparameter and we find \u03b1 = 1 works well. 2.3. ChatGPT Mix-up Augmentation As a novel data augmentation measure, we employ another LLM, ChatGPT [18], to \u2018mix-up\u2019 [23, 31] pairs of captions in the Clotho dataset, and create more complex and diverse in-domain training data. Specifically, we mix-up captions with different correspond- ing audio clips, rather than two ground-truth captions for the same audio. The corresponding audio waveforms are also mixed up to ensure consistency between audio and mixed-up captions. (6) the log-likelihood is computed on decoder outputs, we call this decoder reranking. Audio-caption representation similarity: We feed the gener- ated caption \u02c6y into the INSTRUCTOR model to get its text em- bedding \u02c6c, and fetch the audio embedding a of the input wave- form x from our trained audio encoder stack. Then, we compute the cosine similarity between the text and audio embeddings, i.e., (a\u22a4\u02c6c) / (||a|| ||\u02c6c||) (cf. Eq. (2)). As the representation from the audio encoder is used here, we refer to this as encoder reranking. Candidate captions are ranked by the weighted sum of the two met- rics above (with weights tuned on some held-out validation data), and we return the highest-scoring one as the final predicted caption. Our inference process is about three times slower than simple beam search with a beam size of 4. 3. EXPERIMENTS AND RESULTS We collect such mix-up augmentations using the public Chat- GPT API. In the prompt, we ask ChatGPT to \u201cGenerate a mix of the following two audio captions, and keep the generation under 25 words:\u201d, and then provide it with two randomly sampled captions from Clotho [2]. We explicitly limit the number of words to force ChatGPT to be more concise. We use the FENSE disfluency detec- tor [32] to filter out poor examples.7 Mix-up of audio waveforms is more straightforward: we follow the algorithm used in WavLM [33], scaling the two waveforms to ensure their relative root-mean-square energy is within \u00b15 dB before adding them together. 3.1. Training and Inference We first pretrain the model on the combined dataset of AudioCaps8 [3] and 50K ChatGPT mix-ups of samples from the better-curated but smaller Clotho [2] dataset for 10 epochs (about 13K gradient steps), and then finetune it on Clotho (development split, \u223c4K sam- ples) for 40 epochs (or 1.2K steps). Teacher-forcing is applied on the BART decoder inputs. We adopt the AdamW optimizer with a 2 \u00d7 10\u22124 learning rate for the \u2018AudioCaps + ChatGPT mix-up\u2019 pre- training stage, and 2 \u00d7 10\u22125 for the Clotho finetuning stage. Table 1 displays a few examples of ChatGPT-generated mix-ups. We try including either 50K or 100K ChatGPT mix-ups, and using 50K yields a better performance. The API cost for generating 50K mix-ups is roughly $8.50. 2.4. Sampling and Reranking In past AAC research works, the most commonly used decoding al- gorithm has been beam search [4\u20136]. However, we find that, after in- troducing all the techniques in Section 2.1\u223c2.3, around 1/3 of gener- ations using nucleus sampling [26], which is known to produce more diverse and informative generations than beam search, score higher in terms of SPIDEr-FL than those using beam search. This reveals the potential advantage of a sampling-then-reranking approach. As the Conformer attention (see Section 2.1) is the primary memory bottleneck due to the long sequence length of audio fea- tures, there is a tradeoff between the batch size that can be used and the downsampling rate for the Conv1D layer between our BEATs and Conformer modules\u2014using less downsampling gives the model finer-grained audio features, but a smaller batch size would lead to less reliable gradients and hamper contrastive learning [35]. Through experiments, we settle on the 3x downsampling rate, which allows a batch size of 32 and achieves the best performance. We train the model on two NVIDIA A100 (40GB) GPUs, and the two training stages take around 6 and 3 hours respectively. Next- token prediction accuracy on the Clotho validation split is used as the checkpoint selection criterion. To \u2018pick the right sample\u2019 with nucleus sampling, we propose a hybrid reranking algorithm that utilizes again the knowledge of both our learned audio encoder stack and our text decoder. The two reranking metrics we consider are: \u2022 Caption log-likelihood: We feed the input waveform x and the generated caption \u02c6y into our captioning model to directly com- pute log p(\u02c6y | x) = (cid:80)| \u02c6y| n=1 log p(\u02c6yn | \u02c6y1:n\u22121; x) (cf. Eq. (1)). As At inference time, we experiment with generating {20, 50, 100} candidate captions per test case with nucleus sampling,9 and find that generating 50 strikes the best balance between performance gain and compute efficiency. Additionally, we leverage the FENSE eval- uator [32] to filter out generations with fluency issues. We tune the weights of the reranking metrics (see Section 2.4) on the Clotho val- idation split and eventually pick {0.3, 0.7} respectively for decoder and encoder reranking metrics.10 6Generally speaking, a higher temperature makes the contrastive objec- tive more challenging, as the distribution is made less peaky. We perform a search in \u03c4 = {0.03, 0.07, 0.2, 0.5, 1.0} and find \u03c4 = 0.5 works the best. 8We filter out captions with <6 words, leading to 35K remaining samples. 9Nucleus sampling hyperparameters: temperature 0.5; cumulative distri- bution truncation point, i.e., top-p, 0.95. 7Less than 1% of ChatGPT mix-ups are detected as disfluent. 10Weights for ablated models (see Table 2) are separately tuned to be fair. Table 2: Ablation study on Clotho [2] evaluation split. The results demonstrate that all components in our AAC model, i.e., BEATs [13] audio encoder (Section 2.1), INSTRUCTOR [17] sentence embedding supervision (Section 2.2), ChatGPT [18] mix-up augmentation (Section 2.3), and nucleus sampling [26] + reranking (Section 2.4), are beneficial to the performance. We highlight the ablated components in brown. Model components Performance metrics (in %) Audio encoder"}