{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Extracting_Training_Data_from_Diffusion_Models_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the experiments described in the text?,answer: Memorization in large diffusion models", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " What type of attacks are performed in the experiments described in the text?,answer: Membership inference attacks, attribute inference attacks, and data reconstruction attacks", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " How many diffusion models were trained on CIFAR-10 for the experiments?,answer: 16 diffusion models", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " What is the purpose of untargeted extraction in the experiments?,answer: Validating that memorization still occurs in smaller models", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " How are extracted images identified in the experiments?,answer: By computing the (cid:96)2 distance to the nearest neighbor in the training set", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " What was the total number of candidate images generated in the experiments?,answer: 220 candidate images", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " What is the threshold used to identify memorized training examples in the experiments?,answer: The (cid:96)2 distance to its nearest neighbor in the training set being abnormally low compared to all other training images", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " How many unique extracted images were identified from the CIFAR-10 dataset in the experiments?,answer: 1,280 unique extracted images", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " What was the set containing the n closest elements from the training dataset to the example used in the attack in the experiments?,answer: S \u02c6x", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}, {"question": " What technique is used in the experiments to evaluate membership inference attacks?,answer: Traditional attack techniques that use white-box access", "ref_chunk": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}], "doc_text": "also result in higher memorization. 5 Investigating Memorization The above experiments are visually striking and clearly indicate that memorization is pervasive in large diffusion models\u2014and that data extraction is feasible. But these experiments do not explain why and how these models memorize training data. In this section we train smaller diffusion models and perform controlled experiments in order to more clearly understand memorization. Experimental setup. For the remainder of this sec- tion, we focus on diffusion models trained on CIFAR-10. We use state-of-the-art training code 5 to train 16 diffu- sion models, each on a randomly-partitioned half of the CIFAR-10 training dataset. We run three types of pri- vacy attacks: membership inference attacks, attribute in- 5We either directly use OpenAI\u2019s Improved Diffusion repos- itory (https://github.com/openai/improved-diffusion) in Section 5.1, or our own re-implementation in all following sections. Models trained with our re-implementation achieve almost identical FID to the open-sourced models. We use half the dataset as is stan- dard in privacy analyses [8]. Figure 6: Direct 2-norm measurement fails to identify memorized CIFAR-10 examples. Each of the above im- ages have a (cid:96)2 distance of less than 0.05, yet only one (the car) is actually a memorized training example. ference attacks, and data reconstruction attacks. For the membership inference attacks, we train class-conditional models that reach an FID below 3.5 (see Figure 11), plac- ing them in the top-30 generative models on CIFAR-10 [16]. For reconstruction attacks (Section 5.1) and at- tribute inference attacks with inpainting (Section 5.3), we train unconditional models with an FID below 4. 5.1 Untargeted Extraction Before devling deeper into understanding memorization, we begin by validating that memorization does still occur in our smaller models. Because these models are not text conditioned, we focus on untargeted extraction. Specif- ically, given our 16 diffusion models trained on CIFAR- 10, we unconditionally generate 216 images from each model for a total of 220 candidate images. Because we will later develop high-precision membership inference attacks, in this section we directly search for memorized training examples among all our million generated exam- ples. Thus this is not an attack per se, but rather verifying the capability of these models to memorize. Identifying matches. In the prior section, we performed targeted attacks and could therefore check for successful memorization by simply computing the (cid:96)2 distance be- tween the target image and the generated image. Here, as we perform an all-pairs comparison, we \ufb01nd that us- ing an uncalibrated (cid:96)2 threshold fails to accurately iden- tify memorized training examples. For example, if we set a highly-restrictive threshold of 0.05, then nearly all \u201cex- tracted\u201d images are of entirely blue skies or green land- scapes (see Figure 6). We explored several other met- rics (including perceptual distances like SSIM or CLIP embedding distance) but found that none could reliably identify memorized training images for CIFAR-10. We instead de\ufb01ne an image as extracted if the (cid:96)2 dis- tance to its nearest neighbor in the training set is abnor- mally low compared to all other training images. Fig- ure 7 illustrates this by computing the (cid:96)2 distance be- tween two different generated images and every image in the CIFAR-10 training dataset. The left \ufb01gure shows a failed extraction attempt; despite the fact that the nearest 8 Frequency 102 0.00 0.75 1.00 0.50 0.50 101 0.75 100 0.25 0.25 0.00 1.00 L2 distance between generated and training images 103 Figure 7: Per-image (cid:96)2 thresholds are necessary to sep- arate memorized images from novel generations on a CIFAR-10 model. Each plot shows the distribution of (cid:96)2 distances from a generated image to all training images (along with the image and the nearest training image). Left shows a typical distribution for a non-memorized image. Right shows a memorized image distribution; while the most similar training image has high absolute (cid:96)2 distance, it is abnormally low for this distribution. The dashed black line shows our adaptive (cid:96)2 threshold. training image has an (cid:96)2 distance of just 0.06, this dis- tance is on par with the distance to many other training images (i.e., all images that contain a blue sky). In con- trast, the right plot shows a successful extraction attack. Here, even though the (cid:96)2 distance to the nearest train- ing image is higher than for the prior failed attack (0.07), this value is unusually small compared to other training images which almost all are at a distance above 0.2. We thus slightly modify our attack to use the distance (cid:96)( \u02c6x, x; S \u02c6x) = (cid:96)2( \u02c6x, x) \u03b1 \u00b7 Ey\u2208S \u02c6x [(cid:96)2( \u02c6x, y)] . where S \u02c6x is the set containing the n closest elements from the training dataset to the example \u02c6x. This distance is small if the extracted image x is much closer to the train- ing image \u02c6x compared to the n closest neighbors of \u02c6x in the training set. We run our attack with \u03b1 = 0.5 and n = 50. Our attack was not sensitive to these choices. Results. Using the above methodology we iden- tify 1,280 unique extracted images from the CIFAR-10 dataset (2.5% of the entire dataset).6 In Figure 8 we show a selection of training examples that we extract and full results are shown in Figure 17 in the Appendix. 6Some CIFAR-10 training images are generated multiple times. In these cases, we only count the \ufb01rst generation as a successful attack. Further, because the CIFAR-10 training dataset contains many dupli- cate images, we do not count two generations of two different (but du- plicated) images in the training dataset. Figure 8: Selected training examples that we extract from a diffusion model trained on CIFAR-10 by sampling from the model 1 million times. Top row: generated output from a diffusion model. Bottom row: nearest ((cid:96)2) example from the training dataset. Figure 17 in the Appendix contains all 1,280 unique extracted images. 5.2 Membership Inference Attacks We now evaluate membership inference with more tra- ditional attack techniques that use white-box access, as opposed to Section 4.2.1 that assumed black-box access. We"}