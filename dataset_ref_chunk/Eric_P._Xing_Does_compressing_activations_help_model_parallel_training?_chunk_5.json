{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Does_compressing_activations_help_model_parallel_training?_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the limitations of Top-K, Random-K, and quantization in terms of encoding/decoding overheads?,        answer: Top-K, Random-K, and quantization have large encoding/decoding overheads and cannot provide end-to-end throughput improvements.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " How does using AE impact the pre-training process?,        answer: Using AE slightly increases the time taken by the backward step, but leads to a \u223c2\u00d7 reduction in communication time and better overall throughput.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " What experimental environment is used for pre-training in the text?,        answer: For pre-training, 4 p3.8xlarge instances on Amazon EC2 are used, each instance has 4 GPUs with NVLink.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " How much speedup is achieved in pre-training by using Top-K and AE, based on Table 6?,        answer: Using Top-K speeds up pre-training by 7%, and using AE speeds it up by 16%.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " Why is the setting TP=4, PP=4 considered the best for pre-training?,        answer: TP=4, PP=4 is considered the best setting for pre-training because the communication cost of tensor parallelism is larger than that of pipeline parallelism, and with TP=4, tensor parallel communication happens over faster NVLinks.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " How can compressing activation for models improve throughput for pre-training?,        answer: Compressing activation for models can improve throughput for pre-training by 16%.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " What is the takeaway regarding compressing activation over pre-training?,        answer: Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " What is observed in Table 9 regarding the use of compression A2 over the last 12 layers?,        answer: Table 9 shows that using A2 to compress the activation over the last 12 layers effectively reduces the communication cost between two pipeline stages.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " What is the observation from Table 8 regarding the use of Top-K for compression?,        answer: Table 8 shows that pre-trained models suffer significant accuracy loss when using Top-K for compression.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}, {"question": " What is the benefit of using quantization as a strategy to compress activation?,        answer: Quantization can preserve the model\u2019s accuracy, but cannot achieve end-to-end speedup as a compression strategy.    ", "ref_chunk": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}], "doc_text": "Top-K, Random-K and quantization have large encoding/decoding overheads and thus cannot provide end-to-end throughput improvements. Although AE slightly increases the time taken by the backward step, the \u223c 2\u00d7 reduction in commu- nication time and the limited encoding/decoding overhead lead to better overall throughput. First, we recap the experimental environment here. For pre- training, we use 4 p3.8xlarge instances on Amazon EC2 and each instance has 4 GPUs with NVLink. From Table 6, we can see that using Top-K and AE can speed up pre- training by 7% and 16% respectively. Among the three distributed settings, TP=4, PP=4 is the best setting for pre-training. That is because the communication cost of tensor parallelism is larger than that of pipeline parallelism and with TP=4, tensor parallel communication happens over faster NVLinks. Takeaway 4 Compressing activation for models can im- prove throughput for pre-training by 16%. From Table 7, we notice that using AE and Top-K can reduce the waiting time and pipeline communication time of pre-training. This is because the inter-node bandwidth (10Gbps) is smaller than the intra-node bandwidth (40GB/s Does compressing activations help model parallel training? Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 276.34 354.16 5.80 9.83 646.14 \\ \\ 150.72 A1 213.83 362.61 6.16 4.06 586.65 2.16 3.12 80.88 A2 219.01 366.51 5.67 4.07 595.25 3.12 4.56 84.48 T1 298.93 355.71 6.79 4.38 665.81 70.08 13.68 85.20 T2 305.47 355.51 6.36 3.91 671.24 70.32 16.80 87.84 T3 331.70 356.80 5.78 5.00 699.27 72.24 27.36 100.80 T4 376.72 359.19 5.89 6.60 748.41 74.88 45.36 124.56 R1 2,408.68 357.02 6.10 7.68 2,779.49 2,040.24 15.84 104.16 R2 R3 R4 4,696.99 12,603.79 46,968.21 356.33 362.13 365.36 6.28 6.81 7.61 6.20 25.28 22.81 5,065.80 12,998.01 47,363.98 4,244.64 11,499.12 44,038.56 19.44 29.76 47.52 135.84 139.92 567.36 Q1 274.03 354.56 5.88 7.98 642.46 20.64 32.16 91.68 Q2 282.64 354.55 5.58 7.58 650.36 19.92 30.24 104.64 Table 4. We breakdown the average iteration time (ms) for \ufb01ne-tuning with various compression techniques when using TP=2 and PP=2, batch size 32, and sequence length 512. The results are collected from the local machine without NVLink. The total time (ms) is divided into following parts: forward step, backward step, optimizer, and waiting & pipeline communication. The last three columns further breakdown the tensor encoder/decoder and communication times which are considered part of the forward step. Compression Algorithm MNLI-(m/mm) QQP SST-2 MRPC CoLA QNLI RTE STS-B Avg. w/o 88.07/88.70 92.02 95.07 88.46 62.22 93.39 82.67 89.16 86.64 A1 85.42/85.43 91.07 92.09 86.14 54.18 91.31 70.04 87.61 82.59 A2 85.53/85.65 91.24 93.23 85.86 55.93 91.01 65.34 87.76 82.40 T1 32.05/32.18 74.31 83.60 70.78 0.00 58.37 51.99 0.00 44.81 T2 44.12/45.67 39.68 90.83 78.09 0.00 84.42 49.82 62.70 55.04 T3 36.12/36.08 74.75 90.25 81.51 0.00 85.41 54.15 0.00 50.92 T4 83.85/84.41 56.39 93.69 83.65 0.00 90.54 59.21 86.02 70.86 Q1 87.25/87.81 91.71 93.46 87.01 55.99 61.38 67.51 88.02 80.02 Q2 87.85/88.47 91.93 93.23 87.42 57.67 93.01 78.34 87.43 85.04 Table 5. Fine-tuning results over GLUE dataset under the setting that the tensor model-parallel size is 2 and pipeline model-parallel size is 2. F1 scores are reported for QQP and MRPC, Matthews correlation coef\ufb01cients are reported for CoLA, and Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. with NVLink), so compression is effective at reducing the communication time between two pipeline stages. From Table 9, we can observe that, by using A2 to compress the activation over the last 12 layers, we can reduce the communication cost between two pipeline stages effectively. Takeaway 5 Among all evaluated methods, AE is the best strategy to compress activation over pre-training. It achieves higher pre-training throughput and preserves the model\u2019s accuracy. pression), we can observe that using AE is able to keep the accuracy when compared to the uncompressed model. In addition, we observe that we can use the AE at the pre- training phase and remove it during the \ufb01ne-tuning phase. In other words, we only need to load the parameter of the BERTLarge model to do \ufb01ne-tuning, and the parameters of the AE can be ignored. Furthermore, Table 8 shows that pre- trained models suffer signi\ufb01cant accuracy loss when using Top-K for compression. Finally, quantization can preserve the model\u2019s accuracy, but we cannot achieve end-to-end speedup by using quantization as strategy to compress ac- From Table 8, compared with the baseline (without com- Does compressing activations help model parallel training? Distributed Setting w/o A1 A2 T1 T2 T3 T4 TP=2, PP=8 1,625.16 1,550.18 1,579.70 1,508.34 1,503.54 1,593.37 1,682.87 TP=4, PP=4 1,422.40 1,242.97 1,223.20 1,360.37 1,352.61 1,410.47 1,721.87 TP=8, PP=2 15,642.30 14,577.29 14,073.45 14,308.12 14,543.81 18,919.92 27,152.07 Distributed Setting w/o R1 R2 R3 R4 Q1 Q2 TP=2, PP=8 1,625.16 10,308.03 20,814.20 55,925.28 >100,000 1,759.27 1,752.24 TP=4, PP=4 1,422.40 15,433.12 31,565.19 87,421.46 >100,000 2,435.03 2,594.94 TP=8, PP=2 15,642.30 32,522.47 61,049.87 >100,000 >100,000 16,414.57 16,517.44 Table 6. The average iteration time (ms) for pre-training with various compression techniques by varying the distributed setting. The results are collected from 4 AWS p3.8xlarge machines with NVLink by using micro-batch size 128, global batch size 1024, and sequence length 128. The best setting is bolded in the table. And the settings, under which we can gain bene\ufb01ts compared with the baseline, are underlined. Compression Algorithm Forward Backward Optimizer Waiting & Pipeline Comm. Total Time Tensor Enc. Tensor Dec. Tensor Comm. w/o 467.73 419.26 7.42 527.99 1,422.40 \\ \\ 91.08 A1 A2 546.95 459.26 455.26 467.51 7.29 9.64 233.47 286.78 1,242.97 1,223.20 8.64 12.96 16.20 20.52 32.76 43.56 T1 712.22 423.91 7.21 217.03 1,360.37 73.44 140.4 80.28 T2 671.19 424.27 7.35 249.80 1,352.61 81.00 170.64 81.36 T3 813.03 433.42 7.35 156.67 1,410.47 108.00 268.92 115.92 T4 1,068.38 444.26 6.75 202.48 1,721.87 153.36 427.68 151.56 R1 R2 14,199.56 29,344.85 421.40 427.18 4.23 3.91 807.93 1,789.25 15,433.12 31,565.19 13,185.72 27,975.24 181.44 181.44 193.68 187.20 R3 78,906.91 444.88 6.08 3,707.37 83,065.23 73,847.16 279.72 649.44 Q1 803.63 417.33 8.61 1,205.46 2,435.03 90.72 304.56 193.68 Q2 805.33 417.74 7.55"}