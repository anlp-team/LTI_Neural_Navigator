{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Data-efficient_Active_Learning_for_Structured_Prediction_with_Partial_Annotation_and_Self-Training_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the initial learning rate used for NER optimization?", "answer": " 1e-5", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " How many model parameters are roughly used for multi-task IE tasks?", "answer": " 186M", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " What optimization algorithm is utilized for training?", "answer": " Adam", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " How often is the model evaluated on the dev set to choose the best checkpoint?", "answer": " every 1K steps", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " What GPU is used for running the experiments?", "answer": " 2080Ti", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " How long does the training of one AL cycle usually take?", "answer": " one or two hours", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " What evaluation metrics are used for DPAR?", "answer": " labeled attachment score (LAS)", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " How is the loss function calculated for learning with incomplete annotations?", "answer": " log of the marginal likelihood", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " What is the objective of knowledge distillation in self-training?", "answer": " soft labels", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}, {"question": " Which function is used for acquisition in the acquisition function based on uncertainty?", "answer": " margin-based", "ref_chunk": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}], "doc_text": "contextual- ized encoder. We further fine-tune it with the task-specific decoder in all the experiments. The number of model parameters is roughly 124M for single-output tasks and around 186M for multi-task IE tasks. For other hyper-parameter settings, we mostly follow common practices. Adam is utilized for optimization, with an initial learning rate of 1e- 5 for NER and 2e-5 for DPAR and IE. The learning rate is linearly decayed to 10% of the initial value throughout the training process. The models are tuned for 10K steps with a batch size of roughly 512 tokens. We evaluate the model on the dev set every 1K steps to choose the best checkpoint. The experiments are run with one 2080Ti GPU. The training of one AL cycle usually takes only one or two hours, and the full simulation of one AL run can be finished within one day. We adopt stan- dard evaluation metrics for the tasks: labeled F1 score for NER, labeled attachment score (LAS) for DPAR, labeled argument and relation F1 score for event arguments and relations (Lin et al., 2020). 11https://www.clips.uantwerpen.be/conll2003/ ner/ 12https://universaldependencies.org/ 13https://catalog.ldc.upenn.edu/LDC2006T06 14https://stanfordnlp.github.io/stanza/ 15http://blender.cs.illinois.edu/software/ oneie/ C Details of Algorithms In this section, we provide more details of the al- gorithms for CRF-styled models (Lafferty et al., 2001). For an input instance x (for example, a sen- tence), the model assigns a globally normalized probability to each possible output structured ob- ject y (for example, a tag sequence or a parse tree) in the target space Y: p(y|x) = = (cid:80) (cid:80) exp s(y|x) y\u2032\u2208Y exp s(y\u2032|x) exp (cid:80) f \u2208y s(f |x) (cid:80) f \u2032\u2208y\u2032 s(f \u2032|x) y\u2032\u2208Y Here, s(y|x) denotes the un-normalized raw scores assigned to y, which is further factorized into the sum of the sub-structure scores s(f |x).16 In plain likelihood training for CRF, we take the negative log-probability as the training objective: L = \u2212 log p(y|x) = \u2212s(y|x) + log (cid:88) exp s(y\u2032|x) y\u2032\u2208Y For brevity, in the remaining, we use log Z(x) to denote the second term of the log partition function. For model training, we need to calculate the gradi- ents of the model parameters \u03b8 to the loss function. The first item is easy to deal with since it only in- volves one structured object, while log Z(x) needs some reorganization according to the factorization: \u2207\u03b8 log Z = (cid:80) y\u2032\u2208Y exp s(y\u2032|x)\u2207\u03b8s(y\u2032|x) y\u2032\u2032\u2208Y exp s(y\u2032\u2032|x) (cid:80) = (cid:88) p(y\u2032|x)\u2207\u03b8s(y\u2032|x) = y\u2032\u2208Y (cid:88) p(y\u2032|x) (cid:88) \u2207\u03b8s(f \u2032|x) = y\u2032\u2208Y (cid:88) f \u2032\u2208y\u2032 \u2207\u03b8s(f \u2032|x) (cid:88) p(y\u2032|x) f \u2032 y\u2032\u2208Yf \u2032 The last step is obtained by swapping the order of the two summations, and finally, the problem is re- duced to calculating each sub-structure\u2019s marginal probability (cid:80) y\u2032\u2208Yf \u2032 p(y\u2032|x). Here, Yf \u2032 denotes all the output structured objects that contain the sub-structure f \u2032, and the marginals can usually be calculated by classical structured prediction al- gorithms such as forward-backward for sequence 16Such as unary and pairwise scores for sequence labeling or token-wise edge scores for dependency parsing. labeling (Baum et al., 1970) or Matrix-tree for non-projective dependency parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007). Learning with incomplete annotations. Follow- ing previous works (Tsuboi et al., 2008; Li et al., 2016; Greenberg et al., 2018), for the instances with incomplete annotations, we utilize the loga- rithm of the marginal likelihood as the learning objective: L = \u2212 log (cid:88) p(y|x) = \u2212 log = \u2212 log y\u2208YC (cid:88) y\u2208YC (cid:88) (cid:80) exp s(y|x) y\u2208Y exp s(y|x) exp s(y|x) + log Z(x) y\u2208YC Here, YC denotes the constrained set of the output objects that agree with the existing partial annota- tions. In this objective function, the second item is exactly the same as in standard CRF, while the first one can be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p\u2032(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = \u2212 (cid:88) p\u2032(y|x) log p(y|x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x)s(y|x) + log Z(x) = \u2212 y\u2208Y (cid:88) p\u2032(y|x) (cid:88) s(f \u2032|x) + log Z(x) = \u2212 y\u2208Y (cid:88) s(f \u2032|x) f \u2032\u2208y\u2032 (cid:88) p\u2032(y\u2032|x) + log Z(x) f \u2032 y\u2032\u2208Yf \u2032 The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to en- force the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that vio- lates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded. \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u001b\u0000\u0019 \u00005\u0000$\u00001\u0000' \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u00003\u0000$\u0000\u0010\u00000 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u001b\u0000\u0019 \u0000\u001b\u0000\u0017 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00005\u0000$\u00001\u0000' \u00003\u0000$\u0000\u0010\u0000( \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u001b \u00003\u0000$\u0000\u0010\u00000 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u0010\u0000% \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000$\u00003\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$\u0000\u0010\u0000/\u0000& \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b Figure 7: Comparisons of different acquisition functions for partial annotation: \u201c-M\u201d denotes margin-based, \u201c-LC\u201d denotes least-confident, \u201c-E\u201d denotes entropy-based, and \u201c-B\u201d indicates BALD. \u00005\u0000$\u00001\u0000' \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u00000 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000( \u0000\u001b\u0000\u0017 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000)\u0000$\u0000\u0010\u0000% \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u0010\u0000/\u0000& \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u0000'\u0000$\u00003\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W Figure 8: Comparisons of different acquisition functions for full annotation. Notations of the methods are the same as in Figure 7. D Extra Results D.1 Using Different Acquisition Functions In the main experiments, our acquisition function is based on margin-based uncertainty, that is, se- lecting the instances that have the largest marginal differences between the most and second-most con- fident predictions. Here, we compare it with var- ious other acquisition functions, including least- confident (-LC), max-entropy (-E) and BALD (- B) (Houlsby et al., 2011). We take DPAR as the studying case and the results for full annotation and partial annotation are shown in"}