{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_A_Pretrainer's_Guide_to_Training_Data:_Measuring_the_Effects_of_Data_Age,_Domain_Coverage,_Quality,_&_Toxicity_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What should inform the filtering strategy for toxicity identification?", "answer": " The intended behavior of the model should inform the filtering strategy.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " According to the text, which filter yields the strongest performance on toxicity identification for every dataset?", "answer": " The inverse toxicity filter yields the strongest performance on toxicity identification.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " What is more important for downstream performance according to the text: data quality or data source heterogeneity?", "answer": " Data source heterogeneity is more important than data quality for downstream performance.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " How did the inclusion of Common Crawl, OpenWeb, and Books affect downstream performance?", "answer": " The inclusion of Common Crawl, OpenWeb, and Books had the strongest positive effects on downstream performance.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " What impact does the choice of pretraining source domains have on downstream performance?", "answer": " The choice of pretraining source domains impacts downstream performance as demonstrated by ablating pretraining sources from the Pile and measuring the performance change in diverse QA tasks.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " Which domains have the strongest positive effects on downstream performance, as shown in the text?", "answer": " Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " What is suggested as a trade-off between toxic identification and generation in the text?", "answer": " The trade-off suggested in the text is that better performance on QA and toxicity identification comes at the cost of more toxic generation.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " Which domains, when removed, lead to the greatest decrease in toxicity metrics?", "answer": " Removing Books, CommonCrawl, and OpenWeb lead to the greatest decrease in toxicity metrics.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " What causes the largest trade-off between toxic identification and generation?", "answer": " Web and Books domains cause the biggest trade-off between toxic identification and generation.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}, {"question": " What is the hypothesis regarding the coverage of topics in CC, OpenWeb, and Books in relation to Academic-specific sources?", "answer": " The hypothesis is that CC, OpenWeb, and Books contain extensive coverage of many topics, which is why removing the Academic-specific category of sources does not remove all relevant academic information.", "ref_chunk": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}], "doc_text": "data instead. though this may be due to the overall decrease in training data. Ultimately, the intended behaviour of the model should inform the filtering strategy, rather than one size fits all. Most interesting of all, the strongest performance on toxicity identification for every dataset comes from the inverse toxicity filter. Practitioners optimizing for performance on toxic domains should intentionally apply inverse filters. 6 Impact of Domain Composition on Pretrained Models Section Findings Inclusion of Common Crawl, OpenWeb and Books have the strongest positive effects on down- stream performance. Data source heterogeneity is more important than data quality or size. Targeted data helps targeted evaluations, but not always as much as including heterogeneous web domains. It is beneficial to include as many pretraining data sources as possible. As shown in Table 1, pretraining datasets seek to generalize to a wide array of downstream tasks by combining data from a diverse set of domains. How does the choice of pretraining source domains impact downstream performance? We empirically answer this question by ablating pretraining sources from the Pile one-at-a-time and measuring the downstream performance change in 27 QA tasks from diverse domains. We first group the Pile data sources into nine domains representing conceptual sources that practitioners could choose to license or scrape more of: Common Crawl (CC), OpenWeb, Wikipedia, Books, PubMed, Academic, Code & Math, Legal, and Social (see Table 8). These are sorted in ascending order by size. We choose to maintain the size disparities in these sources, simply because they reflect reality: curated Wikipedia content is innately finite, while web and books are much more abundant. We then pretrain LM-XL with the full dataset minus each category, yielding nine models, then finetune each for QA using Natural Questions. Finally, we evaluate the model on 27 unique datasets from MRQA (Fisch et al., 2019) and UnifiedQA (Khashabi et al., 2020) that have also been partitioned into domains. Full details are documented in Appendix C.5. Common Crawl, OpenWeb, and Books have the strongest positive effects on downstream performance. Figure 8 shows that average downstream performance degrades the most when we remove web-based domains like CC, Books, and OpenWeb, corroborating recent findings by Xie et al. (2023a). In particular, these sources improve performance on challenging Common Sense and Contrast Sets tasks. While CC is the 13 WikiWebBooksBiomedAcademicCommonSenseContrastSetsAverageFull Dataset (100%)No Social (99%)No Wiki (98%)No Books (93%)No OpenWeb (93%)No Legal (91%)No Academic (87%)No Pubmed (85%)No Code (81%)No CC (73%) 8 8 4 4 0 0.00.00.00.00.00.00.00.0-0.8-3.72.60.13.5-3.53.50.4-1.3-5.33.00.20.9-4.47.2-0.3-3.5-6.31.00.0-1.6-6.5-4.4-2.7-2.0-4.10.1-1.00.6-5.8-2.9-1.4-2.7-2.93.80.40.8-2.6-0.4-0.6-0.3-2.50.3-0.92.2-1.14.30.2-0.3-3.03.9-5.8-1.5-5.93.9-1.2-0.5-3.12.9-1.21.2-5.84.4-0.1-3.2-6.2-2.9-4.6-5.9-8.0-5.2-4.8 2 2 6 6 Figure 8: QA tasks are affected by removing domains when pretraining LM-XL. Each row represents a model with one domain removed, the size of the remaining dataset is shown at the left in parentheses. Each column represents a set of QA evaluations from a domain. The Full Dataset model represents the unfiltered Pile LM-XL, and all scores are relative to this Base model. largest chunk of text in the Pile, Books and OpenWeb are smaller but provide the most heterogeneous and predicted-quality content (see Section 3). These results suggest that more data is not necessarily as important a factor as a combination of heterogeneity and quality. Domain heterogeneity is often more beneficial than targeted data, even for targeted evaluations. Ablating a pretraining domain has varying effects on downstream QA performance. Predictably, performance degrades when we remove domains with close alignment between the pretraining and downstream data sources: removing PubMed hurts the BioMed QA evaluations, dropping Wikipedia hurts the Wikipedia benchmarks, and removing web content hurts web evaluations. However, removing targeted domains does not necessarily have as significant an effect on related downstream domains as removing the large heterogeneous domains. For instance, removing CC from the pretraining dataset reduces performance on downstream Academic QA tasks to a much greater extent than removing the Academic domain. Our hypothesis is that CC, OpenWeb and Books contain extensive coverage of many topics, so removing the Academic-specific category of sources does not remove all relevant academic information. The best performing models use all the pretraining data sources. Despite the importance of data het- erogeneity, the best mean performance still comes from models that train on all, or nearly all, the data. The exceptions are the removal of targeted source domains like the Pile\u2019s Code or Academic (advanced science and math journals) domains. These are both large but perhaps not well matched with the QA evaluation sets, which do not require coding skills or scientific rigour beyond that found on Wikipedia and from web-based sources. This finding suggests that both the quantity and diversity of open source data remain a bottleneck for current pretraining methods. Web and Books domains cause the biggest trade-off between toxic identification and generation. We next consider whether reducing a model\u2019s pretraining exposure to toxic content affects either its propensity to generate toxic language or its ability to identify toxic language. Table 3 shows that the largest decreases in both toxicity generation and identification were caused by removing CC (26.9% of the data), OpenWeb (6.9%), and Books (6.9%). This is consistent with the observation that Web and Books data had the highest concentration of text predicted to be toxic Section 3. These results suggest a trade-off: better performance on QA (Section 6) and toxicity identification comes at the cost of more toxic generation. 14 Table 3: Effect of the Pile\u2019s domain composition on toxicity identification and generation. Removing Books, CommonCrawl and OpenWeb lead to the greatest decrease in toxicity metrics. Removing Wikipedia had a strong increase in toxicity generation. Filter % Data Toxicity Identification (\u2191) Toxic Generation (\u2193) SBF Toxigen DH R3 DH R4 Score RTP-T RTP-NT RepBias Score Full Dataset No Social No Wiki No Books No OpenWeb No Legal No Academic No Pubmed No Code No CC 100.0 90.7 98.8 90.9 97.9 90.6 93.1 89.9 93.1 89.9 91.0 90.9 87.1 90.7 85.1 90.6 80.9 91.0 73.1 89.9 90.8 91.0 90.8 90.3 90.3 90.8 91.0 90.8 91.2 90.0 88.7 87.8 88.1 87.1 86.4 88.1 88.2 88.0 88.5 85.3 84.1 84.9 83.6 82.6 82.5 83.0 84.5 84.3"}