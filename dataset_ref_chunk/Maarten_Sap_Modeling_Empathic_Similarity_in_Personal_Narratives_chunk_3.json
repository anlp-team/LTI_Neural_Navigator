{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Modeling_Empathic_Similarity_in_Personal_Narratives_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the EMPATHICSTORIES dataset?", "answer": " The EMPATHICSTORIES dataset is a corpus of personal stories containing 3,568 total annotations, including empathic similarity annotations of 2,000 story pairs and annotations for main events, emotions, morals, and empathy reasons for 1,568 individual stories.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " How many story pairs are included in the EMPATHICSTORIES dataset?", "answer": " The EMPATHICSTORIES dataset includes empathic similarity annotations for 2,000 story pairs.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " What does the data annotation pipeline for the EMPATHICSTORIES dataset involve?", "answer": " The data annotation pipeline involves annotating individual story events, emotions, and morals, then using these annotations to sample balanced story pairs and rate empathic similarity scores.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " What are some of the themes across the main events of the stories in the dataset?", "answer": " Some themes across the main events of the stories include romantic relationships, positive life events, depression, family, substance use, encouragement, college and school, loneliness, youth, life changes, work, and trauma.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " Where are the stories in the dataset collected from?", "answer": " The stories in the dataset are collected from sources like social media sites, spoken narratives, and crowdsourced stories, including online personal stories, crowdsourced personal stories, and spoken personal narratives.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " How are the story pairs for empathic similarity annotation created?", "answer": " The story pairs for empathic similarity annotation are created using a sampling method that aims to select balanced empathically similar and dissimilar story pairs based on composite similarity scores computed using SBERT.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " What tool is used to summarize stories before presenting them to annotators?", "answer": " ChatGPT (gpt-3.5-turbo) is used to summarize all the stories before presenting the pairs to annotators to simplify the task.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " How many similarity ratings do workers provide for each story pair?", "answer": " Workers provide four similarity ratings on a 4-point Likert scale for each story pair, rating overall empathic similarity, similarity in the main events, emotions, and morals of the stories.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " What are the agreement scores for empathic, event, emotion, and moral similarity among annotators?", "answer": " The agreement scores for empathic, event, emotion, and moral similarity among annotators are shown in Table 3, with most common disagreements being at most 1 Likert point away.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}, {"question": " What approach is taken to handle the cognitive load and simplify the annotation task for story pairs?", "answer": " To handle the cognitive load and simplify the annotation task for story pairs, ChatGPT is used to summarize all the stories before presenting them to annotators.", "ref_chunk": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}], "doc_text": "(Bal and Veltkamp, 2013). 4 EMPATHICSTORIES Dataset We introduce EMPATHICSTORIES, a corpus of per- sonal stories containing 3,568 total annotations. Specifically, the corpus includes empathic similar- ity annotations of 2,000 story pairs, and the main events, emotions, morals, and empathy reason an- notations for 1,568 individual stories. An overview of our data annotation pipeline is shown in Fig- ure 2 and data preprocessing steps are included in Appendix D. In Appendix H, we show that using LLMs for human annotation is not viable for our task. Figure 2: Overview of annotation pipeline starting with (a) individual story event, emotion, and moral to (b) using these annotations to sample balanced story pairs and (c) rating empathic similarity scores Story Main Event Emotional Reaction Moral # sents 13.17 1.48 2.39 1.38 # words 235.14 32.51 46.08 31.35 Table 1: Story and annotation statistics 4.1 Data Sources Topic romantic relationships positive life events depression family substance use encouragement college and school loneliness youth life changes work trauma Keywords relationships, divorced, passion opportunities, wedding, cruise depression, therapy, psych families, parents, relatives recovery, drugs, addiction encouragement, caring, distress students, classes, college loneliness, relationships, haircut teenage, childhood, twenties goodbyes, retired, graduating mundane, coworkers, volunteering abused, traumas, therapist % Stories 15.63% 13.20% 12.95% 10.33% 9.38% 8.42% 7.08% 5.87% 4.97% 4.40% 4.34% 3.44% We collect a diverse set of stories from sources in- cluding social media sites, spoken narratives, and crowdsourced stories. We take approximately 500 stories from each of the following sources (for a full breakdown see Appendix F). These sources contain English-written stories revolving around deep emotional experiences and open-ended con- versation starters. (1) Online Personal Stories. We scrape stories from subreddits2 about personal ex- periences (r/offmychest, r/todayiamhappy, and r/casualconversation). We also include a small set of stories from a public college confessions forum. Table 2: Themes across main events of the stories. main event of the story, (2) the main emotional state induced by the main event, and (3) moral(s) of the story. The story and annotated summary statistics are shown in Table 1. The themes from stories are shown in Table 2, and themes for annotated sum- maries as well as our topic modeling approach are presented in Appendix E. 4.3 Paired Story Annotation (2) Crowdsourced Personal Stories. We use a subset of autobiographical stories from the exist- ing Hippocorpus dataset (Sap et al., 2020), which contains recalled and imagined diary-like personal stories obtained from crowdworkers. (3) Spoken Personal Narratives. We use stories from the Roadtrip Nation corpus (Saldias and Roy, 2020), which contains transcribed personal stories about people\u2019s career trajectories and life stories. 4.2 Individual Story Annotation Using these stories, we designed an annotation framework on Amazon Mechanical Turk (MTurk) that asks workers to label individual story features. Then, we asked for short free responses on (1) the Sampling Empathic Story Pairs. We devise a sampling method to create a sample of balanced empathically similar and dissimilar story pairs, since random sampling across all possible pairs would likely result in an unbalanced dataset with more dissimilar stories than similar stories. First, we split the 1,568 stories into a train, dev, and test set using a 75/5/20 split. Using SBERT (Reimers and Gurevych, 2019), we compute a composite similarity score using average cosine similarity of the embeddings for the story and our 3 empathy features for every possible story pair within the dataset. We randomly sample stories from each bin such that bins with higher composite similarity scores are more likely to be chosen. 2https://api.pushshift.io/ Annotation Procedure With the sampled story pairs, we released an annotation task on Amazon Annotation Empathic similarity Event similarity Emotion similarity Moral similarity Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test Overall Train Dev Test PPA KA .14 .80 .14 .79 .11 .81 .17 .83 .27 .86 .26 .86 .25 .84 .30 .87 .23 .83 .23 .83 .15 .79 .25 .84 .19 .80 .18 .80 .14 .80 .20 .82 Table 3: Similarity agreement scores (PPA = pairwise percent agreement, KA = Krippendorff\u2019s Alpha) MTurk, asking workers to read pairs of stories and rate various aspects of empathic similarity between the stories. Two annotators rated each story pair. From early testing, we found that the task was difficult because of the large amount of text in the stories and the cognitive load of projecting into two narrator\u2019s mental states. To simplify the task, we used ChatGPT (gpt-3.5-turbo) to summarize all the stories before presenting the pairs to annotators. While summarization may remove specific details of the stories, we find that the main event, emotion, and moral takeaway are still present.3 At the beginning of the task, we first provide the annotator with 6 examples of empathically simi- lar stories: one positive and one negative example for stories that are empathically similar/dissimilar based on each feature: main event, emotion, and moral of the story. After reading the two stories, we ask workers to provide explanations of whether and why the narrators would empathize with one another, to prime annotators to think about the em- pathic relationship between the stories. We then ask workers to provide four similarity ratings on a 4-point Likert scale (1 = strongly disagree, 4 = strongly agree): (1) overall empathic similarity (how likely the two narrators would empathize with each other), (2) similarity in the main events, (3) emotions, and (4) morals of the stories. Agreement We aggregate annotations by averaging between the 2 raters. Agreement scores for empa- 3By comparing the cosine similarity of human annotated event, emotion, and moral to the ChatGPT summarized sto- ries, we find that there is high semantic overlap of the human ground-truths to the automatically generated summaries (0.66 for event, 0.64 for emotion, and 0.49 for moral). thy, event, emotion, and moral similarity across the entire dataset are shown in Table 3. While these agreement scores are seemingly on the lower side, using a softer constraint, we see that most common disagreements are at most 1 likert point away (73% of points are at most 1"}