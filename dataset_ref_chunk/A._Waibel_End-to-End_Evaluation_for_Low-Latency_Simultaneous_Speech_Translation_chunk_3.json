{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_End-to-End_Evaluation_for_Low-Latency_Simultaneous_Speech_Translation_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What framework is used to assess the system in different aspects and log the results?", "answer": " MLflow", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " How is the translation quality assessed in the text?", "answer": " Using BLEU score", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " What tool is used to align the final stable translation with the gold reference before calculating the BLEU score?", "answer": " mwerSegmenter", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " What metric is used to assess the transcription quality of the ASR component?", "answer": " Word Error Rate (WER)", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " How is the latency of the system defined in the text?", "answer": " The average time it takes from an utterance is spoken until its first-unchanged translation is returned", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " How are the first-unchanged messages collected in the text?", "answer": " By backtracking the previously received unstable messages in unstable-to-stable message blocks", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " How is the latency calculated in the text?", "answer": " As the weighted average of the delays of all first-unchanged messages based on their length", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " What is the flickering rate defined as in the text?", "answer": " The average number of flickers per reference word", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " What is the domain of the test datasets used in the experimental setup?", "answer": " TED talks", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}, {"question": " What statistics are provided in Table 1 in the text?", "answer": " Statistics of the test data including language pairs and utterances", "ref_chunk": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}], "doc_text": "framework4 that assess the system in different aspects and logs the results to categorized experiments on an UI board using MLflow (Zaharia et al., 2018). We consider different evaluation metrics as follows. BLEU: In order to assess the translation quality, we use case-sensitive BLEU score, calculated using sacreBLEU (Post, 2018). We extract the final stable translation, align it sentence-wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the BLEU score. 4https://git.scc.kit.edu/isl/lt-middleware/ lt-evaluation tete tste1tststste2te3te4tr4 Audio streaming time11312424tr1unstabletr2unstabletr3 Messagearrivaltime teteunstablestabletrtrtrOriginal messages receivedSelecting first-unchangedmessages from the original. A A B HA D C EA D C F GA A B HA D C EA D C F G tste Figure 3: Example of collecting first-unchanged mes- sages from an unstable-to-stable message block. The first-unchanged messages are in green. WER: In order to assess the transcription qual- ity of the ASR component in the cascaded setting, we use the case-sensitive Word Error Rate (WER) calculated using JiWER5. Similar as before, we ex- tract the final stable transcription, align it sentence- wise with the gold reference using mwerSegmenter (Matusov et al., 2005) before calculating the WER. Latency: We evaluate the latency of the system in an end-to-end manner. Factors such as network latency influence our latency metrics. However, our experiments are conducted locally, thus such factors are constant and negligible. We define the end-to-end latency of the system as the average time (in seconds) it takes since an utterance is spoken until its first-unchanged trans- lation is returned by the system. Note that the first- unchanged translation is not necessarily already marked as \u201cstable\" by the system. For each message returned by the system, we have the stable/unstable flag along with three times- tamps, ts, te, tr. The timestamps ts and te are the start and end time of the audio segment that aligns to the message. The timestamp tr is when the mes- sage was received. We collect the first unchanged messages as follows. We split the received mes- sages into blocks of messages marked from \u201cunsta- ble\" to \u201cstable\". In each unstable-to-stable block, from the last stable message, we backtrack the pre- viously received unstable messages to find the first ones that has prefix-overlaps with the final stable message. The illustration is shown in Figure 3. Once we have collected the first-unchanged mes- sages, we can calculate the latency. We use the same definition of delay as Niehues et al. (2016), where the average delay of the ith message is: d(ti s, ti e, ti r) = ti r \u2212 s + ti ti e 2 . 5https://github.com/jitsi/jiwer A A B HA D C EA D C F G Audio streaming timeunstablestable Messagearrivaltimetr1unstabletr2unstabletr3tr4 Figure 4: Example of flickers (denoted by red arrows) in an unstable-to-stable message block. Then we calculate the latency as the weighted average of the delays of all m first-unchanged mes- sages based on their length: D = (cid:80)m i=1 d(ti (cid:80)m e, ti s, ti i=1(ti r) \u2217 (ti e \u2212 ti s) e \u2212 ti s) . Note that the timestamps ts and te in our la- tency formula are calculated by the used streaming algorithm. Therefore, we also tried another model- independent latency metric that only uses tr. This metric approximates the segment-message align- ment by assuming that each word output by the system has the duration of 0.3 second in the au- dio. Due to the strong assumption, this metric does not represent well the perceived latency. We only use this metric in order to verify our main model- dependent latency metric. We find that the model-independent latency met- ric and our model-dependent metric provide the same relative ranking of the systems. This indi- cates that the timestamps ts and te provided by the model itself are reliable to measure latency. Flickering rate: The flickering rate is the av- erage number of flickers per reference word. We count the number of flickers by looking at every pair of consecutive messages in a message block. If two words in the same position in the two messages differ, then it is counted as a flicker (see Figure 4). The flickering rate is calculated as the total number of flickers divided by the total number of words in the reference. 6 Experimental setup 6.1 Evaluation data We test our system using datasets from different language pairs. Test datasets includes: Test data from the IWSLT shared task (tst19, tst20) (Anastasopoulos et al., 2021, 2022), where the domain is TED talks. Test data tst19 tst20 LT CS LT nonCS mTEDx Lang. pair Hours en\u2192de 4.82 en\u2192de 4.09 de\u2192en 6.39 de\u2192en 2.66 es\u2192en 2.07 it\u2192en 2.16 en\u2192X 0.95 # Utterances 2279 1804 2454 1516 1012 999 468 ACL dev * Table 1: Statistics of the test data. *Test data containing en audio with translations into de, ja, zh, ar, nl, fr, fa, pt, ru and tr. Cascaded ST Testset C W \u2193 B \u2191 21.6 tst19 24.6 25.5 25.7 29.8 32.6 34.2 35.2 .5 1 2 3 .5 1 2 3 20.8 17.0 16.4 16.6 18.7 16.7 16.7 17.2 ACL dev L \u2193 3.6 5.6 6.8 7.8 4.3 6.2 7.4 8.6 E2E ST B \u2191 20.5 22.8 23.2 23.6 22.4 25.4 26.5 26.2 L \u2193 2.1 2.6 3.9 5.0 2.1 2.7 4.0 5.3 Table 2: Quality vs. latency (in fixed mode). C: LA2_chunk_size (s), W: WER, B: BLEU score, L: Latency (s) of the translation output. The test split of Multilingual TEDx Corpus (mTEDx) (Salesky et al., 2021), where the domain is TED talks. Lecture data (LT) which we collected inter- nally at our university. This test set include a CS variance which includes lectures on the Computer Science domain, and a nonCS vari- ance which includes lectures outside of the Computer Science domain. ACL development (ACL dev) set (Salesky et al., 2023), where the domain is ACL con- ference talks. The detailed statistics of the test data is shown in Table 1. 6.2 Transcription and translation models The English ASR models are built based on pretrained"}