{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_PWESuite:_Phonetic_Word_Embeddings_and_Tasks_They_Facilitate_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main benefit of word embeddings in modern NLP?", "answer": " Compressing information into fixed-dimensional vectors", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " What type of information is often overlooked by most word embedding methods?", "answer": " Phonetic information", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " What is the objective of phonetic word embeddings?", "answer": " To map words with similar pronunciation to vectors near each other in embedding space", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " Give an example of a task that has benefited from incorporating phonetic word embeddings.", "answer": " Cognate and loanword detection", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " What is the purpose of the evaluation suite introduced for phonetic word embeddings?", "answer": " To test the performance of phonetic embeddings and promote reproducibility in research", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " What is one potential application of phonetic word embeddings mentioned in the text?", "answer": " Spelling correction", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " How do semantic and extrinsic metrics for phonetic word embeddings generally correlate with each other?", "answer": " They generally correlate with each other", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " What is the function of a phonetic embedding function as described in the text?", "answer": " To map words to d-dimensional vector space", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " What are the three distinct alphabets used in the text for phonetic embeddings?", "answer": " Characters, IPA symbols, ARPAbet symbols", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}, {"question": " What is the main contribution of the evaluation suite for phonetic word embeddings according to the text?", "answer": " An evaluation suite that does not yet exist in the subfield", "ref_chunk": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}], "doc_text": "4 2 0 2 b e F 0 2 ] L C . s c [ 2 v 1 4 5 2 0 . 4 0 3 2 : v i X r a PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate Vil\u00e9m Zouhar E = Kalvin Chang C = Chenxuan Cui C Nathaniel Carlson Y Nathaniel R. Robinson C Mrinmaya Sachan E David Mortensen C EDepartment of Computer Science, ETH Zurich CLanguage Technologies Institute, Carnegie Mellon University YDepartment of Computer Science, Brigham Young University {vzouhar,msachan}@ethz.ch natbcar@gmail.com {kalvinc,cxcui,nrrobins,dmortens}@cs.cmu.edu Abstract Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research. Keywords: phonetic word embeddings, representation learning, phonology, articulatory features, evaluation Code: github.com/zouharvi/pwesuite Dataset: huggingface.co/datasets/ zouharvi/pwesuite-eval 1. Introduction soybean | s\u0254\u026abi\u02d0n | S OY B IY N motion | mo\u028a\u0283\u0259n | M OW SH AH N f ocean | o\u028a\u0283\u0259n | OW SH AH N Word embeddings are omnipresent in modern NLP (Le and Mikolov, 2014; Pennington et al., 2014; Almeida and Xex\u00e9o, 2019, inter alia). Their main benefit lies in compressing some information into fixed-dimensional vectors. These vectors can be used as machine-learning features for NLP appli- cations, and their study can reveal linguistic in- sights (Hamilton et al., 2016; Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David R. and Tsvetkov, Yulia, 2020; Francis et al., 2021). Word embeddings are often trained via methods from distributional semantics (Camacho-Collados and Pilehvar, 2018) and thus bear semantic information. For example, the em- bedding for the word carrot may encode higher similarity to embeddings for other vegetables than to that of ocean. Figure 1: Embedding function \u0192 projects words in various forms (left) to a vector space (right). phonetic word embeddings, contain phonetic in- formation and have been of recent interest (Par- rish, 2017; Yang and Hirschberg, 2019; Hu et al., 2020; Sharma et al., 2021).1 The objective is that words with similar pronunciation should be mapped to vectors near each other in embedding space. Many tasks have benefited from incorpo- rating phonetic word embeddings, including cog- nate and loanword detection (Rama, 2016; Nath et al., 2022b,a), named entity recognition (Bharad- waj et al., 2016; Chaudhary et al., 2018), spelling correction (Zhang et al., 2021), and speech recog- nition (Fang et al., 2020). See Section 6.2 for a more detailed list of possible applications. Some applications may require a different type of information to be encoded. The orthography, especially in English, can obscure the pronuncia- tion. A poem generation model, for instance, may need embeddings to reflect that ocean rhymes with motion and not with a soybean, even though the spelling of the words\u2019 final syllables suggest oth- erwise (see Figure 1). Such embeddings, called We introduce four phonetic word embedding methods\u2014count-based, autoencoder, and metric and contrastive learning. Though some of these techniques are inspired by previous work, we are the first to apply them with supervision from articu- latory feature vectors, a seldom-exploited form of =Co-first authors. 1The technically correct term is phonological word embeddings but prior literature uses the term phonetic. linguistic knowledge for representation learning. More importantly, we introduce an evaluation suite for testing the performance of phonetic embeddings. The motivations for this are two- fold. First, prior work is inconsistent in evaluat- ing models. This prevents the field from observ- ing long-term improvements in such embeddings and from making fair comparisons across different approaches. Secondly, when a practitioner is de- ciding which phonetic word embedding method to use, the go-to approach is to first apply the embed- dings (generally fast) and then train a downstream model on those embeddings (compute and time intensive). Instead, intrinsic embedding evaluation metrics (cheap)\u2014if shown to correlate well with extrinsic metrics\u2014could provide useful signals in embedding method selection prior to training of downstream models (expensive). In contrast to semantic word embeddings (Bakarov, 2018), we show that intrinsic and extrinsic metrics for pho- netic word embeddings generally correlate with each other. While Ghannay et al. (2016) evalu- ate acoustic word embeddings, we specialize in phonetic word embeddings for text, not speech. Our main contribution is this evaluation suite for phonetic word embeddings, the equivalent of which does not yet exist in this subfield. We also contribute multiple methods for and a survey of existing phonetic word embeddings. 2. Survey of Phonetic Embeddings Given an alphabet \uef23 and a dataset of words W \u2286 \uef23\u2217, d-dimensional word embeddings are given by a function \u0192 : W \u2192 Rd. This function takes an element from \uef23\u2217 (set of all possible words over the alphabet \uef23) and maps it to a d-dimensional vector of numbers. For many embedding functions, W is a finite set of words, and the embeddings are not defined for unseen words (Mikolov et al., 2013a; Pennington et al., 2014). Other embed- ding functions\u2014which we dub open\u2014are able to provide an embedding for any word \uebf8 \u2208 \uef23\u2217 (Bo- janowski et al., 2017). An illustration of a phonetic embedding function is shown in Figure 1 (motion is closer to ocean than to soybean). We use 3 distinct alphabets: characters \uef23C, IPA symbols \uef23P and ARPAbet symbols \uef23A. We use \uef23 when the choice is not important and refer to ele- ments of \uef23 as characters or phonemes. We review some semantic embeddings in Section 5 and now focus on prior work in phonetic embeddings. From our formalism it also follows that we are interested"}