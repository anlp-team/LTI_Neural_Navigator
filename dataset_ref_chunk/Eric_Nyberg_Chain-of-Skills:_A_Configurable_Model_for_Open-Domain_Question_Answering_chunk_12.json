{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_Chain-of-Skills:_A_Configurable_Model_for_Open-Domain_Question_Answering_chunk_12.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What evaluation scripts were adopted for retrieval evaluations and reader evaluations?", "answer": " DPR evaluation scripts for retrieval evaluations and MDR evaluation scripts for reader evaluations.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " How many parameters does the COS have for pretraining?", "answer": " The COS has 182M parameters for pretraining.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " How many V100-32GB GPUs were used for COS pretraining?", "answer": " 32 V100-32GB GPUs.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " What are some of the models listed in the table for End-to-end QA results on Hotpot-QA?", "answer": " Some models listed include MUPPET, CogQA, GoldEn Retriever, Semantic Retrieval, and Transformer-XH.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " What licenses are associated with the software and data used in the paper?", "answer": " DPR: CC-BY-NC 4.0 License, MDR: CC-BY-NC 4.0 License, Contriever: CC-BY-NC 4.0 License, BLINK: MIT License, NQ: CC-BY-SA 3.0 License, HotpotQA: CC-BY-NC 4.0 License, OTT-QA: MIT License, EntityQuestions: MIT License, SQuAD: CC-BY-SA 4.0 License, WebQuestions: CC-BY 4.0 License.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " How many parameters does the reader model FiE have?", "answer": " The reader model FiE has 330M parameters.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " How many V100-32GB GPUs were used for training the reader model FiE?", "answer": " 16 V100-32GB GPUs.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " How long does it take to train the path reranker for HotpotQA?", "answer": " The path reranker takes about 12 hours to train for HotpotQA.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " Why do they train all their models only once?", "answer": " They train all of their models once due to the large computation cost.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}, {"question": " What licenses are associated with the EntityQuestions dataset?", "answer": " EntityQuestions: MIT License.", "ref_chunk": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}], "doc_text": "the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations. D Computation Our COS has 182M paramteres. For COS pretrain- ing, we use 32 V100-32GB GPUs, which takes 2https://github.com/facebookresearch/ DPR 3https://github.com/facebookresearch/ multihop_dense_retrieval (11) Ans Dev Sup Joint EM F1 EM F1 EM F1 EM MUPPET (Feldman and El-Yaniv, 2019) CogQA (Ding et al., 2019) GoldEn Retriever (Qi et al., 2019) Semantic Retrieval (Nie et al., 2019) Transformer-XH (Zhao et al., 2020) HGN (Fang et al., 2020) GRR (Asai et al., 2020) DDRQA (Zhang et al., 2021b) MDR (Xiong et al., 2021b) IRRR+ (Qi et al., 2021) HopRetriever-plus (Li et al., 2021) TPRR (Zhang et al., 2021a) AISO (Zhu et al., 2021) 31.1 37.6 - 46.5 54.0 - 60.5 62.9 62.3 - 66.6 67.3 68.1 40.4 49.4 - 58.8 66.2 - 73.3 76.9 75.1 - 79.2 80.1 80.9 17.0 23.1 - 39.9 41.7 - 49.2 51.3 56.5 - 56.0 60.2 61.5 47.7 58.5 - 71.5 72.1 - 76.1 79.1 79.4 - 81.8 84.5 86.5 11.8 12.2 - 26.6 27.7 - 35.8 - 42.1 - 42.0 45.3 45.9 27.6 35.3 - 49.2 52.9 - 61.4 - 66.3 - 69.0 71.4 72.5 30.6 37.1 37.9 45.3 51.6 59.7 60.0 62.5 62.3 66.3 64.8 67.0 67.5 COS 68.2 81.0 61.1 85.3 46.4 72.3 67.4 Table A3: End-to-end QA results on Hotpot-QA. about 3 days. For COS finetuning, we used 16 V100-32GB GPUs which takes about 2 days. Our reader model FiE has 330M parameters. We used 16 V100-32GB GPUs for training which takes about 1.5 days. For HotpotQA, both the path reranker and the reader have 330M parameters. We used 16 V100-32GB GPUs for training, the path reranker takes about 12 hours and the reader takes about 4 hours to train. We train all of our models once due to the large computation cost. E Licenses We list the License of the software and data used in this paper below: DPR: CC-BY-NC 4.0 License MDR: CC-BY-NC 4.0 License Contriever: CC-BY-NC 4.0 License BLINK: MIT License NQ: CC-BY-SA 3.0 License HotpotQA: CC-BY-NC 4.0 License OTT-QA: MIT License EntityQuestions: MIT License SQuAD: CC-BY-SA 4.0 License WebQuestions: CC-BY 4.0 License Ans F1 40.3 48.9 49.8 57.3 64.1 71.4 73.0 75.9 75.3 79.9 77.8 79.5 80.5 80.1 EM 16.7 22.8 30.7 38.7 40.9 51.0 49.1 51.0 57.5 57.2 56.1 59.4 61.2 61.3 Test Sup F1 47.3 57.7 64.6 70.8 71.4 77.4 76.4 78.9 80.9 82.6 81.8 84.3 86.0 85.3 Joint EM F1 10.9 12.4 18.0 25.1 26.1 37.9 35.4 36.0 41.8 43.1 41.0 44.4 44.9 27.0 34.9 39.1 47.6 51.3 62.3 61.2 63.9 66.6 69.8 67.8 70.8 72.0 45.7 71.7"}