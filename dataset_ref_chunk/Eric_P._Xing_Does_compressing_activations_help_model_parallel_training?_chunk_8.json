{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Does_compressing_activations_help_model_parallel_training?_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the three major components of an N-layers transformer model?", "answer": " An embedding layer, a stack of N transformer layers, and a prediction layer", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " What does a transformer layer consist of?", "answer": " An attention module and several matrix multiplications", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " What are some optimizations proposed to speed up Transformer model training?", "answer": " Carefully managing I/O and reducing the complexity of the attention module", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " What is the focus of the work by Li et al. (2022)?", "answer": " Developing new model parallelism strategies for Transformer models", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " What is one direction proposed to reduce communication bottleneck in distributed ML model training?", "answer": " Compressing the message size, especially the model gradients", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " What are some common techniques to reduce gradient communication in distributed training?", "answer": " Low-rank updates, sparsification, and quantization", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " What is the main focus of research in the paper regarding compression techniques on tensor and pipeline parallelism?", "answer": " Studying the effect of various compression techniques on tensor and pipeline parallelism in a cloud computing setting", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " What is the impact of compressing activations for models trained using model parallelism?", "answer": " Learning-based compression algorithms are the most effective approach", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " What parallelism strategies have been proposed to train Transformer models according to the text?", "answer": " Tensor model parallelism and pipeline parallelism", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}, {"question": " Who is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison?", "answer": " Shivaram Venkataraman", "ref_chunk": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}], "doc_text": "Touvron et al., 2021), audio (Gong et al., 2021) and beyond (Sharir et al., 2021). An N -layers transformer model is composed of three major components: (1) An embedding layer that maps an input token to a hidden state, (2) A stack of N transformer layers, and (3) a prediction layer that maps the hidden state proceeded by transformer layers to the task output. A transformer layer is composed of an attention module (Bahdanau et al., 2014) and several matrix multipli- cations. Several optimizations have been proposed to speed up Transformer model training such as carefully managing the I/O (Dao et al., 2022) and reducing the complexity of the attention module (Wang et al., 2020). In this work, we speed up the Transformer model training in the distributed setting, where we reduce the communication between workers. parallelism developed in Megatron and data parallelism to train Transformer models at the scale of trillion parame- ters. (Li et al., 2022) considers a more sophisticated model parallelism strategy space for Transformer models and uses a cost model to automatically search for the optimal one. Our work is orthogonal to the direction of developing new parallel training strategies. In this work, we study how to compress communication on existing parallel strategies. Distributed training with Compression. Distributed ML model training requires frequent and heavy synchronization between workers. Several directions have been proposed to reduce the communication bottleneck by compressing the message size. One direction is developed on the data parallelism setting, where workers communicate model gra- dients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsi\ufb01cation (Lin et al., 2017), and quanti- zation (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction \ufb01nd that the activation pro- duced during the forward propagation in neural networks is large, and thus compressing them is bene\ufb01cial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting. 6 CONCLUSION In this work, we studied the impact of compressing acti- vations for models trained using model parallelism. We implemented and integrated several popular compression algorithms into an existing distributed training framework (Megatron-LM) and evaluated their performance in terms of throughput and accuracy under various settings. Our re- sults show that learning-based compression algorithms are the most effective approach for compressing activations in model parallelism. We also developed a performance model to analyze the speedup when scaling up the model. Our ex- periments provide valuable insights for the development of improved activation compression algorithms in the future. Training Large Transformer models. Several parallelism strategies have been proposed to train Transformer mod- els. Megatron (Shoeybi et al., 2019) proposes tensor model parallelism, which parallelizes the computation in attention layers and in the following matrix multiplications. Deep- Speed (Rasley et al., 2020) uses a specialized form of pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019) that treats a transformer layer as the smallest unit in pipeline stages. It further combines the tensor model Acknowledgments Shivaram Venkataraman is supported by the Of\ufb01ce of the Vice Chancellor for Research and Graduate Education at UW-Madison with funding from the Wisconsin Alumni Research Foundation. Eric Xing is supported by NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF BCS2040381. Does compressing activations help model parallel training? hidden size number of layers number of nodes batch size speedup 6144 40 1 1024 1.91\u00d7 8192 48 2 1536 1.75\u00d7 10240 60 4 1792 1.63\u00d7 12288 80 8 2304 1.55\u00d7 16384 96 16 2176 1.46\u00d7 20480 105 35 2528 1.46\u00d7 25600 128 64 3072 1.47\u00d7 Table 10. Weak-scaling speedup for the Transformer models. The number of tensor model parallelism is 4, and the micro-batch size is 16. As for the change of the hidden size, the number of layers, and the batch size, we follow the setting of Table 1 in (Narayanan et al., 2021). REFERENCES Agarwal, S., Wang, H., Venkataraman, S., and Papailiopou- los, D. On the utility of gradient compression in dis- tributed training systems. Proceedings of Machine Learn- ing and Systems, 4:652\u2013672, 2022. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Hinton, G. E. and Zemel, R. Autoencoders, minimum de- scription length and helmholtz free energy. Advances in neural information processing systems, 6, 1993. Ho, Q., Cipar, J., Cui, H., Lee, S., Kim, J. K., Gibbons, P. B., Gibson, G. A., Ganger, G., and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural information processing systems, pp. 1223\u20131231, 2013. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand- kumar, A. signsgd: Compressed optimisation for non- convex problems. In International Conference on Ma- chine Learning, pp. 560\u2013569. PMLR, 2018. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat- tention: Fast and memory-ef\ufb01cient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Izsak, P., Berchansky, M., and Levy, O. train bert with an academic budget. arXiv:2104.07705, 2021. How to arXiv preprint Dettmers, T. 8-bit approximations for parallelism in deep learning. arXiv preprint arXiv:1511.04561, 2015. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16"}