{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Cascaded_Unsupervised_Speech_Translation_with_Denoising_Back-translation_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the approach introduced in the text for robust UNMT?", "answer": " Denoising back-translation", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " How is the pseudo parallel data generated in denoising back-translation?", "answer": " By passing the source sentence and target sentence through a noise function and translating them back", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " What is the objective of denoising back-translation?", "answer": " To reconstruct input and target sentences from the translated noisy versions", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " What is the denoising back-translation loss formula?", "answer": " LDBT = Ex\u2208S[- log PT\u2192S(x|u\u2217(f(x)))] + Ey\u2208T[- log PS\u2192T(y|v\u2217(f(y)))]", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " How are the pseudo labels of denoising back-translation different from the original back-translation?", "answer": " The pseudo labels of DBT are generated from noisy sentences", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " What data is used to demonstrate the US2ST systems in the experiments section of the text?", "answer": " CVSS, which is a multilingual corpus built on top of CoVoST 2 and Common-Voice ver.4", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " What is the data source for the audio and text used in the UASR and UTTS processes?", "answer": " Audio from Common Voice ver.4 and text from Wikipedia", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " What is the configuration chosen for the loss function during GAN training of UASR?", "answer": " The gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " How is the input data for the text denormalizer (TDN) constructed?", "answer": " By normalizing 8\u201310M of plain text data from CC100", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}, {"question": " What model was used for training the UNMT models in the text?", "answer": " The back-translation fine-tuned MASS model released by Microsoft", "ref_chunk": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}], "doc_text": "Inference with adversar- ial learning for end-to-end Text-to-Speech (VITS), which has shown a signi\ufb01cant performance gain over Tacotron2 and Transformer TTS in both sub- jective and objective evaluation (Kim et al., 2021; Hayashi et al., 2021), and use the UASR system mentioned in section 3.1 to generate pseudo labels for training. 3.2 Mitigating Error Propagation Denoising back-translation. In our setting, the input of our translation model comes from the out- put of UASR, which might contain some noise, and worsen the performance signi\ufb01cantly. In this paper, we introduce denoising back-translation, a novel approach for robust UNMT. Given a source sentence x \u2208 S, a target sentence y \u2208 T , and u\u2217(\u00b7), v\u2217(\u00b7) are translation functions with directions of S \u2192 T and T \u2192 S respectively. we generate the pseudo parallel data by passing x and y through a noise function f (\u00b7), transcribed them into u\u2217(f (x)) \u2208 T and v\u2217(f (y)) \u2208 S re- spectively. f (\u00b7) can be some arti\ufb01cial data aug- mentations including deletion, insertion, or other 4 Figure 2: The illustration of our proposed method: DBT. modules like a language model. The objective of denoising back-translation is to reconstruct x and y from u\u2217(f (x)) and v\u2217(f (y)) respectively. The denoising back-translation loss is as follows: LDBT =Ex\u2208S[\u2212 log PT \u2192S(x|u\u2217(f (x)))] +Ey\u2208T [\u2212 log PS\u2192T (y|v\u2217(f (y)))] Unlike the original back-translation, the pseudo labels of DBT are generated from noisy sentences, so the model should learn to transcribe the noisy pseudo sentences into clean sentences, and thus become more robust. The whole process of DBT was illustrated in Figure 2. 4 Experiments 4.1 Data To demonstrate our US2ST systems across differ- ent languages, we evaluate our S2ST results on CVSS, which is a multilingual corpus built on top of CoVoST 2 (Wang et al., 2020b) and Common- Voice ver.4 (CV4; Ardila et al., 2019). Thus, we are also available to evaluate our S2TT results on CoVoST 2 and ASR results on CV4. However, we do not utilize any paired data from the corpus during training; instead, we use audio and text data from different corpora, con- structing an unpaired scenario for our US2ST. For audio, we adopt Common Voice ver.4 for UASR and LJspeech (Ito and Johnson, 2017) for UTTS without using any transcriptions from them; and for text, we extract sentences from Wikipedia4, WMT\u201914, CC100 (Conneau et al., 2019), and Lib- riSpeech LM data5. All of the data we used is open-sourced and public-available. 4We extract the data from wiki using WikiExtractor (At- tardi, 2015) 5Following Liu et al. (2022b), we exclude the transcrip- tions of LJspeech to form fully unpaired scenario (1) 4.2 System setups UASR We used audio from CV4 and text data from Wikipedia to train our UASR models. More precisely, we use 100 hours of audio and about 1\u20133M sentences for each language. After train- ing, we evaluate the results with the transcriptions from CV4. For the pre-trained W2V2 model, we directly use the cross-lingual version (XLSR; Con- neau et al., 2020) without \ufb01netuning. XLSR had pre-trained in many different languages thus suit- ing our needs for training UASR in languages other than English. During preprocessing, we adopted the same con- \ufb01guration in wav2vec-U, except for the silence in- sertion rate of French. We found that our French model converged better when <SIL> token inser- tion rate is 0.5 instead. As for the GAN training con\ufb01guration, we chose the coef\ufb01cients of the loss function according to the original paper as follows: the gradient penalty weight \u03bb = 1.5 or 2.0, the smoothness penalty weight \u03b3 = 0.5, and the phoneme diversity loss weight \u03b7 = 4. We trained 3 seeds for each con\ufb01gu- ration, conducting 6 models for each language. TDN For the text denormalizer (TDN), we adopted transformer encoder-decoder architecture, both of which contained 4 layers of transformer blocks. We constructed the input data by normaliz- ing 8\u201310M of plain text data from CC100, and the objective is to reconstruct the unnormalized data. UNMT We used the back-translation \ufb01ne-tuned MASS model released by Microsoft6 to initialize our German-English and French-English UNMT models. For the Spanish-English UNMT model, we followed the same pretraining and \ufb01ne-tuning steps as the standard MASS model. For denoising back-translation as discussed in Section 3.2, the arti\ufb01cial noise f (\u00b7) included ran- dom drop, substitution, and insert, whose proba- bility were 0.05, 0.01, and 0.05 respectively. We continually \ufb01ne-tune the model by denoising back- translation loss plus denoising autoencoding loss to build a robust UNMT model. UTTS In this paper, we conducted single-speaker UTTS, so we trained a UASR model on the audio of LJSpeech (Ito and Johnson, 2017). Using the UASR-generated pseudo labels, we then proceeded with the training of VITS7, but the transcription of 6https://github.com/microsoft/MASS 7https://github.com/jaywalnut310/vits 5 LJSpeech was replaced by the pseudo label. 4.3 Supervised cascaded S2ST First, we constructed our upper bound model supervised cascaded S2ST by training a (ASR\u2192MT\u2192TTS) which shares similar model architecture with our US2ST. For ASR, we \ufb01netuned the whole XLSR instead of treating it as a feature extractor. We adopted letter-based training and followed the con\ufb01guration from fairseq (Ott et al., 2019). The amount of au- dio data was exactly the same as those in UASR. Furthermore, we \ufb01netuned the XLSR models in- dividually for each language. MT is achieved by training the same initial model with UNMT, but the training data were the transcriptions of CoVoST 2 plus CC100. We supervised trained with CoV- oST 2, and CC100 was used for back-translation training, which can boost the performance of super- vised machine translation models. Finally, instead of training on pseudo labels from UASR, the su- pervised TTS model directly uses the reference phoneme sequences and their corresponding utter- ance audio as paired data. By constructing cascaded supervised S2ST, we can discuss the performance individually for each component. 4.4 Evaluation The evaluation metric of ASR was normalized word error rate (WER), which removed all punctu- ation marks and converted all characters to lower- case. We"}