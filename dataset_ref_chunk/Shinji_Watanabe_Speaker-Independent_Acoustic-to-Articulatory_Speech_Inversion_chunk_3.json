{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Speaker-Independent_Acoustic-to-Articulatory_Speech_Inversion_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the improvement in performance on the HPRC dataset?", "answer": " 12.5%", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " By how much does the model outperform the SOTA model on MOCHA-TIMIT?", "answer": " 0.176 correlation", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " What type of features yield the largest improvement in performance for multi-speaker AAI tasks?", "answer": " HuBERT", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " Why do the authors attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models?", "answer": " Due to HPRC having more hours of speech and potentially higher-quality EMA labels", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " What is the phoneme classification accuracy on MOCHA?", "answer": " 0.682", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " What metric is used to compute the average L1 distances between predicted and ground truth tract variables?", "answer": " L1 distance", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " What error was corrected in the ICASSP 2023 version for MOCHA results?", "answer": " preprocessing error", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " What method was used to obtain phoneme alignments?", "answer": " Montreal Forced Aligner", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " Which phonemes did the model noticeably outperform the baseline on?", "answer": " nasals (M, N, NG) and liquids (L, R)", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}, {"question": " What evaluation metric is used to compare inversion performance in resynthesis analysis experiments?", "answer": " DTW-MCD", "ref_chunk": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}], "doc_text": "SOTA performance on the HPRC dataset by 12.5% and outperform the SOTA model by 0.176 correlation on MOCHA-TIMIT. Switching from MFCC in- puts to HuBERT ones yields the largest improvement, sug- gesting that self-supervised features are better than spectral ones for multi-speaker AAI tasks. We attribute the lower performance on MOCHA-TIMIT compared to HPRC for all models due to HPRC having more hours of speech and po- tentially higher-quality EMA labels. This performance dif- ference is probably not due to phoneme label quality, as the phoneme classification accuracy on MOCHA, 0.682, is com- parable to that on HPRC, 0.673. To further study which types of inputs our model con- tributes improved inversions, we compare the baseline with our proposed method at the phoneme level. Specifically, we calculate the average L1 distance between the predicted and ground truth tract variables, defined in Section 3.1, in our 3MOCHA results are updated from ICASSP 2023 version after correcting a preprocessing error. [15] Ours Diff. AA .206 .175 .031 AO .214 .182 .032 AW ER .211 .229 .168 .187 .042 .042 F .208 .163 .045 K .222 .193 .029 L .222 .177 .045 M .216 .176 .041 N .195 .163 .032 NG OY .226 .244 .191 .213 .035 .031 P .206 .178 .028 R .206 .173 .033 TH .207 .179 .028 V .198 .165 .033 Table 2. Average L1 distances (\u2193) between estimated and ground truth TVs per phoneme. Phonemes listed here have the largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1. Model Baseline [15] Ours AWB 9.23 8.03 SLT 10.24 9.69 Average 10.94 \u00b1 0.76 9.52 \u00b1 0.59 Table 3. MCDs (\u2193) in resynthesis analysis experiments on ARCTIC speakers AWB and SLT, as well as the average and standard deviation across all speakers. Fig. 3. Predicted normalized lip aperture (LA) for our ap- proach (blue) and the baseline (green) across 18 ARCTIC speakers for each vowel. HPRC test set for each phoneme. We obtain phoneme align- ments with the Montreal Forced Aligner [38] and use the L1 distance metric since it can be computed at the frame level, e.g., as opposed to a correlation-based metric. Table 2 sum- marizes these results. We observe that our model outperforms the baseline noticeably on nasals (M, N, NG) and liquids (L, R). One potential reason for this is that modelling improve- ments may help compensate for the lack of voicing and velar information in EMA. Generally, we observed that our model performs better than the baseline across all phonemes, and we encourage readers to see the supplementary material for all phoneme results. evaluating synthesis quality. To study performance on un- seen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, con- sistent with our inversion results in Table 1 and earlier single- speaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articu- latory synthesis direction, we plan to improve such models and continue validating inversion performance in this man- ner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION 5.2. Inversion Analysis without EMA Labels Figure 3 contains the predicted normalized lip aperture (LA) for our approach and the baseline across 18 ARCTIC speak- ers for each vowel. As mentioned in Section 2.4, all of these speakers are unseen during training. Both approaches are able to generally predict the biologically plausible LA val- ues. For example, the lip aperture, visualized in Figure 1, is predicted to be wide for the \u201dAE\u201d sound and narrow for the \u201dUW\u201d sound. However, our model is much more consistent across speakers, as evinced by the lower variance. We note that this lower variance is consistent with the fact that the TVs are normalized and much less speaker dependent than EMA features [13]. Since the baseline incorrectly estimates lip aperture for a larger number of speakers, this suggests that our method is better at generalizing to unseen speakers. In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this eval- uation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS 5.3. Resynthesis Analysis without EMA Labels We can also compare inversion performance through resyn- thesizing speech from estimated features (Section 4.2) and This research is supported by the following grants to PI Anumanchipalli \u2014 NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Founda- tion. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., \u201cCharacteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,\u201d Journal of Communi- cation Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., \u201cNeural analysis and syn- thesis: Reconstructing speech from self-supervised representa- tions,\u201d NeurIPS, 2021. [3] A. Polyak et al., \u201cSpeech Resynthesis from Discrete Disentan- gled Self-Supervised Representations,\u201d in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech synthesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [5] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?,\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthe- sizer for perceptual research,\u201d The"}