{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Pragmatic_Inference_with_a_CLIP_Listener_for_Contrastive_Captioning_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What approach is used to iteratively generate captions with the pragmatic speaker S1?", "answer": " Beam search with beam width B", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " Why is it computationally infeasible to obtain the exact solution to Equation 3?", "answer": " Because it requires encoding all possible next partial captions with CLIP at each step", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " How are candidate next partial captions obtained at each step of decoding?", "answer": " Via beam search from the base speaker distribution PS0", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " What criteria are used to evaluate PICL and competitive baseline methods?", "answer": " Informativeness and fluency", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " How is the performance of pragmatic models evaluated for informativeness?", "answer": " By evaluating the performance of pragmatic models with an evaluating listener Leval", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " How is the discriminativeness of the method being evaluated quantified?", "answer": " By the retrieval accuracy of Leval with method-generated captions as input", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " How is the fluency of generated captions scored?", "answer": " With the perplexity (PPL) under GPT-2", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " What is used as the base listener in PICL?", "answer": " CLIP-zero-shot", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " What architecture is used for the evaluative listener in automatic evaluations of informativeness?", "answer": " ALBEF vision-language model", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}, {"question": " Why is an explicit listener model used to guide utterance generation in PICL?", "answer": " To mitigate the codebooking issue in evaluation", "ref_chunk": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}], "doc_text": "captions. 3.3 Decoding with Approximation To iteratively generate captions with the pragmatic speaker S1, we perform beam search with beam width B, which involves solving arg max ot PS1(ot|o<t, i+, I) for each beam item. However, it is computation- ally infeasible to obtain the exact solution to Equa- tion 3 since it requires encoding all #(vocabulary size) possible next partial captions with CLIP to calculate PL0 at each step. Thus, we adopt a sub- sampling approach similar to Andreas and Klein (2016); Fried et al. (2018). At each step of decod- ing, a subset of N (N > B) candidate next partial captions o1:T are obtained via beam search from the base speaker distribution PS0, and these N candi- dates are rescored with Equation 2 to approximate Equation 3. Finally, only the top B candidates after rescoring are retained to continue with. 4 Experimental Setup We evaluate PICL on ImageCoDe (Krojer et al., 2022), a dataset originally designed for image re- trieval with contextual descriptions. Given the high visual similarity of the images in each problem in the dataset, we adopt it as a challenging testbed for discriminative captioning. We evaluate PICL and competitive baseline methods on two criteria, infor- mativeness and fluency, using both automatic and human evaluation. For informativeness, we follow previous work (Cohn-Gordon et al., 2018; Newman et al., 2020) to automatically evaluate the perfor- mance of pragmatic models with an evaluating lis- tener Leval. The discriminativeness of the method being evaluated is quantified by the retrieval accu- racy of Leval with method-generated captions as input. For fluency, we score the well-formedness of (2) (3) generated captions with the perplexity (PPL) under GPT-2 (Radford et al., 2019). In addition to the automatic evaluation, we con- duct human evaluation where annotators are tasked to a) retrieve the target image given the caption and b) score the fluency of the caption. 4.1 Dataset We use sets of images collected in ImageCoDe (Krojer et al., 2022) to evaluate the proposed ap- proach. Each image set in ImageCoDe consists of 10 visually similar images. The image sets are col- lected in two categories: static pictures and video frames. A random subset of images per set is se- lected as targets, for which human annotators write discriminative captions that are retained if other hu- mans can successfully use it to retrieve the target. In our experiments, we use the validation split of ImageCoDe for hyper-parameter selection and eval- uate model performance on the test split. The valid and test sets contain 1,039 and 1,046 sets of im- ages and 2,302 and 2,306 human written captions, respectively. Table 1 shows the retrieval performance of sev- eral models on ImageCoDe test split, where CLIP- zero-shot is the base listener used in PICL and ALBEF-finetuned is the evaluating listener used for automatic evaluation (see Section 4.2). Given the large performance gap of all models between static and video subsets, we believe the video frames are too challenging for current neural mod- els to make pragmatic and contextual inferences for both captioning and retrieving. Therefore, we use only static images in our experiments. 4.2 Automatic Evaluation Informativeness Following Cohn-Gordon et al. (2018) and Newman et al. (2020), we evaluate the informativeness of captions generated by our method and baselines using a listener test: whether an evaluative listener model could identify the tar- get image out of the distractors, given generated captions. However, an evaluative listener can only be an imperfect proxy for human listeners, and past work has found that utterances that are in- formative to an evaluative listener model can be uninterpretable to people, a phenomenon known as codebooking (Kim et al., 2019) or language drift (Lazaridou et al., 2020). This issue is par- ticularly likely to complicate evaluation in a prag- matic framework like ours, where an explicit lis- tener model (a frozen CLIP model, in our PICL All Video Static CLIP-zero-shot CLIP-finetuned-best ALBEF-finetuned 22.4 29.9 33.6 15.6 22.0 22.7 47.8 59.8 74.2 Table 1: Retrieval accuracy on ImageCoDe test split with human-written contextual captions as input. In the proposed method, we use CLIP-zero-shot as the base listener and ALBEF-finetuned as the listener for evaluation. CLIP-finetuned denotes the best-performing model in previous work. The fine-tuned ALBEF outper- forms the best CLIP model with a large margin on static images while improving slightly on video frames. Com- paring with performances on static images, all models struggle on video frames. approach) is used to guide utterance generation. To mitigate this codebooking issue in evaluation, past work has made the evaluative listener dissim- ilar by training it on separate data (Cohn-Gordon et al., 2018; Kim et al., 2019; Fried et al., 2021); we additionally use a separate architecture for the evaluative listener, dissimilar from our CLIP lis- tener: the ALBEF vision-language model (Li et al., 2021). We finetune ALBEF on the human-written contextual captions for the retrieval task in Image- Code.4 As shown in Table 1, finetuned ALBEF outperforms the best-performing retrieval model from previous work (Krojer et al., 2022) on Im- ageCoDe with human-written captions, so we use ALBEF-finetuned as our evaluating listener in au- tomatic evaluations of informativeness. Fluency While being informative, discrimina- tive captions should also be natural and fluent. Therefore, we additionally perform automatic eval- uations of the fluency of generated captions by computing their perplexity using a GPT-2 language model (Radford et al., 2019). 4.3 Human Evaluation Recent analysis on ImageCode (Dess\u00ec et al., 2022) and in other reference game settings (Lazaridou et al., 2020) reveals that utterances generated by neural models can be discriminative enough for other neural models to retrieve the target image while being misleading to humans. This implies that the performance of a neural retriever evalu- ative listener (e.g., ALBEF) on model-generated captions might not correctly reflect the degree of informativeness of the captions from a human\u2019s 4Specifically, we finetuned the refcoco-checkpoint con- trastively, i.e. with the 9 distractors in the same batch. perspective. Therefore, we further conduct a hu- man evaluation for PICL and baseline methods on"}