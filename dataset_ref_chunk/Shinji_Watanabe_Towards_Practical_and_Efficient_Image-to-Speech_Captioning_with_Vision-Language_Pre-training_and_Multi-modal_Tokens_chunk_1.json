{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Towards_Practical_and_Efficient_Image-to-Speech_Captioning_with_Vision-Language_Pre-training_and_Multi-modal_Tokens_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the proposed Im2Sp model?", "answer": " The proposed Im2Sp model focuses on building a powerful and efficient Image-to-Speech captioning model.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " How does the Im2Sp model incorporate language modeling capability?", "answer": " The Im2Sp model incorporates language modeling capability by setting the output as discretized speech units derived from a self-supervised speech model.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " What is the purpose of converting the original image into image units in the Im2Sp model?", "answer": " The purpose of converting the original image into image units is to drastically reduce the required data storage for saving image data.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " Why is the performance of the Im2Sp model lower compared to image captioning?", "answer": " The performance of the Im2Sp model is lower compared to image captioning due to the weak supervision of speech regression and limited training data.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " What is the main goal of multi-modal language technologies?", "answer": " The main goal of multi-modal language technologies is to associate image and speech for jointly understanding the content.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " How can Im2Sp technology enhance daily experiences for individuals with visual impairments?", "answer": " Im2Sp technology can enhance daily experiences for individuals with visual impairments by providing audible descriptions of images, such as narrating traffic signs on roads for a comprehensive understanding.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " What is the main challenge in developing an end-to-end Im2Sp model?", "answer": " The main challenge in developing an end-to-end Im2Sp model is the weak supervision of speech regression in comprehending visual input.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " What approach does the paper propose for improving the performance of the Im2Sp model?", "answer": " The paper proposes transferring the rich knowledge of a pre-trained vision-language model to the Im2Sp model to improve performance.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " What is the significance of incorporating vision-language pre-training in the Im2Sp model?", "answer": " Incorporating vision-language pre-training in the Im2Sp model significantly enhances its performance and reduces data storage and computational memory costs.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}, {"question": " What are the major contributions of the paper?", "answer": " The major contributions of the paper include achieving state-of-the-art performances in Im2Sp, exploring image token-to-speech token translation framework, and conducting comprehensive experiments for evaluation.", "ref_chunk": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}], "doc_text": "3 2 0 2 p e S 5 1 ] V C . s c [ 1 v 1 3 5 8 0 . 9 0 3 2 : v i X r a TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS Minsu Kim1, Jeongsoo Choi1, Soumi Maiti2, Jeong Hun Yeo1, Shinji Watanabe2, Yong Man Ro1\u2217 1Integrated Vision and Language Lab, KAIST, South Korea 2Language Technologies Institute, Carnegie Mellon University, USA {ms.k,jeongsoo.choi,sedne246,ymro}@kaist.ac.kr, {smaiti,swatanab}@andrew.cmu.edu ABSTRACT tures (e.g., Mel-spectrogram) similar to that of ground-truth speech, may prevent the model from focusing on the image content [4, 5]. In this paper, we propose methods to build a powerful and effi- cient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehen- sion and language modeling from a large-scale pre-trained vision- language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech fea- tures of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteris- tics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ. These days, discretized speech unit [6] has drawn big atten- tion with its significant potential in diverse tasks such as speech-to- speech translation [7\u20139], spoken language understanding [10, 11], speech synthesis [12, 13], and speech recognition [14\u201316]. The speech units can be obtained by quantizing speech features derived from self-supervised speech models. Since they are discrete and can be generated to exclusively encapsulate linguistic factors (i.e., phoneme) [6, 9, 10], speech units can serve as pseudo-text. By utilizing the pseudo-text characteristics of the speech unit, one can build an end-to-end Im2Sp model by guiding the model with strong discrete supervision (e.g., classification) instead of using a regres- sion criterion. Nevertheless, the performance of the Im2Sp model remains notably lower than that of image captioning, making it in- adequate for practical real-world utilization. Since acquiring paired data of images and human spoken speech is challenging, the limited training data makes it difficult for models to learn how to compre- hend images and convert them into speech descriptions. As jointly understanding the image and speech is one of the key elements in developing multi-modal language technologies [17\u201319], it is impor- tant to devise an approach for associating the image and speech even when faced with limited image-speech paired data. Index Terms\u2014 Image-to-speech captioning, Image-to-speech synthesis, Multi-modal speech processing, Multi-modal tokens 1. INTRODUCTION Directly synthesizing a speech description for an image holds sub- stantial promise in enhancing people\u2019s daily experiences. By nar- rating traffic signs on roads through generated speech descriptions, individuals with visual impairments can gain a comprehensive un- derstanding of their immediate surroundings and route. Thereby, they can make informed decisions for their safe journey. Moreover, the capability to audibly check the image messages, even while en- gaged in activities like driving, can positively impact our daily rou- tines. This Image-to-Speech captioning (Im2Sp) technology [1] can be viewed as an audio counterpart of image captioning [2] that pre- dicts textual sentences describing input images. Despite the potential benefits of Im2Sp, the technology has not been well-addressed com- pared to image captioning. Different from text-based image caption- ing, developing an end-to-end Im2Sp model is regarded as a chal- lenging problem, due to the weak supervision of speech regression in comprehending the visual input [3, 4]. As speech contains not only linguistic information but also various irrelevant factors (e.g., speaker characteristics, duration, noises) to the input image, guiding the model with regression criteria that force to produce speech fea- In this paper, we focus on improving the performance of an end- to-end Im2Sp model. To this end, we investigate whether the rich knowledge of image understanding and language generation of a large-scale pre-trained vision-language model [20, 21] can be trans- ferred to Im2Sp. Then, we show that even if the vision-language model is pre-trained with image-text modalities instead of speech, we can significantly improve the performance of Im2Sp by incorpo- rating its pre-trained knowledge. Furthermore, we explore how we can enhance the efficiency of the Im2Sp model. Similar to the speech unit case, we quantize the input image into image units. Concretely, we tokenize the input image into image units by applying Vector Quantization (VQ) technique of ViT-VQGAN [22, 23]. With the to- kenized inputs, our Im2Sp problem becomes a translation between multi-modal tokens like language translation [9]. In this setup, both the input and output are discrete, enabling efficient model training and economic data storage management. The image units reduce the required bits more than 100 times compared to the original raw im- age. We show that with the vision-language pre-training strategy, we can still achieve reasonable Im2Sp performances while effectively reducing the required data storage and computational memory costs. The major contributions of this paper can be summarized as fol- lows: 1) This is the first work exploring vision-language pre-training in Im2Sp. By employing the vision-language pre-trained image en- coder and text decoder in our Im2Sp framework, we achieve state- Corresponding Author. of-the-art performances, demonstrating a significant performance margin compared to previous methods on two popular benchmark databases, COCO [24] and Flickr8k [25]. 2) This is the first work in- vestigating the image token-to-speech token translation framework with NLP-like processing of multi-modality, which can greatly reduce the required data storage. 3) Through comprehensive ex- periments including caption quality evaluations, human subjective evaluation, and"}