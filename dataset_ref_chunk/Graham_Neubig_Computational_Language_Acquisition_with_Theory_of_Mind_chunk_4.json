{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Computational_Language_Acquisition_with_Theory_of_Mind_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What features are computed to determine the similarity between images and captions in the study?", "answer": " Visual Cosine Similarity, Textual Cosine Similarity, and Visual+Textual Similarity", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " How are the textual similarities of captions calculated in the study?", "answer": " By taking the cosine similarity of vector representations of the text", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " What is the purpose of using term frequency \u2013 inverse document frequency (TF-IDF) in the study?", "answer": " To weight the vector for each token in the captions to upweight rarer content words", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " What is the significance of the caption weight parameter (wc) in the study?", "answer": " It is used to compute a weighted average of image and caption similarity in cases where both are used, with the aim of capturing distractors with both visual and semantic similarities", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " What are some of the metrics used to evaluate the quality of the learned language speakers in the study?", "answer": " Fluency, average length, F1 score for salient parts of speech, and caption quality scores", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " How is the fluency score of the learned language speakers defined in the study?", "answer": " It is defined as the average gain in log probability when moving from a unigram model to a pretrained language model", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " What additional performance metrics are considered in the study besides fluency and accuracy?", "answer": " BLEU score, ToM Accuracy, and part-of-speech statistics", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " What does ToM Accuracy measure in the study?", "answer": " How often the candidate image that the Theory of Mind (ToM) listener believes has the highest probability of being chosen is the actual listener\u2019s choice", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " How are parts of speech for words in speaker utterances and ground-truth captions tagged in the study?", "answer": " They are tagged with spaCy", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}, {"question": " From where are the images and captions sourced in the study?", "answer": " They are drawn from the MS COCO dataset introduced in Lin et al. (2014)", "ref_chunk": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}], "doc_text": "kitten next to a blue fence. two sheep standing next to each other in the snow. a woman hugs a gray cat to her chest. Image Similarity Caption Similarity Hybrid Similarity a little dog sitting on a wooden bench. Original Image a kitten that is sitting down by a door. Random Distractor Published as a conference paper at ICLR 2023 Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings. Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target. Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency \u2013 inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words. Visual+Textual Similarity. In cases where both image and caption similarity were used, we added a caption weight parameter wc \u2208 [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities. We also train models on randomly selected, or \u201ceasy\u201d, distractors to create a baseline to study the effects of distractor dif\ufb01culty. Examples of a selection of the distractor settings are shown in Fig. 2. 6 EXPERIMENTAL SETUP AND RESULTS 6.1 EXPERIMENTAL SETUP We focus on two main results: A speaker\u2019s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so. To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers\u2019 learned languages using \ufb02uency, average length, F1 score for salient parts of speech, and caption quality scores. We use a \ufb02uency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is de\ufb01ned as the average gain in log probability when moving from a unigram model to a pretrained language model. \ufb02uency = 1 |u| (ln(pM (u)) \u2212 ln(pU (u))) We train a unigram model, pU , and \ufb01ne-tune GPT-2 large (Radford et al., 2019), pM , on our training set. In addition to \ufb02uency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, de\ufb01ned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener\u2019s choice. We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use 6 (12) Published as a conference paper at ICLR 2023 Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside. Distractor a group of men on a field playing baseball. Target a person on some skis. Distractor a white building with a clock on the front and side of it. Figure 3: Examples of output utterances generated by various speaker models. Here, the \u201cHard Distractors\u201d referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more dif\ufb01cult distractors and training with ToM leads to more accurate, \ufb02uent utterances. Table 1: Performance and language features of various ToM speakers. Model Performance POS F1 ToM Weight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard 0.81 0.81 0.92 0.20 0.24 1.00 1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00 0.52 0.58 1.00 0.41 0.46 1.00 Zero Normal High High RSA Hard Hard Hard Hard 0.83 0.85 0.88 0.87 0.26 0.26 0.27 0.28 1.99 2.25 2.23 2.26 0.81 0.88 0.89 0.93 0.22 0.22 0.22 0.23 0.64 0.65 0.66 0.65 0.49 0.52 0.52 0.50 Zero Normal High High RSA Easy Easy Easy Easy 0.85 0.88 0.88 0.89 0.25 0.26 0.27 0.29 1.73 2.09 2.07 1.91 0.85 0.91 0.91 0.94 0.21 0.21 0.22 0.17 0.57 0.64 0.65 0.65 0.48 0.50 0.51 0.52 this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech. Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup. 6.2 EFFECTS OF TOM We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a"}