{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Making_Scalable_Meta_Learning_Practical_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What issue can arise when obtaining meta gradients with noisy Hessian-vector products?", "answer": " Biased meta gradients may result in training instability.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " Why are the most efficient distributed training features designed for first-order gradients rather than higher-order gradients?", "answer": " They are designed for first-order gradients as they do not involve Hessian-vector products.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " What is the proposed solution for avoiding issues stemming from base Jacobian inversion?", "answer": " Approximating the base Jacobian with an identity matrix.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " In what setting does Jacobian-free backpropagation show that approximating the base Jacobian as an identity matrix can be understood as preconditioning the original meta gradient?", "answer": " Deep equilibrium model setting.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " What is the impact of using an adaptive optimizer in base optimization on the relationship between the base Jacobian and the Hessian?", "answer": " The base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " Why do existing implicit differentiation algorithms face challenges when the iterative solver for base optimization is an adaptive optimizer?", "answer": " The inconsistency between assumed and actual optimizers results in an incorrect meta gradient.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " What does the acronym SAMA stand for in the context of meta gradient computation in a distributed data parallel setting?", "answer": " SAMA stands for Solution Algorithm for Meta-Adaptation.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " How does the proposed algorithm SAMA handle adaptive update rules for any optimizer?", "answer": " It expands the meta Jacobian term with the chain rule to address adaptive update rules.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " What is the central idea behind the update rule u in the context of the algorithmic adaptation for any adaptive optimizer?", "answer": " The update rule u reduces to an identity matrix in the case of SGD.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}, {"question": " Why is a custom implementation of the backward function necessary for meta gradient algorithms like SAMA in distributed data parallelism?", "answer": " Most libraries like PyTorch only have native support for basic backward functions, not customized ones required by meta gradient algorithms.", "ref_chunk": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}], "doc_text": "access to a stochastic estimation of the Hessian due to mini-batch sampling [34]. Hence, meta gradients obtained with noisy Hessian-vector products can be biased, which may result in training instability. Finally, the most efficient distributed training features, such as communication-computation overlap [35], are designed for first-order gradients, rather than the higher-order gradients involved in Hessian-vector products. Solution A simple solution for avoiding the aforementioned issues stemming from base Jacobian inversion is to approximate the base Jacobian with an identity matrix as follows: \u2202Lmeta \u2202\u03bb = \u2212 \u2202u \u2202\u03bb (cid:18) \u2202u \u2202\u03b8\u2217 (cid:19)\u22121 \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 Under the deep equilibrium model setting, Jacobian-free backpropagation [17] shows that such an approximation can be understood as preconditioning the original meta gradient. Our approximation also resembles approximating the Hessian as an identity matrix in one-step unrolling techniques (i.e. T1 \u2212 T2) from [38, 41]. While this approximation is exact when the iterative solver u is naive SGD where u = \u2202Lbase , we note that the base Jacobian does not necessarily equate with the Hessian when an adaptive optimizer is used in base optimization (more detailed discussion on this issue is deferred to Sec. 3.2). Furthermore, their methods calculate the meta Jacobian at initialization \u03b8 instead of at convergence \u03b8\u2217 due to their close connection to iterative differentiation [42], and thereby inhibit unroll steps larger than 1, unlike our approach. Considering that a larger number of unroll steps allows for less frequent computations of expensive meta gradients, our implicit-differentiation-based derivation can lead to a further computational gain in large-scale meta learning. In Appendix E, we investigate the effect of this identity approximation in the \u201cbiased regression\u201d setting where the closed-form solution can be analytically calculated, and empirically show that the identity approximation still allows for accurate estimation of the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, even when the true base Jacobian is not an identity matrix. \u2202\u03b8 \u2202\u03bb 3.2 Algorithmic Adaptation for Adaptive Optimizers Problem Most existing implicit differentiation algorithms [21, 26, 40] compute the best-response Jacobian in Eq. (1) based on the assumption that the iterative solver u for base optimization is vanilla SGD, whereas recent large models, exemplified by Transformers [5, 62], are by default trained with adaptive optimizers, such as Adam [32]. While the fixed point condition can be theoretically identical for any gradient-based optimizer at convergence (i.e., \u2202Lbase \u2202\u03b8\u2217 = 0), researchers in practice approximate \u03b8\u2217 with a small number of gradient steps, at which the above fixed point condition would unlikely hold. Thus, the inconsistency between assumed and actual optimizers results in an incorrect meta gradient, which is a source of training instabilities and reduced performance in meta learning (as we demonstrate in Table 1). 4 \u2202\u03b8\u2217 (3) : local backward: global backward: manual backward: dataDevice 1Device 2Gradientsynchronizationw/ comm-compoverlap Figure 2: The overall workflow of meta gradient computation with SAMA in the distributed data parallel setting. In detail, SAMA consists of three first-order backward passes performed with the underlying automatic differentiation engine, and one manual backward pass for algorithmic adaptation for the adaptive optimizer. Gradient synchronization is performed only once in the last backward pass with communication-computation overlap to minimize the communication bottleneck. Solution To take adaptive update rules into account, we propose to further expand a meta Jacobian term in Eq. (3) with the chain rule as follows: =\u21d2 \u2202Lmeta \u2202\u03bb \u2202u \u2202\u03bb = \u2248 \u2212 \u2202gbase \u2202\u03bb \u2202u \u2202gbase = \u2202u \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 = \u2212 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 \u2202u \u2202gbase \u2202u \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 (4) where gbase is base-gradient computed at convergence (i.e. \u2202Lbase ). In short, we accomplish the \u2202\u03b8\u2217 algorithmic adaptation for any adaptive optimizer with the update rule u through the middle term \u2202u in Eq. (4), which reduces to an identity matrix in the case of SGD. To analyze the adaptation \u2202gbase cost, we note that parameter updates u in most optimizers are performed only with element-wise operations. Thus, the adaptation matrix is diagonal, for which computation/memory complexities are only O(nb). As a concrete example, we provide an adaptation matrix for the most popular Adam optimizer [32] in Appendix C. Furthermore, to reduce computation/memory complexities involving the costly second-order deriva- tive \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 , we instead perform the matrix-vector product with the central-difference method from DARTS [38] and the associative rule of matrix multiplication. All combined, we propose SAMA, a highly compute/memory efficient meta gradient algorithm for scalable meta learning, the formulation of which is as follows: \u2212 \u2202Lbase(\u03b8\u2212,\u03bb) \u2202\u03bb 2\u03f5 \u2202Lbase(\u03b8+,\u03bb) \u2202\u03bb (cid:18) \u2202u (cid:19) \u22022Lbase \u2202\u03bb\u2202\u03b8\u2217 \u00b7 where \u03b8\u00b1 = \u03b8\u2217 \u00b1 \u03f5v with the perturbation vector v = \u2202u and the step size \u03f5 = \u03b1 . \u2225v\u22252 We empirically observe that \u03b1 = 1.0 works well across a multitude of tasks without further tuning. Finally, we notice that prevoius work in penalty-based bilevel optimization (e.g. F2SA [34] and BOME [37]) further uses the direct gradient \u2202Lmeta explicitly in the base level optimization to \u2202\u03b8\u2217 maximize the performance of the final base parameter \u03b8\u2217 on the meta objective. Given that our perturbation vector v includes the direct gradient term, we also follow a similar strategy and update the base parameter \u03b8 in the direction of v (i.e. \u03b8t+1 = \u03b8t \u2212 \u03f5v) every time the meta update is performed. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 \u2248 \u2212 \u2248 \u2212 \u2202gbase \u2202Lmeta \u2202\u03b8\u2217 \u2202gbase (5) 3.3 Efficient Distributed Training & Implementation Problem In large-scale learning, distributed data parallelism (DDP), which communicates and synchronizes local gradients from each device before the parameter update, is necessary to improve both compute and memory efficiency. However, most automatic differentiation libraries like Py- Torch [46] only have native DDP support for their basic backward function, whereas SAMA, similar to other meta gradient algorithms, requires a custom implementation of the backward function, which consists of three basic backward passes as shown in Eq. (5).2 In addition, while a few meta learning 2Eq. (5) technically involves four derivatives in total, but the adaptation matrix \u2202u \u2202gbase can be"}