{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Grounding_Language_Models_to_Images_for_Multimodal_Generation_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the R@1 value achieved by the model when it is provided with only caption inputs?,answer: 2.4", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " What is the R@1 value achieved by the model when it is provided with 5 captions?,answer: 8.8", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " How does the model FROMAGe perform compared to CLIP under the same settings?,answer: FROMAGe performs significantly better than CLIP.", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " Why is CLIP considered unsuitable for dialogue?,answer: CLIP is unsuitable for dialogue because it does not have few-shot and in-context learning abilities.", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " What are the advantages of FROMAGe as an image-text model?,answer: FROMAGe is sensitive to complex language descriptions, multimodal context, and outperforms CLIP for longer input descriptions.", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " What task is FROMAGe evaluated on in the Visual Dialog (VisDial) setting?,answer: FROMAGe is evaluated on zero-shot Visual Dialog, specifically on image-and-text-to-text (IT2T) and text-to-image (T2I) tasks.", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " How does FROMAGe perform on the Visual Dialog task compared to other models like ESPER, CLIP, and ViLBERT?,answer: FROMAGe outperforms ESPER, CLIP, and ViLBERT on R@1, with a significant relative improvement over ESPER.", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " What is the relative improvement of FROMAGe compared to CLIP on the T2I retrieval task?,answer: FROMAGe achieves a 17.5% relative improvement over CLIP on R@1 in the T2I retrieval task.", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " What are some key differences between ESPER, Flamingo, and FROMAGe?,answer: ESPER and Flamingo are trained to generate text-only outputs, while FROMAGe is capable of image retrieval tasks.", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}, {"question": " Why is freezing the language model essential in retaining key abilities of FROMAGe?,answer: Freezing the language model is essential for retaining in-context learning and few-shot generalization abilities of FROMAGe.", "ref_chunk": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}], "doc_text": "than when only caption inputs are provided: it achieves a R@1 of 2.4, a significant decrease from the CLIP R@1 of 8.8 when it is provided with 5 captions. likely be- cause it is trained to condition on image-text inputs. These results are significantly worse than that of FROMAGe under the same settings. 4Previous versions of the paper had slightly worse scores due 5For these same reasons, CLIP is unsuitable for dialogue, and to a normalization bug. does not have few-shot and in-context learning abilities. 6 Grounding Language Models to Images for Multimodal Inputs and Outputs These results showcase the efficacy of FROMAGe as an image-text model sensitive to complex language descrip- tions and multimodal context (more analysis in Sec. 5.2). It is capable of parsing interleaved multimodal inputs, and strongly outperforms CLIP for longer input descriptions. As for free-form text generation, we also run human evaluations to evaluate the ability of FROMAGe to generate stories by learning in-context from VIST examples (Sec. 5.2). 4.2. Visual Dialogue We evaluate FROMAGe on zero-shot Visual Dialog (Vis- Dial) (Das et al., 2017). We test its ability to (1) select the correct text answer (from 100 candidates) for a question given an image and a conversation about it (image-and- text-to-text, IT2T, which is the standard VisDial task), and (2) retrieve the correct image given a conversation about it (text-to-image, T2I). Our results are summarized in Tab. 2. For IT2T, since FROMAGe is an autoregressive generative model, we compute the perplexity of each question and answer sequence, and select the option with the lowest per- plexity. FROMAGe outperforms ESPER (Yu et al., 2022b), CLIP (Radford et al., 2021), and ViLBERT (Lu et al., 2019) on R@1, improving by 20.5% relative to ESPER. FRO- MAGe also achieves a competitive Mean Reciprocal Recall (MRR) of 22.0 and Normalized Discounted Cumulative Gain (NDCG) of 16.5. This is substantially higher than ViL- BERT and CLIP, but worse than ESPER. We hypothesize that this is due to differences in training: ESPER uses rein- forcement learning and trains on MS-COCO (from which VisDial images are derived). Flamingo (Alayrac et al., 2022) is substantially better than all other zero-shot models, which we attribute to its larger model size (80B parameters, of which 10.2B are trainable), and larger training data of multi- modal webpages (43M webpages) and image-and-text data (1.8B pairs). In contrast, FROMAGe has 5M trainable pa- rameters and is trained on CC3M (3.1M image-text pairs). Our approach may also be applied to the Flamingo model (which uses a 70B language model backbone) to enable image retrieval, which is likely to improve overall capabili- ties and extend it to a greater variety of tasks. On the T2I retrieval task, FROMAGe significantly outperforms prior work, achieving a 17.5% relative improvement over CLIP on R@1. ESPER and Flamingo are trained to generate text-only outputs, and are hence incapable of this task. in-context to perform many different zero-shot and few-shot tasks. Many of the most interesting settings are those which produce interleaved images and texts as outputs, which prior work (Tsimpoukelli et al., 2021; Alayrac et al., 2022) is incapable of, or does not generate semantically meaning- ful outputs for (see appendix for further comparison with CM3 (Aghajanyan et al., 2022)). Our model is capable of holding multimodal dialogue conversations \u2014 processing input images and text and responding with coherent text and image outputs. It is able to refine input images by com- positing images and text concepts. FROMAGe also inherits the world knowledge of the frozen LLM, and can answer questions that require specific real world facts. 5. Analysis We analyze various aspects of FROMAGe to determine their effects on its overall capabilities. In all experiments, models were trained on CC3M for 24 hours on a single A6000 GPU. 5.1. Ablation Experiments We perform several ablation experiments to validate the design choices made in FROMAGe. We provided further details and results of various other ablations in the appendix. Freezing the LLM. We find that freezing the language model is essential to retaining in-context learning and few- shot generalization abilities. When finetuned, FROMAGe performs significantly worse on VIST and VisDial. Fine- tuning decreases retrieval performance on VIST (R@1 with full multimodal context decreases from 12.8 to 6.2) as well as VisDial text retrieval (R@1 from 14.6 to 1.0). Learning a dedicated retrieval token. As described in Sec. 3.2, we add a special [RET] token to represent em- beddings for retrieving images from text. When the model is trained without the [RET] token, R@1 performance on VIST (with full multimodal context) is significant worse. We observe that adding the [RET] token improves R@1 by a substantial 38.1% relative gain over the model without the [RET] token. We observe similar improvements across the board for other tasks. 5.2. The Effect of Context Our experiments demonstrate that FROMAGe achieves com- petitive results on zero-shot Visual Dialogue. We emphasize that unlike previous models, FROMAGe can output inter- leaved image-and-text content, and is a more general model. 4.3. Qualitative Results We also share selected examples covering various interac- tion settings in Fig. 3. FROMAGe is capable of learning Multimodal context helps. Since FROMAGe can pro- cess interleaved image-text data, a natural question is on the effect of image context compared to text context. To quantify these effects, we run ablations (Fig. 5, top) varying the number of captions and images provided to the model. We measure the recall of retrieving the correct image con- ditioned on the provided context for VIST dataset (Huang et al., 2016). Increasing context from 1 to 5 captions substan- tially improves the model: R@1 increases by 5.3% relative 7 Grounding Language Models to Images for Multimodal Inputs and Outputs IT2T T2I Model Trainable Params Finetuning Data NDCG MRR R@1 R@5 R@10 R@1 R@5 R@10 ViLBERT (Lu et al., 2019) CLIP ViT-L/14 (Radford et al., 2021) Flamingo (Alayrac et al., 2022) ESPER (Yu et al., 2022b) FROMAGe (ours) 114M 300M 10.2B 4M 5.5M 3.1M 400M 1.8B 0.5M 3.1M 11.6 10.9 52.0 22.3 16.5 6.9 8.5"}