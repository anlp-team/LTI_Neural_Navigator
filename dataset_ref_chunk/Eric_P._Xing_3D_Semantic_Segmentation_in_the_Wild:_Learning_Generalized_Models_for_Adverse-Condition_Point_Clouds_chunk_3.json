{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_3D_Semantic_Segmentation_in_the_Wild:_Learning_Generalized_Models_for_Adverse-Condition_Point_Clouds_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What challenges did the existence of invalid regions pose in the annotation process?", "answer": " The existence of invalid regions posed challenges in the annotation process by making point-wise annotation more difficult.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " How did annotators achieve reliable high-quality annotations in SemanticSTF?", "answer": " Annotators achieved reliable high-quality annotations in SemanticSTF by leveraging 2D camera images, Google Street views, and cross-checking their initial annotations.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " How long did it take on average for an annotator to label a LiDAR scan in downtown areas with complex scenery?", "answer": " On average, it took an annotator 4.3 hours to label a single LiDAR scan in downtown areas with complex scenery.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " Why were points with indiscernible semantic contents caused by adverse weather labeled as invalid?", "answer": " Points with indiscernible semantic contents caused by adverse weather were labeled as invalid to differentiate them from the 20 semantic categories used for labeling.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " What were the most frequently occurring classes in the SemanticSTF annotations?", "answer": " The most frequently occurring classes in the SemanticSTF annotations were road, sidewalk, building, vegetation, and terrain.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " What were the characteristics of point clouds captured under adverse weather conditions according to the text?", "answer": " Point clouds captured under adverse weather conditions exhibited properties such as snow coverage, rain-induced noise points, reduced working range due to fog, and similarities to normal-weather point clouds.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " Why is class imbalance an issue in SemanticSTF and other benchmarks?", "answer": " Class imbalance is an issue in SemanticSTF and other benchmarks due to various object sizes and unbalanced distribution of object categories in transportation scenes.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " What was the purpose of SemanticSTF in the context of 3DSS benchmarks?", "answer": " The purpose of SemanticSTF was to provide the first large-scale adverse-weather 3DSS benchmark with high-quality point-wise annotations for universal 3DSS research under all weather conditions.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " Why is SemanticSTF considered important for training universal 3DSS models?", "answer": " SemanticSTF is considered important for training universal 3DSS models because it introduces domain shifts from normal-weather point clouds, highlighting the need for models that can perform well under all weather conditions.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}, {"question": " What is the problem definition addressed by PointDR, as mentioned in the text?", "answer": " PointDR addresses the problem of domain generalization for semantic segmentation of LiDAR point clouds under all weather conditions by training a generalizable segmentation model from normal-weather point clouds to work well for adverse-weather point clouds in SemanticSTF.", "ref_chunk": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}], "doc_text": "snow cover) that make it difficult to identify the ground type. The exis- tence of such invalid regions makes point-wise annotation even more challenging. We designed a customized labeling pipeline to handle the annotation challenges while performing point-wise an- notation of point clouds in SemanticSTF. Specifically, we first provide labeling instructions and demo annotations and train a team of professional annotators to provide point- wise annotations of a set of selected STF LiDAR scans. To achieve reliable high-quality annotations, the annotators leverage the corresponding 2D camera images and Google Street views as extra references while identifying the cate- gory of each point in this initial annotation process. After that, the annotators cross-check their initial annotations for identifying and correcting labeling errors. At the final stage, we engaged professional third parties who provide another round of annotation inspection and correction. Annotation of SemanticSTF is a highly laborious and time-consuming task. For instance, while labeling down- town areas with the most complex scenery, it took an anno- tator an average of 4.3 hours to label a single LiDAR scan. Labeling a scan captured in a relatively simpler scenery, such as a highway, also takes an average of 1.6 hours. In ad- dition, an additional 30-60 minutes are required per scan for verification and correction by professional third parties. In total, annotating the entire SemanticSTF dataset takes over 6,600 man-hours. While annotating SemanticSTF, we adopted the same set of semantic classes as in the widely-studied semantic seg- mentation benchmark, SemanticKITTI [2]. Specifically, we annotate the 19 evaluation classes of SemanticKITTI, which encompass most traffic-related objects in autonomous driv- ing scenes. Additionally, following [37], we label points with indiscernible semantic contents caused by adverse weather (e.g. ground covered by snowdrifts) as invalid. Fur- bicyclisttraffic sign 10! 10# 10\" roadsidewalkparkingother-groundbuildingfencevegetationtrunkterraincarbicycletruckpersonpoleinvalidflatconstructionnaturevehiclehumanobject motorcycleother vehiclemotorcyclist Figure 2. Number of annotated points per class in SemanticSTF. thermore, we label points that do not belong to the 20 cat- egories or are indistinguishable as ignored, which are not utilized in either training or evaluations. Detailed descrip- tions of each class can be found in the appendix. 3.4. Data Statistics SemanticSTF consists of point-wise annotations of 21 semantic categories, and Fig. 2 shows the detailed statistics of the point-wise annotations. It can be seen that classes road, sidewalk, building, vegetation, and terrain appear most frequently whereas classes motor, motorcyclist, and bicyclist have clearly lower occurrence frequency. Such class imbalance is largely attributed to the various object sizes and unbalanced distribution of object categories in transportation scenes, and it is also very common in many existing benchmarks. Overall, the statistics and distribu- tion of different object categories are similar to that of other 2D and 3D semantic segmentation benchmarks such as Cityscapes [8], ACDC [37], and SemanticKITTI [2]. To the best of our knowledge, SemanticSTF is the first large-scale adverse-weather 3DSS benchmark that provides high-quality point-wise annotations. Table 1 compares it with several existing point cloud datasets that have been widely adopted for the study of 3D detection and semantic segmentation. We can observe that existing datasets are ei- ther collected under normal weather conditions or collected for object detection studies with bounding-box annotations only. 3DSS benchmark under adverse weather is largely blank, mainly due to the great challenge in point-wise an- notations of adverse-weather point clouds as described in previous subsections. From this sense, SemanticSTF fills up this blank by providing a large-scale benchmark and test bed which will be very useful to future research in universal 3DSS under all weather conditions. 3.5. Data illustration Fig. 3 provides examples of point cloud scans captured under adverse weather conditions in SemanticSTF (in row 1) as well as the corresponding annotations (in row 2). Compared with normal-weather point clouds, point clouds captured under adverse weather exhibit four distinct prop- erties: 1) Snow coverage and snowflakes under snowy weather introduce many white points (labeled as \u201cinvalid\u201d) as illustrated in Fig. 3(a). The thick snow coverage may lead to object deformation as well; Rainy conditions may cause specular reflection of laser signals from water on the ground 4 Dataset #Cls Type Annotation Fog Rain Snow \u2717 real bounding box \u2717 \u2717 real bounding box \u2717 real bounding box \u2717 \u2717 real bounding box \u2713 \u2713 \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 real \u2717 \u2717 SynLiDAR [51] 32 synth. \u2713 \u2713 real 8 nuScenes [5] 23 4 Waymo [41] 5 STF [3] SemanticKITTI [2] 25 nuScenes-LiDARSeg [11] 32 Waymo-LiDARSeg [41] 21 KITTI [13] point-wise point-wise point-wise point-wise point-wise SemanticSTF (ours) 21 Table 1. Comparison of SemanticSTF against existing outdoor LiDAR benchmarks. #Cls means the class number. and produce many noise points as shown in Fig.3(b); 3) Dense fog may greatly reduce the working range of LiDAR sensors, leading to small spatial distribution of the collected LiDAR points as illustrated in Fig. 3(c); 4) Point clouds un- der light fog have similar characteristics as normal-weather point clouds as illustrated in Fig. 3(d). The distinct prop- erties of point clouds under different adverse weather intro- duce different types of domain shift from normal-weather point clouds which complicate 3DSS greatly as discussed in Section 5. They also verify the importance of develop- ing universal 3DSS models that can perform well under all weather conditions. 4. Point Cloud Domain Randomization Leveraging SemanticSTF, we explore domain general- ization (DG) for semantic segmentation of LiDAR point clouds under all weather conditions. Specifically, we de- sign PointDR, a domain randomization technique that helps to train a generalizable segmentation model from normal- weather point clouds that can work well for adverse-weather point clouds in SemanticSTF. 4.1. Problem Definition Given labeled point clouds of a source domain S = {Sk = {xk, yk}}K k=1 where x represents a LiDAR point cloud scan and y denotes its point-wise semantic annota- tions, the goal of domain generalization is to learn a seg- mentation model F by using the source-domain data only that can perform well on point clouds from an unseen tar- get domain T . We consider a 3D point cloud segmentation model F"}