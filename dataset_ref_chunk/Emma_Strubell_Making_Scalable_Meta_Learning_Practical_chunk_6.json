{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Making_Scalable_Meta_Learning_Practical_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What improvements in test accuracy were observed at the pruning ratio of 0.1 and 0.2?", "answer": " GBML-based data pruning led to improvements in test accuracy at the pruning ratio of 0.1 and 0.2.", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " What was the reported relative accuracy for ImageNet-1k data pruning results with ResNet-50 compared to full training accuracy?", "answer": " 0.99", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " What is the major bottleneck in GBML applications according to the text?", "answer": " Compute/memory inefficiency", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " What are the two major lines of research in gradient-based meta learning algorithms mentioned in the text?", "answer": " Iterative and implicit differentiation", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " What are some applications of meta learning mentioned in the text?", "answer": " Few-shot learning, neural architecture search, hyperparameter optimization, data optimization, and reinforcement learning", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " What is the conclusion regarding the practicality of scalable meta learning made in the text?", "answer": " Striving to make scalable meta learning practical via algorithmic and systems advancements", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " What important directions are planned for exploration in future work according to the text?", "answer": " Extending SAMA compatibility with techniques for training extremely large models and focusing on large-scale meta learning application research", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " What are the limitations mentioned in the text regarding the testing of SAMA?", "answer": " Inability to test SAMA on larger models with 1B+ parameters due to a lack of computational resources", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " What is the chance mentioned in the text regarding the broader impacts of meta learning?", "answer": " There is a chance that practitioners may amplify the bias or toxicity of machine learning programs by enforcing them in the meta objective", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}, {"question": " Which organizations supported the authors of the text?", "answer": " NSF, NGA, NIGMS, Amazon, AFOSR, ARO, CZ Biohub, Sloan Fellowship, DSO National Laboratories", "ref_chunk": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}], "doc_text": "works well across different dataset scales. Surprisingly, we observe that GBML- based data pruning even leads to improvements in test accuracy at the pruning ratio of 0.1 and 0.2 8 0.94 0.1 EL2N_1 0.95 0.3 0.4Pruning Ratio 1.00Relative Top-1 Accuracy Random ImageNet-1k w/ ResNet-50 0.2 0.99 0.0 GradND SAMA DynaMS 0.96 EL2N_10 0.98 0.97 CIFAR-10 w/ ResNet-18 0.1 0.3 margin 0.2 Random 1.00Relative Top-1 Accuracy GradND 0.0 0.99 0.6Pruning Ratio 0.98 0.97 0.4 forgetting SAMA 0.5 EL2N1 [47] EL2N10 [47] GradNd10 [47] DynaMS [63] SAMA (ours) Relative Search Time 0.16 1.65 1.65 0.16 0.46 Figure 3: Top Left: ImageNet-1k data pruning results with ResNet-50. Reported numbers are relative accuracy compared to full training accuracy (i.e., pruned_acc/full_acc). Accuracy for other baseline methods is obtained from DynaMS [63]. Top Right: CIFAR-10 data pruning results with ResNet-18. Accuracy for other baseline methods is obtained from Deepcore [22]. Bottom: Relative time spent in finding data to prune compared to full ImageNet-1k training time. on ImageNet-1k. The potential implication is that ImageNet-1k may have noisy labels or semantic redundancy and that GBML is able to automatically figure and filter out these samples. Further in-depth investigation of filtered data remains an interesting research direction. Considering that compute/memory inefficiency has traditionally been the major bottleneck in GBML applications, we also compare the relative search time for data pruning. Our result shows that SAMA demonstrates comparable or even shorter search time than heuristics-based methods. We also note that, while the original MWN [58] encounters the OOM error under our setup of batch_size=256, the throughput analysis with the reduced batch size reveals that efficient distributed training with SAMA on 4 GPUs achieves 15-20\u00d7 speed up compared to the original MWN that lacks distributed training support. 5 Related Work Algorithms Two major lines of research in gradient-based meta learning algorithms are iterative and implicit differentiation [19]. Iterative differentiation [14, 15, 16, 42] computes meta gradients by differentiating through the optimization path and therefore requires saving all intermediate states on the path. This makes the memory/compute costs of iterative differentiation increase linearly with the number of unrolling steps. While the linear cost can be avoided with the use of techniques like truncated backpropagation [36], it is still more expensive than that of implicit differentiation, in which meta gradient computation is independent of the length of the optimization path. More specifically, meta gradient computation in implicit differentiation depends only on the final state of the optimization path. To compute base Jacobian inversion in implicit differentiation, a multitude of variants have been proposed, each of which uses Neumann series [7, 40], conjugate gradient [51], Nystrom method [25], and more. While generally being more compute/memory efficient than iterative differentiation, most existing implicit differentiation algorithms have poor scalability due to the issues studied in Sec. 3. Applications Meta learning has found many applications in machine learning including few-shot learning [14, 51, 71], neural architecture search [38, 69], hyperparameter optimization [15, 16, 40, 42], data optimization [21, 54, 58, 70], and reinforcement learning [23, 29, 52], to name a few. Notably, most of these applications share the underlying mathematical formulation of bilevel optimization and are conceptually related to optimal design and inductive/declarative programming paradigms. Systems Compared to algorithms and systems research, there are relatively fewer research efforts In an attempt to facilitate research in few-shot image classification, in meta learning systems. higher [20], learn2learn [3], and TorchMeta [9] have been developed. However, due to their specific focus on few-shot image classification, these libraries have not been actively used in other meta learning tasks, such as data optimization or neural architecture search. Recently, software libraries 9 for implicit differentiation including JaxOpt [4] and Betty [6], have been proposed. Given that Betty\u2019s software architecture is specifically designed to support various systems optimization for large-scale meta learning, we chose to implement SAMA in this framework. 6 Conclusion In this paper, we strived to make scalable meta learning practical via both algorithmic and systems advancements. Towards this goal, we investigated diverse scaling bottlenecks in meta learning at a technical level and resolved them by developing SAMA. Tested on multiple benchmarks, SAMA empirically demonstrated its scaling efficiency as well as its capability to optimize a variety of high-dimensional inductive biases of large-scale learning. In future work, we plan to explore two directions. First, given that training extremely large models with 10B+ parameters require various systems techniques such as model/pipeline parallelism or optimizer sharding, extending SAMA to be compatible with these techniques would be highly important for further scalability. Second, we will also focus on large-scale meta learning application research, such as neural architecture search for Transformer-family models. Overall, we hope that our work can serve as a stepping stone for a lot of interesting scalable meta learning research to come. Limitations & Broader Impacts While SAMA has demonstrated significantly improved com- pute/memory efficiencies, and stably worked with fairly large models like BERT/RoBERTa, we could not test it on larger models with 1B+ parameters due to a lack of computational resources. Meta learning itself is mostly value-neutral, but there is a chance that practitioners amplify the bias or toxicity of the machine learning programs by enforcing them in the meta objective. Acknowledgements We thank all the reviewers for their invaluable comments and feedback. We like to acknowledge CMU Workhorse and TIR group for providing compute resources for this work. EX and SKC acknowledge the support of NSF IIS2311990, NSF IIS2123952, NSF CNS2008248, NSF BCS2040381, NGA HM04762010002, NIGMS R01GM140467, and Amazon 2023-5007107. WN was supported in part by NSF (#1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), CZ Biohub, and Sloan Fellowship. ES and SVM acknowledge the support in part by DSO National Laboratories. References [1] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [2] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. International Conference on Learning Representations, 2019. [3] S\u00e9bastien MR Arnold, Praateek Mahajan,"}