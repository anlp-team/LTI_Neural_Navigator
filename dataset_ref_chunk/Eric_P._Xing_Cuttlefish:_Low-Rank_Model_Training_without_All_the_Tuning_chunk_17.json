{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Cuttlefish:_Low-Rank_Model_Training_without_All_the_Tuning_chunk_17.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does it indicate when the curves are closer to the reference line in the context of factorizing layers?", "answer": " It indicates that CUTTLEFISH begins factorizing layers after the first embedding layer.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " At what encoder block does PUFFERFISH start factorizing layers for DeiT?", "answer": " PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " What are the fine-tuning epochs typically set to for all downstream tasks in BERTBASE on the GLUE benchmark?", "answer": " Fine-tuning epochs are typically set to T = 3, 5.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " What is the batch size used for ResNet-18 and VGG-19 models trained on CIFAR-10 in the hyperparameters optimized by CUTTLEFISH?", "answer": " The batch size used is 1,024.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " During fine-tuning the factorized BERTBASE, what layers do not get updated?", "answer": " During fine-tuning, the feed-forward network (FFN) layers FC1 and FC2 in BERT do not get updated.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " What task is considered to be relatively challenging, leading to the fine-tuning of all models except vanilla BERTBASE for 5 epochs instead of 3 epochs?", "answer": " The CoLA task is considered to be relatively challenging.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " What is the purpose of scaled stable rank employed by CUTTLEFISH?", "answer": " Scaled stable rank is used to avoid overly aggressive low rank estimations that could harm the final accuracy of factorized low-rank models.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " What were the results of combining CUTTLEFISH with Frobenius decay across various machine learning tasks?", "answer": " Applying FD to CUTTLEFISH did not consistently lead to better model accuracy.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " Why was the implementation and hyperparameter configurations for experiments taken directly from the original GitHub repository associated with LC compression?", "answer": " To ensure consistency with the VGG-19 architecture used in the experiments.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}, {"question": " What is the observation regarding the accuracy of factorized low-rank models when Frobenius decay is applied?", "answer": " The observation is that applying FD does not always enhance the accuracy of factorized low-rank models.", "ref_chunk": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}], "doc_text": "in Figure 9). If the curves are closer to the reference line, it indicates CUTTLEFISH begins factorizing layers after the \ufb01rst embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFER- FISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE \ufb01ne- tuning on the GLUE benchmark, the \ufb01ne-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when CUTTLEFISH: Low-rank Model Training without All The Tuning Table 8. The hyperparameters \u02c6s \u2208 S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU. CIFAR-10 CIFAR-100 SVHN Model: ResNet-18 E K E K E K CUTTLEFISH PUFFERFISH 82.3\u00b110.1 80 5 3 55.7\u00b18.7 80 5 3 61.0\u00b12.2 80 5 3 SI&FD 0 1 0 1 0 1 Model: VGG-19 E K E K E K CUTTLEFISH PUFFERFISH 97.3\u00b11.2 80 4 9 86.0\u00b15.7 80 4 9 84.0\u00b10.8 80 4 9 SI&FD 0 1 0 1 0 1 Table 9. The hyperparameters \u02c6s \u2208 S obtained by CUTTLEFISH, as well as the manually tuned s from PUFFERFISH, for ResNet-50 and WideResNet-50-2 trained on ImageNet using a batch size of 256. ImageNet Model: ResNet-50 E K CUTTLEFISH PUFFERFISH 19.3\u00b10.5 10 40 40 Model: WideResNet-50-2 E K CUTTLEFISH PUFFERFISH 21.3\u00b10.5 10 40 40 \ufb01ne-tuning the factorized BERTBASE. As in LoRA, during the \ufb01ne- tuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates \u03b3s for all methods during GLUE \ufb01ne-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we \ufb01ne-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we \ufb01ne-tune Distill BERT for 5 epochs. C.3 Hyperparameters for other baselines. SI&FD. We adjust the \ufb01xed global rank ratios, denoted as \u03c1s, for SI&FD so that the resulting model sizes align with the fac- torized low-rank models produced by CUTTLEFISH. Detailed information on the \u03c1s used in our experiments can be found in Table 12. in the LC compression setup to ensure consistency with the VGG- 19 architecture used in our experiments. D ADDITIONAL EXPERIMENTAL RESULTS D.1 Ablation study. Combining CUTTLEFISH with Frobenius decay. In this section, we present the results of an ablation study examining the combination of Frobenius decay (FD) with CUTTLEFISH across various machine learning tasks. The results can be found in Ta- ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH does not consistently lead to better model accuracy. For instance, combining CUTTLEFISH with FD yields a 1.8% higher accuracy for ResNet-18 training on CIFAR-100. However, for other tasks, incorporating FD with ResNet-18 either results in worse \ufb01nal model accuracy or only marginal accuracy improvements. This observation aligns with the \ufb01ndings of (Vodrahalli et al., 2022), indicating that FD does not always enhance the accuracy of factor- ized low-rank models. LC compression. The implementation and hyperparameter con\ufb01gurations for our experiments are taken directly from the original GitHub repository6 associated with (Idelbayev & Carreira- Perpin\u00b4an, 2020). We modi\ufb01ed the VGG-19 model implementation 6https://github.com/UCMerced-ML/ LC-model-compression The impact of scaled stable rank. As mentioned in the main paper, the use of stable rank can lead to overly aggressive low rank estimations, potentially harming the \ufb01nal accuracy of factorized low-rank models. To address this issue, CUTTLEFISH employs scaled stable rank. The ablation study results are pre- CUTTLEFISH: Low-rank Model Training without All The Tuning 10 8 Cuttlefish Full-rank 300 2 500Rank Selection \ue23e 100 14 Pufferfish 400 200 16Layer Index 0 4 12 6 4 Full-rank 8 16Layer Index 12 0 10 400 Pufferfish 2 500Rank Selection \ue23e 300 100 Cuttlefish 6 14 200 14 6 0 Cuttlefish 8 100 Pufferfish 2 Full-rank 200 4 500Rank Selection \ue23e 400 300 12 16Layer Index 10 (a) ResNet-18 on CIFAR-10 (b) ResNet-18 on CIFAR-100 (c) ResNet-18 on SVHN Full-rank 0 LC Compres. 10 12 400 200 8 600Rank Selection \ue23e Pufferfish 500 14 300 Cuttlefish 100 4 2 16Layer Index 6 0 Cuttlefish 12 300 14 500 400 8 100 200 LC Compres. 600Rank Selection \ue23e Pufferfish 10 Full-rank 16Layer Index 4 2 6 Full-rank 0 400 500 200 6 8 Pufferfish 300 10 16Layer Index LC Compres. 14 12 100 Cuttlefish 600Rank Selection \ue23e 2 4 (d) VGG-19 on CIFAR-10 (e) VGG-19 on CIFAR-100 (f) VGG-19 on SVHN Figure 7. The ranks (R) determined by CUTTLEFISH, PUFFERFISH, and LC compression (available only for VGG-19 experiments) for various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024. 44 Pufferfish 200 46 38 Cuttlefish 40 50Layer Index 48 36 42 600 Full-rank 1000Rank Selection \ue23e 400 800 Full-rank 42 48 800 38 50Layer Index 40 1000Rank Selection \ue23e Pufferfish 36 46 400 200 Cuttlefish 44 600 (a) ResNet-50 on ImageNet (b) WideResNet-50-20 on ImageNet Figure 8. The ranks Rs obtained by CUTTLEFISH and PUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2 (b) trained on ImageNet with a batch size of 256. sented in Table 15. We \ufb01nd that for CIFAR-10 and CIFAR-100 datasets, utilizing scaled stable rank is crucial for achieving satis- factory \ufb01nal accuracy in factorized low-rank networks. For SVHN, which is a comparatively simpler task, even the vanilla stable rank is suf\ufb01cient to attain good accuracy. Thus,"}