{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Federated_Learning_as_Variational_Inference:_A_Scalable_Expectation_Propagation_Approach_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the motivation for a Bayesian approach in federated learning?", "answer": " Uncertainty quantification.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " How does the convergence of approximate inference techniques change with model complexity?", "answer": " The convergence benefits of more advanced approximate inference decline as the model gets more complex.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " Define Expected Calibration Error (ECE) in the context of Bayesian treatment of federated learning.", "answer": " ECE is defined as the sum of accuracy and confidence in each bin divided by the fraction of data points in that bin.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " What are the two ways considered to estimate accuracy and calibration from the resulting approximate posterior?", "answer": " Point estimation and marginalized estimation.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " What are FedEP and FedSEP in the context of the text?", "answer": " FedEP refers to stateful EP and FedSEP refers to stateless stochastic EP.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " What are the limitations of FedEP mentioned in the text?", "answer": " Increased memory requirements for clients and additional communication overhead for non-scaled-identity approaches.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " What is the focus of the related work section in the text?", "answer": " Federated Learning and Expectation Propagation for passing messages in partitioned data.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " What theoretical guarantees have been extensively studied for FedAvg?", "answer": " Convergence properties under various assumptions.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " What is the historical reference for EP as a method of passing messages?", "answer": " DKL(p||q), also known as Variational Message Passing (VMP) or Laplace propagation (LP).", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}, {"question": " How is FedEP different from prior works applying EP to federated learning?", "answer": " This work discusses algorithmic considerations to scale up EP to contemporary benchmarks.", "ref_chunk": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}], "doc_text": "setting, we conduct experiments with EP using scaled-identity and NGVI and plot the results in Fig. 4 (right). We observe that under the \u201csmall\u201d setting, a more advanced approximate inference technique converges faster than scaled-identity EP, consistent with the toy experiments. As we increase the model size however (\u201clarge\u201d setting), the gap between these two approaches disappears. This indicates that as the model gets more complex, the convergence bene\ufb01ts of more advanced approximate inference decline due to covariance estimation\u2019s becoming more dif\ufb01cult. Uncertainty Quanti\ufb01cation. One motivation for a Bayesian approach is uncertainty quanti\ufb01cation. We thus explore whether a Bayesian treatment of federated learning results in models that have bet- ter expected calibration error (ECE, Naeini et al., 2015; Guo et al., 2017), which is de\ufb01ned as (cid:12) ECE = \u2211Nbins (cid:12) . Here accuracyi is the top-1 prediction accuracy in i-th bin, con\ufb01dencei is the average con\ufb01dence of pre- dictions in i-th bin, and bi is the fraction of data points in i-th bin. Bins are constructed in a uniform way in the [0, 1] range.8 We consider accuracy and calibration from the resulting approximate posterior in two ways: (1) point estimation, which uses the \ufb01- nal model (i.e., MAP estimate from the approximate posterior) to obtain the output probabilities for each data point, and (2) marginalized estimation, which samples 10 models from the approximate pos- terior and averages the output probabilities to obtain the \ufb01nal prediction probability. In Table 5, Accuracy (%, \u2191) Point Est. Marg. ECE-15 (%, \u2193) Point Est. Marg. Method \u2212 48.1 46.6(0.7) \u2212 \u2212 13.6 19.5(0.4) \u2212 FedPA FedAvg FedEP (I) FedEP (M) FedEP (L) FedEP (V) 50.8(0.4) 50.5(0.5) 47.7(0.5) 49.7(0.5) 49.6(0.6) 50.2(0.4) 47.8(0.5) 49.5(0.3) 4.9(0.3) 5.9(0.5) 8.8(0.4) 5.9(0.4) 7.9(0.2) 4.6(0.4) 6.6(0.4) 2.2(0.5) (cid:12) (cid:12)accuracyi \u2212 con\ufb01dencei bi i FedSEP (I) FedSEP (M) FedSEP (L) FedSEP (V) 49.0(0.4) 48.9(0.4) 47.7(0.5) 48.5(0.4) 48.5(0.4) 48.6(0.4) 47.8(0.5) 48.7(0.4) 10.0(0.4) 10.1(0.4) 9.6(0.6) 9.3(0.4) 3.4(0.3) 3.5(0.3) 7.2(0.6) 3.7(0.4) Table 5: CIFAR-100 Calibration Experiments. FedEP and FedSEP refer to the stateful EP and stateless stochastic EP. We use I (Scaled Iden- tity Covariance), M (MCMC), L (Laplace), and V (NGVI) to refer to different inference techniques. 8We also experimented with an alternative binning method which puts an equal number of data points in each bin and observed qualitatively similar results. 8 Published as a conference paper at ICLR 2023 FedEP (NGVI)FedEP (Scaled Identity) Accuracy (small)Accuracy (large)FedEP (NGVI)FedEP (Scaled Identity) EuclideanDistance FedEP (Scaled Identity)FedEP (Diagonal)FedAvgFedPA (Diagonal) Figure 4: Analysis Experiments. Left: the average Euclidean distances between the estimated and target global mean as a function of rounds in the toy setting. Middle and Right: accuracy as a function of rounds in the CIFAR-100 setting, with either a (relatively) small model (middle) or large model (right). we observe that FedEP/FedSEP improves both the accuracy (higher is better) as well as expected calibration error (lower is better), with marginalization sometimes helping. Hyperparameters. Table 8 shows the robustness of FedEP w.r.t. various hyperparameters. Limitations. While FedEP outperforms strong baselines in terms of convergence speed and \ufb01nal accuracy, it has several limitations. The stateful variant requires clients to maintain its current con- tribution to the global posterior, which increases the clients\u2019 memory requirements. The non-scaled- identity approaches also impose additional communication overhead due to the need to communi- cate the diagonal covariance vector. Further, while SG-MCMC/Scaled-Identity approaches have the same compute cost as FedAvg on the client side, Laplace/NGVI approaches require more compute to estimate the Fisher term. Finally, from a theoretical perspective, while the convergence properties of FedAvg under various assumptions have been extensively studied (Li et al., 2018; 2020b), such guarantees for expectation propagation-based approaches remains an open problem. 4 RELATED WORK Federated Learning. FL is a paradigm for collaborative learning with decentralized private data (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017; Li et al., 2020a; Kairouz et al., 2021; Wang et al., 2021). Standard approach to FL tackles it as a distributed optimization problem where the global objective is de\ufb01ned by a weighted combination of clients\u2019 local objectives (Mohri et al., 2019; Li et al., 2020a; Reddi et al., 2020; Wang et al., 2020b). Theoretical analysis has demonstrated that federated optimization exhibits convergence guarantees but only under certain conditions, such as a bounded number of local epochs (Li et al., 2020b). Other work has tried to improve the averaging- based aggregations Yurochkin et al. (2019); Wang et al. (2020a). Techniques such as secure ag- gregation (Bonawitz et al., 2017; 2019; He et al., 2020) and differential privacy (Sun et al., 2019; McMahan et al., 2018) have been widely adopted to further improve privacy in FL (Fredrikson et al., 2015). Our proposed method is compatible with secure aggregation because it conducts server-side reductions over \u2206\u03b7k, \u2206\u039b Expectation Propagation and Approximate Inference. This work considers EP as a general tech- nique for passing messages between clients and servers on partitioned data. Here, the cavity distri- bution \u201csummarizes\u201d the effect of inferences from all other partitions and can be used as a prior in the client\u2019s local inference. Historically, EP usually refers to a speci\ufb01c choice of divergence func- tion DKL(p(cid:107)q) (Minka, 2001). This is also known as Variational Message Passing (VMP, Winn et al., 2005) when DKL(q(cid:107)p) is used instead, and Laplace propagation (LP, Smola et al., 2003) when Laplace approximation is used. There have been works that formulate federated learning as a probabilistic inference problem. Most notably, Al-Shedivat et al. (2021) formulate FL as a posterior inference problem. Achituve et al. (2021) apply Gaussian processes with deep kernel learning (Wil- son et al., 2016) to personalized FL. Finally, some prior works also consider applying EP to federated learning (Corinzia et al., 2019; Kassab & Simeone, 2022; Ashman et al., 2022), but mostly on rela- tively small-scale tasks. In this work, we instead discuss and empirically study various algorithmic considerations to scale up expectation propagation to contemporary benchmarks. k. 5 CONCLUSION This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP). Messages (probability distributions) are passed to and from"}