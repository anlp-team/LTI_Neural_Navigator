{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_Multi-Objective_Improvement_of_Android_Applications_2024-02-27_01-10-55_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many generations were evolved in the study?,        answer: 10 generations    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " Which approach took longer, SO-GI or MO-GI?,        answer: SO-GI took longer    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " What was the median time taken by SO-GI?,        answer: 3.5 hours    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " Why was the cost of running MO-GI considered worth it?,        answer: Users consider apps running for 150ms laggy, which might lead to them abandoning an app    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " What tool was used to compare the approach to improving performance?,        answer: A Linter (PMD)    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " How many benchmarks were improved by repairing Linter warnings?,        answer: 21 benchmarks    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " What was the maximum improvement in memory usage achieved by GIDroid?,        answer: 69%    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " What was the maximum improvement in execution time achieved by GIDroid?,        answer: 35%    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " How many of the best patches found did not disrupt the functionality of the apps?,        answer: 1352 out of 1753    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}, {"question": " What is proposed to automatically improve Android apps in the study?,        answer: Multi-objective genetic improvement (MO-GI)    ", "ref_chunk": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}], "doc_text": "evolve 10 generations, each with 40 individuals. We find that SO-GI takes longer than MO-GI, with a minimum of 0.4 hours, a maximum of 19.0 hours, and a median of 3.5 hours. SO-GI can only find improvements to one property at a time, showing the much-improved efficiency of using MO-GI. Despite hour-long runtimes, we note that this is a one-off cost. Given that users con- sider apps running for 150ms laggy, which might lead to them abandoning an app, we deem the cost of running MO-GI worth it. 7.6 RQ6: Comparison to Linter In order to compare our approach to the currently available tooling for improving performance for Android, we run a well-known Linter (PMD) on all of the bench- marks which we improved. We configured it to provide warnings when any of its performance rules were violated. We then manually analyzed each of the warnings that it provided, and in the cases where they could be repaired without disrupting the functionality of the application, we repaired them. We then measured the performance differences between the repaired and unre- paired versions of the applications. We found that in our 21 benchmarks, 5 had either no warnings or warnings that could not be repaired without introducing buggy behav- ior. For example, a warning about instantiating an object in a loop could be \u201cunfixable\u201d as a reference to each instantiated object is held in an array. So, moving the instan- tiation outside of the loop would result in an array with the same reference repeated multiple times. 24 Table 7 Improvements (%) from repairing linter warnings, for benchmarks where viable improvements were found. Application Version Exec. Time Mem. Con. Time (min.) PortAuthority 1 PortAuthority 5 PortAuthority Current TowerCollector 2 TowerCollector Current 2.5 2.4 0.9 0 1 0.0 2.8 10.4 -2.8 0 1.9 2 9 1 5 7 Fdroid 1 Fdroid Current 4.5 2.3 0 -0.2 13 9 LightningBrow. 1 LightningBrow. Current 2.2 0.9 0.4 -1.6 1 5 FrozenBubble 1 FrozenBubble Current 3.5 -1.6 0.1 0.4 20 15 In all cases where possible, the fixes were easily created and very similar to the examples given in the PMD documentation, and are available in our online repository (GIDroid (2023)). Of the 16 where fixes were possible, only 9 actually offered any improvement. The maximum improvement to execution time was 4.5%, while to memory it was 10.42%, when compared with 35% and 69%, respectively, achieved by GIDroid. No improvements to bandwidth usage were found. Only a single one of these patches improved multiple properties, and 6 were detrimental to other properties. Of those improvements, none had any impact on the bandwidth of the applications. The linters were, however, significantly quicker than GI, taking a maximum time of 20 minutes to repair the warnings. However, unlike GI this process is not automatic and requires a developer to be engaged at all times and the improvements found were much smaller than those found by GI. 8 Threats to Validity There are a number of threats to the validity of our study. We discuss these next, including steps we took to mitigate them. The measurements we use for our fitnesses are noisy. To mitigate this threat, we repeat each measurement 20 times during search and after the search is complete. We use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement. We tested our measurements on known improvements and found that they are consistently detected. Furthermore, we use tests to determine whether or not a patch is valid. This does not guarantee correctness. However, the patches produced can undergo the standard code review procedure as any other code being integrated into a project would. We conducted a manual analysis of all the patches on the Pareto fronts (1753 total), to ensure the improvements reported here do not disturb app functionality. Through manual analysis, we found that 1352 out of the 1753 best patches found did not disrupt the functionality of the apps, demonstrating the strength of our test suites. 25 Disruptive patches included the removal of some error handling and the deletion of some components rendered on screen that could not be detected with unit tests. They would be easily discarded by code review. Using stochastic search may result in us finding improvements out of sheer luck. In order to avoid this issue, reliably compare different algorithms, and demonstrate generalisability of our approach, we run each of the algorithms tested 20 times on each of our 21 benchmarks. The search algorithms we use rely on parameters such as mutation and crossover rate. The values of these parameters can have an effect on the effectiveness of the algorithms. To mitigate this threat, we use the same parameters across all experiments for fair comparison. We use settings used in previous work that found improvements in software. We tested our approach on 21 versions of 7 Android apps, which poses a threat to generalisability to other software. However, these apps are diverse in size and type. Moreover, we found improvements in current app versions, which were previously undiscovered. Unfortunately, currently, the big obstacle to wider adoption is test avail- ability. For each benchmark, these took us hours to produce. However, the benefits of testing go beyond the applicability of our approach. We envision with the develop- ment of more fine-grained automated test generation tooling for Android and better testing practices, further benefits of GI can be unlocked. To mitigate such threats further, we make all our code and results freely avail- able (GIDroid (2023)), allowing other researchers and developers to use and extend our tool and validate our work. 9 Conclusions and Future Work We propose to use multi-objective genetic improvement (MO-GI) to automatically improve Android apps. We are the first to apply MO-GI with three objectives to improve software performance and evaluate feasibility of MO-GI for bandwidth and memory use in the Android domain. To evaluate the effectiveness of the proposed approach we developed GIDroid, which contains 3 MO algorithms and 2 novel"}