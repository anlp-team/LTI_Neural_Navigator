{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_StarCoder:_may_the_source_be_with_you!_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What kind of model did the researchers train for PII detection?", "answer": " They trained an encoder-only model, specifically bi-directionally self-attentive Transformers.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " Which objectives from BERT did they use in their PII detection model training?", "answer": " They used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " How many tokens are masked in the input for the Masked Language Modelling loss?", "answer": " Tokens in the input are masked independently with a probability of 15%.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " How many steps did they train the model for?", "answer": " They trained the model for 100,000 steps.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " What types of PII were excluded from the model training?", "answer": " IDs were excluded from the model training due to low annotation quality.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " How many examples were in the training set and test set for the named entity recognition (NER) task?", "answer": " There were 7,878 examples in the training set and 4,000 examples in the test set.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " What F1 scores did the researchers achieve on passwords and names?", "answer": " They achieved F1 scores of 73.39% on passwords and more than 90% on names.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " What is the purpose of employing pseudo-labeling in the model training process?", "answer": " Pseudo-labeling is employed to improve the detection of key and password entities.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " How many files were annotated using the ensemble of two encoder models for pseudo-labeling?", "answer": " They annotated 18,000 files using an ensemble of two encoder models.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}, {"question": " What is the outcome of the comparison between the PII detection models and the regex approach?", "answer": " The PII detection models surpassed the regex approach in detecting all three entities.", "ref_chunk": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}], "doc_text": "negatives. As a result, we decided to exclude this category from the PII detection model training. 4.2 StarEncoder As part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive Transformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019; Liu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences occur as neighbors in a document. We separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code snippets are selected randomly, either from the same source file or from two distinct documents. For the MLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use a linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are 10https://github.com/Yelp/detect-secrets 11 Published in Transactions on Machine Learning Research (12/2023) PII type Count Recall Precision IP_ADDRESS KEY PASSWORD ID EMAIL EMAIL_EXAMPLE EMAIL_LICENSE NAME NAME_EXAMPLE NAME_LICENSE USERNAME USERNAME_EXAMPLE USERNAME_LICENSE AMBIGUOUS 2526 308 598 1702 5470 1407 3141 2477 318 3105 780 328 503 287 85% 91% 91% 53% 99% 89% 74% 97% 78% 86% 51% 97% 94% 86% Table 5: Overview of the PII types and the number of collected annotations. We investigate the annotation quality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was mapped back to its corresponding PII type for the inspection. Hyperparameter Value Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 768 3072 1024 12 12 Multi-head Num. of parameters \u2248125M Table 6: Model architecture of StarEncoder. observed. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture are reported in Table 6. 4.3 PII detection model We fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We added a linear layer as a token classification head on top of the model, with 6 target classes: names, emails, keys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not differentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s poor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test set of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See Table 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode. Fine-tuning baseline We fine-tune StarEncoder on the PII training set, and 400 annotated files from Ben Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and 73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores of only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of labels for this type of PII, as only 308 instances were available. For usernames, we observed the model often confused them with decorators and values in paths. This is most likely because we annotated usernames inside links for social media platforms. 12 Published in Transactions on Machine Learning Research (12/2023) Entity type Train Test EMAIL NAME IP_ADDRESS USERNAME PASSWORD KEY 4721 3847 1941 1320 390 171 1742 1298 521 346 148 118 Table 7: Train-test split of the annotated PII dataset. Method Email address IP address Key Prec. Recall F1 Prec. Recall F1 Prec. Recall Regex NER + pseudo labels 97.73% 98.94% 98.15% 90.10% 96.20% 97.47% 96.83% 71.29% 6.74% 94.01% 98.10% 96.01% 88.95% 94.43% 91.61% 60.37% 53.38% 56.66% 93.86% 91.94% 62.38% 80.81% 70.41% 87.71% 78.65% 3.62% 49.15% Table 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data, and NER Pipeline with Annotated Data + Pseudo-Labels Pseudo-labels To improve the detection of key and password entities, we employed a pseudo-labeling technique as described by Lee (2013). This method involves training a model on a small set of labeled data and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated 18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from Ben Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from our models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities, except for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the results, we found a significant number of false positives for keys and passwords. As a result, we decided to only retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100 characters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results for all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did not show significant improvement, so we decided to exclude it from the PII redaction process. Comparison against regex baseline We compared our PII detection models against the regular expres- sions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP addresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the regex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in detecting all three entities, as shown in Table 8. We note that the performance difference was especially large on keys and found that the detect-secrets tool generated many false positives, especially in specific pro- gramming languages like Go and"}