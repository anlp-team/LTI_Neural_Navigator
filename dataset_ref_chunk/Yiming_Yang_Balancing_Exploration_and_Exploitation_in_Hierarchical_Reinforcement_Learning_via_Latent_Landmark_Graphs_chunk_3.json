{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Balancing_Exploration_and_Exploitation_in_Hierarchical_Reinforcement_Learning_via_Latent_Landmark_Graphs_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the function L\u03c6(s) used for in the subgoal representation learning loss?", "answer": " To regularize the update of subgoal representation", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " How does the high-level controller choose a subgoal according to the temporal coherence in GCHRL?", "answer": " Selects a subgoal every c time steps", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " What does the function Lc(st, st+1, st+c) measure in Building Latent Landmark Graphs?", "answer": " The distance between representations of states at time steps st, st+1, and st+c", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " What method is used to select a collection of representations that maximize the coverage of the latent space explored in Novelty-guided Exploration Based on Nodes?", "answer": " Farthest Point Sampling (FPS)", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " How is novelty measured for landmarks in the latent landmark graph?", "answer": " By utilizing the count-based method to estimate expected future occupancies of representations", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " What is the utility measure U for in Utility-guided Exploitation Based on Edges?", "answer": " Estimating the benefits of transitioning from one landmark to another in completing the source task", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " How is the convergence of V accelerated in the context of utility measure U?", "answer": " By using Hindsight Experience Replay (HER) to generate more feedback for the agent", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " How is the exploration-exploitation dilemma tackled in the algorithm HILL?", "answer": " By selecting a landmark that well-balances novelty and utility from the latent landmark graph", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " What is the purpose of updating \u03b8h using SAC in the algorithm HILL?", "answer": " To train the high-level policy network", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}, {"question": " What is the distribution of U(wi,j) used for in the context of determining the probability of success?", "answer": " As an incremental probability of success for transitioning between landmarks", "ref_chunk": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}], "doc_text": "their Lr to regularize its update. The overall subgoal representation learning loss is: structure is depicted in Figure 1. A. Contrastive Subgoal Representation Learning L\u03c6(s) = Est\u223cBl [Lc(st, st+1, st+c)] + Es\u2032 t\u223cBp [Lr(s\u2032 t)], We define a subgoal representation function \u03c6 : S \u2192 Rd which abstracts states s \u2208 S to latent representations z \u2208 Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., \u03c6(st) and \u03c6(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., \u03c6(st) and \u03c6(st+1)) are relatively similar. To train \u03c6, we define a negative-power contrastive representation learning objective as follows: where Bl is a replay buffer containing historical episodes. B. Building Latent Landmark Graphs At time steps t \u2261 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using \u03c6, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset. Lc(st, st+1, st+c) =||\u03c6(st) \u2212 \u03c6(st+1)||2 2 1 + \u03b2 \u00b7 ||\u03c6(st) \u2212 \u03c6(st+c)||n 2 + \u03f5 , where n \u2265 1, \u03b2 is a scaling factor, and \u03f5 > 0 is a small constant to avoid a zero denominator. The function \u03c6 abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty. (2) 1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation (3) (4) of the current state, to Lt. A single landmark is denoted as l \u2208 Lt. Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l \u2208 Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si: N (zi) = (cid:88) \u230a(T \u2212i)/c\u230b (cid:88) \u03b3j\u03b7(zi+jc), T \u2208Bl j=0 where zi = \u03c6(si), T is the length of episode T and \u03b7 is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table \u03b7 at the end of each episode. 2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj \u2208 Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task: U (wi,j) = Esx\u2208Bl [I(\u03c6(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method: U (wi,f ) = max lj [U (wi,j) + U (wj,f )] = max lj ,...,lv [U (wi,j) + v\u22121 (cid:88) x=j U (wx,x+1) + U (wv,f )], (7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process. C. Balance Exploration and Exploitation We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the a landmark from Lt (5) (6) Algorithm 1 HILL algorithm Initialize: \u03c6, \u03c0\u03b8h, \u03c0\u03b8l and p. 1: for i = 1...num episodes do for t = 0...T \u2212 1 do 2: 3: 4: 5: 6: 7: 8: if t \u2261 0 (mod c) then Generate a random float value q \u2208 [0.0, 1.0]. if 2q \u2212 1 \u2264 p then Build a latent landmark graph. Select gt with the strategy in Equation 9. else Sample gt using \u03c0\u03b8h . 9: 10: 11: 12: 13: 14: end if Update \u03b8h using SAC. end if rt, st+1 \u2190 execute at \u223c \u03c0\u03b8l (\u00b7|st, gt). end for if i \u2261 0 (mod 100) then 15: 16: 17: 18: 19: 20: end for 21: return \u03c6, \u03c0\u03b8h and \u03c0\u03b8l . Update \u03c6 using Equation 4. Update p to the newest average train success rate. end if Update \u03b8l and \u03c6 using SAC. subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows: P(U (wi,j)) = eU (wi,j ) y=1 eU (wi,y) (cid:80)Y , where Y is"}