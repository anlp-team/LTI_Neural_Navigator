{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Structured_Dialogue_Discourse_Parsing_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the underlying idea behind using bidirectional LSTMs to encode contextual information?,        answer: The underlying idea is to collect information of connecting one utterance to all other utterances that appear later chronologically.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " How are the contextual encoding vectors Vr and Vc combined to get the final context-aware potential score?,        answer: The final context-aware potential score is obtained by adding Vr and Vc together.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " What is the purpose of using a linear transformation layer in the overall process?,        answer: The linear transformation layer is used to convert the contextualized vectors to individual scores of a discourse pair.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " How is the non-projective multi-root spanning tree constraint defined in the learning and inference process?,        answer: The non-projective multi-root spanning tree constraint is defined by using a tree T as the collection of discourse pairs {(h, m, r)} and imposing it in the learning and inference processes.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " What role does the Matrix-Tree Theorem play in the process of learning the tree?,        answer: The Matrix-Tree Theorem permits efficient calculation of the summation of scores of all possible trees, which is crucial for optimizing the reference tree structure during learning.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " What does the partition function Z(\u03b8) represent in the context of computing the probability of a reference tree structure?,        answer: The partition function Z(\u03b8) is the sum of scores of all trees in T(D) and is used to calculate the probability of the reference tree structure.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " What is the numerator s(T) of any tree T defined as in the tree-structured learning process?,        answer: The numerator s(T) of any tree T is defined as the exponential of the scores of the discourse pairs in the tree.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " How is the exponential matrix Ah,m constructed to account for edge labels in the learning process?,        answer: The exponential matrix Ah,m is constructed by setting cells to zero or the exponential of the score depending on whether h is greater than or equal to m.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " What is the challenge faced in performing batchwise training on GPUs according to the text?,        answer: The challenge is handling variable-length padding during batchwise training, as padding with zeros may lead to singular matrices and erroneous results.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}, {"question": " How is the padding issue overcome in batchwise training on GPUs?,        answer: The padding issue is overcome by using the cofactor expansion formula instead of padding zeros, to prevent singular matrices and incorrect partition results.    ", "ref_chunk": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}], "doc_text": "(McDonald et al., 2005; Koo et al., 2007) d Input DialoguePairwise Potential Vectors17Relation ScalarsperPair : Fancy some tasty ore?Dave: I do not ^_^dummy rootTomm: But you can you know...smelt it?Tomm 1234501234 1234501234 1234501234 : How about clay?Dave: nope, sorry!\"!#!$!%!& !\",$ 17 .. + ... ... !\",$%d =!' LSTM !\",$%LSTM Tomm Figure 2: The contextual encoding process. Row numbers from 0 to 4 represent h, and column numbers from 1 to 5 represents m. In this example, the 1-st utterance can connect to the 2-nd, 3-rd or 5-th utterances when predicting the V1,4 cell. This is represented by the orange rectangles. Similarly, the 4-th utterance can have 0, 1, or 3-rd utterances as its parent, represented by the green rectangles. We use two LSTMs for two directions (orange and green). Finally, we add the contextualized vectors together to get the purple vector Linear(V r 1,4 + V c 1,4) = \u03b81,4. utterances. The representation of the [CLS] token is further used to calculate d-dimensional pairwise representation: Vh,m = BERTCLS(Uh, Um) One immediate drawback of eq. (1) is that the pair- wise scores are calculated independently. We al- leviate this issue by using a bidirectional LSTM to encode the contextual information. For the h-th row, we obtain the hidden states for all timestep t: {V r h,t}n t=h+1 = LSTM({Vh,t}n t=h+1) The underlying idea is that to accurately decide if Uh should point to Um, we should collect the infor- mation of connecting Uh to all the other utterances that appear later chronologically. Similarly, for the m-th column, all the hidden states are: t,m}m\u22121 t=0 = LSTM({Vt,m}m\u22121 t=0 ) {V c The final context-aware potential score is: \u02dcVh,m = V r h,m + V c h,m \u02dcV \u2208 R(n+1)\u00d7(n+1)\u00d72d. Every pairwise score is now aware of neighboring pairs. It still remains to convert \u02dcV to individual score of a discourse pair. We do so by simply passing \u02dcV through a linear transformation layer: (1) (2) (3) (4) assume \u03b8 to be in log space. Another important property of \u03b8 is that it is a strictly upper-triangular matrix due to the h < m constraint. In practice, this can be enforced by setting the lower triangular and diagonal elements to -inf. We illustrate the overall idea in Figure 2. 4.2 Learning the Tree The parameterization we define in eq. (5) still does not impose any structural constraints. Based on our conclusion in \u00a7 3, we would like to impose a non- projective multi-root spanning tree constraint in the learning and inference process. We define a tree T to be the collection of discourse pairs {(h, m, r)}. We use T (D) to denote all possible trees of a di- alogue session D. During learning, the reference tree structure \u00afT \u2208 T (D) is given. If the score of \u00afT and the summation of the scores of all trees in T (D) is tractable, we can obtain the probability of \u00afT and optimize it using gradient descent. The challenge lies in the exponentially many candidates of T (D), which is computationally infeasible to naively enumerate. Fortunately, we will see that the Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) permits efficient calculation of the summa- tion we need. 4.2.1 Matrix-Tree Theorem \u03b8h,m = Linear( \u02dcV ) where \u03b8 \u2208 R(n+1)\u00d7(n+1)\u00d717. Note that there is no activation function after the linear layer since we (5) Before we dive into the details of the Matrix-Tree Theorem, we have to give enough credits to Tutte (1984); Koo et al. (2007) as we are applying their proposed theorem. The probability of the reference \"$(\")$()$()$()'(') ','-'/ ','-'/ \"=$ Para. ','-'.'/ ','-'.'/ QA.Cont.Corr.QA.QE.'. QA.Cont.Corr.QA.QE.'. '(') ! ++++ . . . \"='(') QA.Contr.QE.QA.Expl.QE. '*'+ '*'+ '*'+ '*'+ QA.Cont.Corr.QA.Contr.QE.'(') Figure 3: This is the graphical illustration of of how we perform tree-structured learning. Note that we are summing the scores of all possible labeled trees as the denominator, which is eq. (11). tree \u00afT , which is to be optimized, can be defined as: P( \u00afT ) = s( \u00afT ) Z(\u03b8) , Z(\u03b8) = (cid:88) s(T ) T \u2208T (D) Z(\u03b8) is also known as the partition function. The numerator s(T ) of any tree T is defined to be: s(T ) = (cid:89) exp(\u03b8h,m,r), (6) (7) whre \u02c6L is defined to be the submatrix constructed by removing the first row and column from L. The computational complexity of eq. (11) is de- termined by the determinant operation, which is O(n3). While the cubic time complexity might seem scary at the first glance, it does not incur significant computational overhead in our experi- ments, where the time to compute the determinant is negligible (< 1%) compared to BERT encoding in eq. (1). (h,m,r)\u2208T With this definition, the score is merely the product of corresponding cells in exp(\u03b8) (\u03b8 from eq. (5)). Next, we need to find an efficient way to com- pute the partition function as there are exponen- tially many candidate trees. The first step is to cal- culate the exponential matrix Ah,m,r from eq. (5): Ah,m,r(\u03b8) = (cid:40) 0, exp(\u03b8h,m,r), if h \u2265 m otherwise Note that the first 0 = exp(\u2212inf ) condition ap- plies to all h \u2265 m cells as \u03b8 is an upper-triangular matrix described earlier. To account for edge labels, we have to marginalize r out : Ah,m(\u03b8) = (cid:88) Ah,m,r(\u03b8) r (8) (9) 4.2.2 Efficient GPU Implementation The equations derived so far work well for a single training instance. However, it becomes problematic if we want to perform batchwise training on GPUs, which was not addressed in Koo et al. (2007). The main challenge is the variable-length padding. In particular, we have to calculate batchwise deter- minants in eq. (11) with different sizes of \u02c6L. The naive option is to pad the extra rows and columns with zeros. Unfortunately, this would result in a singular matrix and give erroneous partition results. To circumvent the padding issue, we can use the cofactor expansion formula. Concretely, all the di- agonal elements of the padding part"}