{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Chenyan_Xiong_OpenMatch-v2:_An_All-in-one_Multi-Modality_PLM-based_Information_Retrieval_Toolkit_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the OpenMatch-v2 toolkit?", "answer": " The main focus of the OpenMatch-v2 toolkit is on Information Retrieval models powered by Pre-trained Language Models (PLMs).", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " Where can the code of OpenMatch be found?", "answer": " The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " What are some of the key features of OpenMatch-v2 compared to the previous version?", "answer": " Some key features of OpenMatch-v2 compared to the previous version include further improvements in dense text retrieval, re-ranking, query synthesis, unsupervised pre-training, and efficient data accessing, among others.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " What is the significance of Pre-trained Language Models (PLMs) in Information Retrieval (IR) models?", "answer": " PLMs have emerged as the foundation of advanced IR models, enabling novel models, domain adaptation algorithms, and larger datasets in IR research.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " What is the primary focus of recent IR research according to the text?", "answer": " Recent IR research has become almost exclusively PLM-centric and is conducted under larger, more diverse, and cross-modality datasets.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " Why was OpenMatch-v2 developed as an upgrade to OpenMatch?", "answer": " OpenMatch-v2 was developed as an upgrade to OpenMatch to catch up with the latest trends in IR research, enhance flexibility, scalability, and support new-generation PLM-based IR models with improved domain adaptation techniques.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " What new backbone for IR models is introduced in OpenMatch-v2?", "answer": " OpenMatch-v2 introduces T5 as a new backbone for IR models in addition to traditional BERT-series models.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " What types of retrieval does OpenMatch-v2 support?", "answer": " OpenMatch-v2 supports CLIP-based text-to-image retrieval, structured data retrieval including code and product search, in addition to traditional dense text retrieval and re-ranking.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " What is the role of template-based data processing in OpenMatch-v2?", "answer": " Template-based data processing in OpenMatch-v2 helps researchers extract data from different datasets in a general way and provides an efficient method for data accessing and searching to handle large corpora with limited resources.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}, {"question": " What are some of the types of algorithms supported in OpenMatch-v2 for domain adaptation?", "answer": " OpenMatch-v2 supports algorithms such as query synthesis, unsupervised pre-training, and re-ranker score distillation for domain adaptation to enhance model performance in few-shot scenarios.", "ref_chunk": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}], "doc_text": "OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Shi Yu Tsinghua University Beijing, China yus21@mails.tsinghua.edu.cn Zhenghao Liu Northeastern University Shenyang, China liuzhenghao@mail.neu.edu.cn Chenyan Xiong Microsoft Research Redmond, USA chenyan.xiong@microsoft.com Zhiyuan Liu Tsinghua University Beijing, China liuzy@tsinghua.edu.cn ABSTRACT Pre-trained language models (PLMs) have emerged as the foun- dation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel mod- els, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch- v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, opti- mized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch. CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and ranking; Eval- uation of retrieval results. Table 1: Comparison of the functions of OpenMatch-v2 with the previous version. Double check \u201c\u2713\u2713\u201d denotes further im- provements over the previous version. Features OpenMatch OpenMatch-v2 IR Models Domain Adaptation Infrastructure Dense Text Retrieval Re-ranking Cross-Modality Retrieval Query Synthesis Unsupervised Pre-training Re-ranker Score Distillation Template-based Data Processing Efficient Data Accessing Sharded Search \u2713 \u2713 \u2713 \u2713\u2713 \u2713\u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 KEYWORDS PLM-based IR, dense retrieval, re-ranking, open-source toolkit ACM Reference Format: Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch- v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3539618.3591813 To facilitate research on IR, in early 2021, we proposed Open- Match [17], an open-source toolkit for developing neural ranking models. OpenMatch was integrated with state-of-the-art neural ranking models and few-shot learning methods for domain adapta- tion at the time. With the help of OpenMatch, we achieved success in ranking on a set of IR benchmarks like MS MARCO [1] and TREC-COVID [30]. 1 INTRODUCTION Recent advances in deep learning and neural networks have re- shaped traditional bag-of-words Information Retrieval (IR) systems into intelligent agents which better understand users\u2019 intent and sat- isfy their information needs [21, 38, 40]. The advent of Pre-trained Language Models (PLMs) further brings a leap forward in data representation [13, 32, 39] and matching [21, 22, 24] for IR. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3591813 Since then, however, IR research has made significant progress. It has become almostly PLM-centric, learned and tested under larger, more diverse, or even cross-modality datasets [23, 25, 34]. New IR models [6, 22, 32] based on next-generation PLMs [26] are proposed, and algorithms for domain adaptation are further improved [11, 35]. The original design of OpenMatch lacked the flexibility and scalability to adapt to these new changes in IR. To catch up with the latest trends of IR in 2023, we present OpenMatch-v2, a complete upgrade of OpenMatch. OpenMatch- v2 serves as a comprehensive platform for IR training, inference, and evaluation, and has been fully refactored to accommodate new-generation PLM-based IR models with a unified interface and improved domain adaptation techniques. The infrastructure of OpenMatch-v2 has also been enhanced to support efficient process- ing and search on data with growing size. Specifically, OpenMatch- v2 encapsulates popular IR models that are based on different PLMs, SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. introducing T5 [26] as a new backbone for IR models in addi- tion to traditional BERT-series models. It also supports CLIP-based text-to-image retrieval [25] and structured data retrieval, includ- ing code [10] and product search [27]. For domain adaptation, in addition to query synthesis [19, 33], we add supports for unsuper- vised pre-training [11] and re-ranker score distillation [9, 29, 35] to enhance model performance in few-shot scenarios. Concerning infrastructure, we propose template-based data processing as a gen- eral way to help researchers extract data from different datasets and an efficient way of data accessing and searching to help researchers handle large corpora with limited computational resources. We summarize the key updates of OpenMatch-v2 in Table 1. The rest of the paper is organized as follows. Section 2 reviews related work on PLM-based IR and existing IR toolkits. Section 3 gives an overview of OpenMatch-v2. Section 4 presents the main functions and upgrades of OpenMatch-v2 with examples. 2 RELATED WORK PLM-based IR. In recent years, Pre-trained Language Models (PLMs) have become widely applied in Information Retrieval (IR). Among the earliest PLM-based IR models is the BERT re-ranker [3, 21, 24], which concatenates the query and document with the [SEP] token and adds a linear head to obtain the ranking score. T5-based re-rankers [22] are the current state-of-the-art, employing a prompt- based training method for greater effectiveness. Another line of research proposes dense retrieval (DR) [13, 32, 39], which can be described as \u201cdual encoders\u201d, where the query and document are encoded separately through their own PLM-based encoders. All documents are encoded as embedding vectors for later approxi- mate nearest neighbor (ANN) search. Research on DR has explored topics including negative mining techniques [32, 39], pre-training approaches [6, 11, 18] for better domain adaptation, and ranking knowledge distillation from re-rankers [9, 29]. In OpenMatch-v2, we focus on PLM-based re-rankers and retrievers since they are the most effective and efficient models to date. IR Toolkits. Numerous IR toolkits have been developed by re- searchers with various focuses. MatchZoo [8] implements a series of traditional text-matching models but is not compatible with PLM- based models. Capreolus1 implements traditional lexical models and neural models including BERT-based re-rankers. Tevatron [7] is a DR-only IR package optimized for efficiency and flexibility. Open- Match [17] provides support for dense retrieval and re-rankers, along with some data augmentation techniques. Sentence Trans- formers [28] has full support for PLM-based dense retrieval and re-ranking but lacks an efficient infrastructure."}