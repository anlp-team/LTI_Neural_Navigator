{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Reverse-Engineering_Decoding_Strategies_Given_Blackbox_Access_to_a_Language_Generation_System_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two most popular decoding strategies discussed in the text?", "answer": " top-k and nucleus sampling (or top-p)", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " What are the consequences of knowing the decoding method used to generate text?", "answer": " It becomes easier to detect whether a writing sample was generated by a language model or human-written, and it helps in reducing biases introduced by decoding settings.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " What are the implications of discovering the decoding strategy used for generating text?", "answer": " It helps in detecting generated text and uncovering biases caused by decoding settings which truncate a model\u2019s predicted distributions.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " What are some common applications of neural language models mentioned in the text?", "answer": " They are deployed in APIs and websites that allow users to input prompts and receive generated text.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " What is the primary goal of the algorithms presented in the text?", "answer": " To identify the decoding strategy employed for text generation, even in a blackbox setting.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " Why is argmax decoding rarely used in practice?", "answer": " Because it only allows generating one output for a given prompt and tends to produce repetitive and low-quality text.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " In top-k sampling, how are tokens selected for output?", "answer": " Tokens are chosen from the k most likely items, where all other items are assigned a score of 0 before sampling.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " What is the purpose of modifying the distribution in random sampling strategies?", "answer": " To reduce entropy in the distribution before sampling to prevent text from being too erratic and error-prone.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " What is the difference between top-k sampling and top-p sampling?", "answer": " Top-k sampling selects a fixed number of most likely items (k) while top-p sampling selects items to cover a fixed proportion (p) of the total probability mass.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}, {"question": " Why are strategies like beam search and temperature annealing omitted from the discussion in the text?", "answer": " They require more complex implementations and are not the focus of the current paper.", "ref_chunk": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}], "doc_text": "3 2 0 2 p e S 9 ] G L . s c [ 1 v 8 5 8 4 0 . 9 0 3 2 : v i X r a Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Daphne Ippolito\u2217 dei@google.com Nicholas Carlini* ncarlini@google.com Katherine Lee* katherinelee@google.com Milad Nasr* miladnasr@google.com Yun William Yu\u2020 ywyu@math.toronto.edu Abstract sided die, we found that it only returns 14 of the 20 options, even though all should be equally likely. Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse- engineer the decoding method used to generate text (i.e., top-k or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Addi- tionally, the process of discovering the decoding strategy can reveal biases caused by selecting de- coding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT). 1 Introduction Language models are increasingly being incorporated into web applications and other user-facing tools.1 These applications typically do not provide direct access to the underlying language model or the decoding configuration used for generation. In this paper, we show how even in this blackbox setting, it is possible to identify the decoding strategy employed for generation. We consider the case where one only has access to a system that inputs a prompt and out- puts a generated response. We present algorithms for distinguishing the two most popular decoding strate- gies, top-k and nucleus sampling (a.k.a. top-p), and estimating their respective hyperparameters (k and p). The choice of decoding strategy\u2014the algorithm used to sample text from a language model\u2014has a profound impact on the randomness of generated text, introducing biases toward some word choices. For ex- ample, when OpenAI\u2019s ChatGPT,2 a chatbot built with large language models, is repeatedly passed a prompt asking it to report the outcome of rolling a twenty- \u2217Google Deepmind, \u2020University of Toronto 1E.g., see https://gpt3demo.com/ for a list of such Prior work has shown that knowing the decoding method makes it easier to detect whether a writing sample was generated by a language model or else was human-written (Ippolito et al., 2020). As generated text proliferates on the web, in student homework, and elsewhere, this disambiguation is becoming increasingly important. Concurrent work to ours by Naseh et al. (2023) has developed similar strategies for detecting decoding strategy from a blackbox API: however, they focus more on identifying hybrid decoding strategies (includ- ing beam search), whereas we focus more on prompt engineering to produce close-to-uniform token distri- butions that reduce the number of queries needed. Our proposed methods complement but are not compara- ble to those of Tay et al. (2020). Their method trains classifiers that input a generated text sequence and output a prediction for the decoding strategy used to generate it. In contrast, our method interacts with an API and does not require any data or ML training. 2 Background Neural language models are not inherently generative. language model f\u03b8 takes as input a A causal sequence of tokens x1,...,xt\u22121 and outputs a score for each possible next token xt, computing the a likelihood score for each token in the vocabu- lary, which can be transformed into a probability distribution by applying a softmax such that Prob(xt|x1,...,xt\u22121)\u223cf\u03b8(x1,...,xt\u22121). A decoding method takes this probability distribution as input and samples a particular token to output. The simplest algorithm is argmax decoding (also known as \u2018greedy decoding\u2019), where the most likely next token is outputted. Argmax is rarely used in practice because (1) only one generation can be produced for any given prompt, and (2) generations with argmax tend to be repetitive and low-quality. apps. 2https://openai.com/blog/chatgpt/ Most commonly used decoding algorithms are based on random sampling: a token is chosen with probability proportional to the likelihood assigned to it by the model. Whereas argmax sampling has too little randomness, purely random sampling over the full distribution can have too much, leading to text that is too erratic and prone to errors. Thus, it is common to modify the distribution to reduce entropy before sampling from it. In this short paper, we focus on two popular strategies researchers have developed for decoding: top-k sampling (Fan et al., 2018) and top-p sampling (Holtzman et al., 2019) (also known as nucleus sampling). Top-k sampling involves the implementer picking a fixed hyperparemter k then only ever sampling from the k most likely items by assigning all other items a score of 0 before applying the softmax. Top-p sampling involves the implementer picking a fixed hyperparamter p. Then at each step t of generation, a kt is selected such that the kt most likely vocabulary items cover p proportion of the total prob- ability mass in the distribution. More precisely, let the notation x(l) refer to the lth most likely token in the distribution predicted at step t. We set kt to the first value for which (cid:80)kt l=1Prob(xt = xl|x1,...,xt\u22121) \u2265 p. Then, the distribution is truncated to the kt most likely tokens, as described above for top-k. Other common methods like beam search and temperature annealing are omitted in the interest of space (cf. Zarrie\u00df et al. (2021) and Wiher et al. (2022)). Temperature annealing simply modifies the probability distributions of the output tokens, so the methods in this manuscript can be easily generalized (and indeed were in the concurrent work of Naseh et al. (2023)). Beam search is a bit more complicated, as tokens are not chosen independently of previous tokens; instead, multiple candidate token paths are retained. As such, it would be necessary to generate more than a single word for each prompt, which is the primary interrogative tool we use here. 3 Method 3.1 Threat Model We assume black-box, query-only access to the system"}