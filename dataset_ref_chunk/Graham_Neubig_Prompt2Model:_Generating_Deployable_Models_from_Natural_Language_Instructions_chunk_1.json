{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Prompt2Model:_Generating_Deployable_Models_from_Natural_Language_Instructions_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of Prompt2Model?", "answer": " The purpose of Prompt2Model is to generate deployable models from natural language instructions.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " How does Prompt2Model differ from traditional special-purpose NLP models?", "answer": " Prompt2Model is a step forward from traditional special-purpose NLP models as it is designed to create models that are conducive to deployment using a multi-step process.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " What is the framework of Prompt2Model based on?", "answer": " Prompt2Model is based on a multi-step process of dataset retrieval, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " What are some advantages of Prompt2Model over LLMs like GPT-3?", "answer": " Prompt2Model can produce models that outperform LLMs like GPT-3 by an average of 20% while being significantly smaller in size, up to 700 times.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " How does Prompt2Model help assess model reliability before deployment?", "answer": " Prompt2Model provides a way to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " What are the three main tasks demonstrated to show the effectiveness of Prompt2Model?", "answer": " The three main tasks demonstrated to show the effectiveness of Prompt2Model are dataset retrieval, dataset generation, and supervised fine-tuning.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " What role does dataset retrieval play in the Prompt2Model process?", "answer": " Dataset retrieval in Prompt2Model involves collecting training data by retrieving task-relevant annotated data to train the model.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " How does Prompt2Model automatically collect and synthesize task-specific knowledge?", "answer": " Prompt2Model automatically collects and synthesizes task-specific knowledge through three channels: dataset retrieval, dataset generation, and model retrieval.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " What is the significance of using a pre-trained language model in Prompt2Model?", "answer": " Using a pre-trained language model in Prompt2Model serves as the student model which is further fine-tuned and evaluated using the generated and retrieved data.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}, {"question": " What is the potential impact of Prompt2Model on the community?", "answer": " Prompt2Model can serve as a tool for quickly building small and competent NLP systems, enabling the community to produce task-specific models efficiently.", "ref_chunk": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}], "doc_text": "3 2 0 2 g u A 3 2 ] L C . s c [ 1 v 1 6 2 2 1 . 8 0 3 2 : v i X r a PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions Vijay Viswanathan1\u2217 , Chenyang Zhao1,2\u2217, Amanda Bertsch1, Tongshuang Wu1, Graham Neubig1 1Carnegie Mellon University, 2Tsinghua University Abstract Large language models (LLMs) enable system builders today to create competent NLP sys- tems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from tradi- tional special-purpose NLP models; they re- quire extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts pro- vided to LLMs, and uses it to train a special- purpose model that is conducive to deploy- ment. This is done through a multi-step pro- cess of retrieval of existing datasets and pre- trained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains mod- els that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model re- liability before deployment. Prompt2Model is available open-source at https://github. com/neulab/prompt2model.1 Question: What does LPC stand for? Context: The psychoacoustic masking codec was... BERT Score: 94.0, ChrF++: 58.9, EM: 61.5 Answer: linear predictive coding Retrieve DataGenerate Data Retrieve Pretrained model Answer questions given context from a relevant Wikipedia article. Prompt2ModelInput: Prompt (task description + optional examples)Output: Deployment-ready model Figure 1: Prompt2Model is a framework for generat- ing a small yet accurate model from a prompt. LLMs like GPT-3 (Brown et al., 2020; Liu et al., 2023b) offer a lighter-weight paradigm for NLP system construction through \u201cprompt- ing\u201d (Reynolds and McDonell, 2021). Practitioners can now write a prompt specifying the intended system behavior (optionally with a few demonstra- tions), and ask an LLM to generate a desired out- put via text completion. This makes it possible to prototype NLP systems rapidly for a variety of applications without writing a single line of code (Floridi and Chiriatti, 2020). 1 Introduction Traditionally, building an NLP model from scratch has been a substantial undertaking. An NLP practi- tioner seeking to solve a new problem would need to define their task scope, find or create data that specifies the intended system behavior, choose a suitable model architecture, train the model, assess its performance through evaluation, and then de- ploy it for real-world usage (Paleyes et al., 2022). \u2217equal contribution. 1Our demo video is posted at youtu.be/LYYQ_EhGd-Q. However, there is still a gap between proof- of-concept prototyping \u2014 showing LLMs can be prompted for a particular task \u2014 and practical de- ployment. Prompting LLMs can be expensive as they require either a significant amount of com- puting or access to commercial APIs, and their re- liance on the input prompt quality makes them un- stable compared to trained models (Min et al., 2022; Bubeck et al., 2023). Because practitioners usually do not have enough annotated validation data to measure their system performance, it is also more challenging for them to debug their systems be- fore deployment (Jiang et al., 2022). Additionally, LLM-prompted systems pose usability challenges. Practitioners have expressed concerns about the high serving cost and slow prediction time asso- ciated with using LLMs (Park et al., 2022), and those working in high-stakes domains cannot rely on commercial LLM APIs due to privacy concerns. For instance, sharing user data with LLM service providers is illegal for many applications in the US (Sezgin et al., 2022). In this work, we present Prompt2Model, a system that retains the ability to specify system behavior in a light-weight way through prompt- ing, while still resulting in a deployable special- purpose model, maintaining all the advantages thereof. Prompt2Model is designed as an auto- mated pipeline that extracts essential task informa- tion from users\u2019 prompts and then automatically collects and synthesizes task-specific knowledge through three channels: Dataset retrieval: Whenever possible, we col- lect training data by retrieving task-relevant annotated data (F\u00e4rber and Leisinger, 2021; Viswanathan et al., 2023). Dataset generation: We distill knowledge from an LLM (\u201cteacher model\u201d) by employing it to generate a pseudo-labeled dataset. Prior work has demonstrated that such a dataset can be used to train a smaller \u201cstudent\u201d model to emulate the behavior of the teacher model (Wang et al., 2021a; He et al., 2023; Gudibande et al., 2023). \u2022 Model retrieval: Based on the prompt, we iden- tify a pretrained language model whose paramet- ric knowledge is appropriate for the user\u2019s intent. This chosen model serves as the student model and is further fine-tuned and evaluated using the generated and retrieved data. Prompt2Model is designed to support differ- ent instantiations of each of these components. We provide a reference implementation where we demonstrate its utility with a gpt-3.5-turbo- based dataset generator, a dataset retriever based on DataFinder (Viswanathan et al., 2023), and a model retriever using BM25. We evaluate on three tasks covering both traditional NLP bench- marks and novel applications and find that, empiri- cally, Prompt2Model sometimes produces small models that outperform gpt-3.5-turbo when us- ing the same prompt as input. On 2 of these 3 tasks, we observe >20 point improvements over the gpt-3.5-turbo baseline, despite the final model produced by Prompt2Model being up to 700 times smaller. We also find that we can generate effective evaluation datasets; performance improvements on these synthetic clones of real benchmarks also hold on their real counterparts. We believe that Prompt2Model can serve the following purposes for the community: 1. A tool for quickly building small and com- petent NLP systems: Prompt2Model can be directly used to produce task-specific models"}