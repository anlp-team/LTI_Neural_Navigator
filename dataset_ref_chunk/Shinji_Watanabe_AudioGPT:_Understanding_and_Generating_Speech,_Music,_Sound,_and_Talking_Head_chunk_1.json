{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_AudioGPT:_Understanding_and_Generating_Speech,_Music,_Sound,_and_Talking_Head_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the multi-modal AI system proposed in the text?", "answer": " AudioGPT", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " How does AudioGPT complement Large language models (LLMs) like ChatGPT?", "answer": " By processing complex audio information, solving understanding and generation tasks, and supporting spoken dialogue with input/output interfaces (ASR, TTS).", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " What are some of the capabilities of AudioGPT as mentioned in the text?", "answer": " AudioGPT can handle speech, music, sound, and talking head understanding and generation in multi-round dialogues.", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " Why is processing audio modality information considered important for artificial general intelligence?", "answer": " It is considered important as humans communicate using spoken language, and processing audio modality information is essential for achieving artificial general intelligence.", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " What challenges are mentioned in training LLMs that support audio processing?", "answer": " Challenges include obtaining human-labeled speech data, limited conversational speech data availability, and the computational intensity and time-consuming nature of training multi-modal LLMs.", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " What is the purpose of using ChatGPT within the AudioGPT system?", "answer": " To serve as a general-purpose interface and bridge spoken language LLMs with audio foundation models for various audio understanding and generation tasks.", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " How is the process of AudioGPT divided according to the text?", "answer": " It is divided into four stages: modality transformation, task analysis, model assignment, and response generation.", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " What are some design principles used to evaluate multi-modal LLMs like AudioGPT?", "answer": " Consistency, capability, and robustness are the design principles used to evaluate multi-modal LLMs.", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " What are some key contributions of the paper regarding AudioGPT?", "answer": " The proposal of AudioGPT, outlining design principles, and testing in terms of consistency, capability, and robustness to handle complex audio tasks.", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}, {"question": " Why is it mentioned in the text that training multi-modal LLMs from scratch can be wasteful?", "answer": " Because there are existing audio foundation models available that can understand and generate audio information, making it unnecessary to start training from scratch.", "ref_chunk": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}], "doc_text": "3 2 0 2 r p A 5 2 ] L C . s c [ 1 v 5 9 9 2 1 . 4 0 3 2 : v i X r a AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Rongjie Huang1\u2217, Mingze Li1\u2217, Dongchao Yang2\u2217, Jiatong Shi3\u2217, Xuankai Chang3 Zhenhui Ye1, Yuning Wu4, Zhiqing Hong1, Jiawei Huang1, Jinglin Liu1, Yi Ren1, Zhou Zhao1, Shinji Watanabe3 Zhejiang University1, Peking University2, Carnegie Mellon University3, Remin University of China4 {rongjiehuang, limingze, zhaozhou}@zju.edu.cn, {dongchao98}@stu.pku.edu.cn, {jiatongs, xuankaic, dongsli}@cs.cmu.edu, {yuningwu}@ruc.edu.cn, {shinjiw}@ieee.org https://github.com/AIGC-Audio/AudioGPT Abstract Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of process- ing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robust- ness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. 1 Introduction Nowadays, Large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Ouyang et al., 2022; Zhang et al., 2022a) are posing a signi\ufb01cant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the advancement of natural language processing. Based on the massive corpora of web-text data and powerful architecture, LLMs are empowered to read, write, and communicate like humans. Despite the successful applications in text processing and generation, replicating this success for audio modality (speech (Ren et al., 2020; Huang et al., 2022a), music (Huang et al., 2021; Liu et al., 2022a), sound (Yang et al., 2022; Huang et al., 2023a), and talking head (Wu et al., 2021; Ye et al., 2023)) is limited, while it is highly bene\ufb01cial since: 1) In real-world scenarios, humans communicate using spoken language across daily conversations, and utilize spoken assistant (e.g., Siri or Alexa) to boost life convenience; 2) As an inherent part of intelligence, processing audio modality information is a necessity to achieve arti\ufb01cial general intelligence. Understanding and generating speech, music, sound, and talking head could be the critical step for LLMs toward more advanced AI systems. \u2217 Equal contributions Despite the bene\ufb01ts of audio modality, training LLMs that support audio processing is still challenging due to the following issues: 1) Data: Obtaining human-labeled speech data is an expensive and time-consuming task, and there are only a few resources available that provide real-world spoken dialogues. Furthermore, the amount of data is limited compared to the vast corpora of web-text data, and multi-lingual conversational speech data is even scarcer; and 2) Computational resources: Training multi-modal LLMs from scratch is computationally intensive and time-consuming. Given that there are already existing audio foundation models that can understand and generate speech, music, sound, and talking head, it would be wasteful to start training from scratch. In this work, we introduce \u201cAudioGPT\", a system designed to excel in understanding and generating audio modality in spoken dialogues. Speci\ufb01cally, 1) Instead of training multi-modal LLMs from scratch, we leverage a variety of audio foundation models to process complex audio information, where LLMs (i.e., ChatGPT) are regarded as the general-purpose interface (Wu et al., 2023; Shen et al., 2023) which empowers AudioGPT to solve numerous audio understanding and generation tasks; 2) Instead of training a spoken language model, we connect LLMs with input/output interface (ASR, TTS) for speech conversations; As illustrated in Figure 1, the whole process of AudioGPT can be divided into four stages: Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT under- stands the intention of a user to process audio information. Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. Response Generation. Generating and returning a \ufb01nal response to users after the execution of audio foundation models. Figure 1: A high-level overview of AudioGPT. AudioGPT can be divided into four stages, including modality transformation, task analysis, model assignment, and response generation. It equips ChatGPT with audio foundation models to handle complex audio tasks and is connected with a modality transformation interface to enable spoken dialogue. We design principles to evaluate multi-modal LLMs in terms of consistency, capability, and robustness. Task AnalysisModality Transformation Evaluating Multi-modal LLMs LLMsSpeech RecognitionSpeech TranslationText-to-SpeechText-to-Audio Audio Inpainting \u2026 RobustnessResponse Generation AudioModels Consistency WhisperMultiDecoderMake-An-AudioVISingerDiffSinger\u2026Assign AudioGPT Capability As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: 2 We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. Demonstrations present the ef\ufb01ciency of AudioGPT in audio understanding and generation with multiple rounds of dialogue,"}