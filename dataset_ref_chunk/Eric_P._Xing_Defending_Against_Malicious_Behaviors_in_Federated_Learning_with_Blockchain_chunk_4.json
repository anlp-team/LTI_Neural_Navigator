{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Defending_Against_Malicious_Behaviors_in_Federated_Learning_with_Blockchain_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the goal of malicious behaviors according to the text?", "answer": " The goal of malicious behaviors is to decrease the global model performance.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What does it mean for all clients to be rational?", "answer": " All clients, both honest and malicious, expect to maximize their gain or minimize their loss while achieving their goals.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What assumption is made about the ratio of honest clients to malicious clients in a federated system?", "answer": " The assumption is that there are always more honest clients than malicious clients, with \u03b7 being strictly smaller than 50%.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What is the condition regarding collusion among malicious clients in the text?", "answer": " There is no collusion among malicious clients, as each malicious client acts independently.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What is deemed almost impossible in the application scenarios of interest according to the text?", "answer": " It is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What is the first malicious behavior described in the text regarding interactions with the federated system?", "answer": " The first malicious behavior is proposing harmful local model updates.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What is the second malicious behavior described in the text regarding interactions with the federated system?", "answer": " The second malicious behavior is voting dishonestly.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What is the ratio denoted by \u03b7 in the text?", "answer": " The ratio of malicious clients among all clients, denoted as \u03b7, is \u03b7 = |Km| / K.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What is the consensus protocol that ensures the security of the underlying blockchain in the FL system?", "answer": " The underlying blockchain of the FL system is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}, {"question": " What is the process through which the aggregated global model is distributed to a randomly selected set of clients for validation?", "answer": " Instead of directly recording the aggregated global model in a block, a copy is downloaded by a randomly selected set of clients, denoted as voters, who run a local inference with it on their local test set.", "ref_chunk": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}], "doc_text": "and the clients. It is worth mentioning that this work focuses on the interactions between FL and blockchain, where blockchain computing (or mining, in a more fashionable sense) and the application of additional privacy-preserving techniques [18] are considered orthogonal research directions and thus beyond the scope of this work. k}K p, the client k updates \u03b8t\u22121 k \u2190 \u03b8t A1: The goal of malicious behaviors is to decrease the global model performance. This is also reflected in Sec. III-B. Under these assumptions, behaviors that are harmful to the system but do not influence the global model performance are beyond the scope of discussion in this work. An example is eavesdropping, i.e. cloning the model specifications. A2: All clients are rational. This means that both honest and malicious clients expect to maximize their gain or minimize their loss while achieving their goals. A3: Following previous studies on blockchain [13], we assume that \u03b7 is strictly smaller than 50%. This means there are always more honest clients than malicious clients in a federated system. A4: There is no collusion among malicious clients. That is to say, each malicious client acts independently. In the application scenarios of this work (e.g. Sec. V), there is a minimal bar for a client to participate in the system, e.g. basic qualifications or industry standards for clinical or financial institutes. Meanwhile, it is difficult to com- promise multiple clients with independent cybersecurity systems simultaneously. Thus, we deem that it is almost impossible to launch large-scale multi-agent attacks in the application scenarios of interest. As an exploratory study, this work considers the single-agent scenario. (See Sec. IV-E for the discussion on a multi-agent scenario.) \u2022 A5: There is no capacity constraint on the hardware, including computing, communication, and storage, allow- ing us to solely focus on the algorithmic side of the problem. B. Malicious Behaviors The definition of malicious behavior in this work is an action that intentionally decreases the global model perfor- mance. There are two types of actions for each client that interact with the federated system, i.e. a client can propose (i.e. be a proposer) and vote (i.e. be a voter). Proposing is to upload local model or gradient updates to the parameter server, while voting is a peer-review process to validate the \u201cvirtually\u201d aggregated model updates. The technical details of the two actions are described in Sec. IV. There are thus two corresponding malicious behaviors. The first malicious behavior is to propose harmful local model updates and the second one is to vote dishonestly. More specifically, in the second case, a client votes for approval when it is aware that the proposed model updates are poisoned and votes for rejection when there is no evidence that indicates that the proposed model updates are poisoned. It is worth mentioning that the clients themselves might not intentionally attack the FL system as they can be compromised by attackers. For simplicity, we define the clients that have malicious behaviors as malicious clients in this work, denoted as Km. We use \u03b7 to denote the ratio of malicious clients among all clients, i.e. \u03b7 = |Km| K , where | \u00b7 | is the cardinality of a set. A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD A6: The underlying blockchain of the FL system of interest is running securely with a consensus protocol that ensures the validity and integrity of transactions and blocks. While the security of the blockchain is crucial for the overall security of the FL system, addressing the malicious miners falls outside the scope of this study. IV. METHOD In this work, we illustrate the proposed framework in the the k=1 are uploaded and context of the seminal FL method, FedAVG [6]. At end of round t, the local models {\u03b8t aggregated as a weighted average: k}K \u03b8t 0 = K (cid:88) ak\u03b8t\u22121 k , k=1 where ak = nk local training examples stored in client k and N = (cid:80)K is the total number of training examples in the K clients. N . The metadata nk = |Dk| is the number of k=1 nk B. Local Validation C. Assumptions There are six important assumptions in this work. In contrast to standard FL algorithms, the aggregated global model is not recorded in a block directly. Instead, \u02dc\u03b8t 0, a copy of \u03b8t 0 is downloaded by a randomly selected set of clients, denoted as voters, Kt v. A voter k runs a local inference with \u02dc\u03b8t 0 on its local test set and outputs a local validation score. (1) FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE Proposers Proposers Proposers Evaluate Participants Voters Voters Voters Train Evaluate global model update candidates Revert and Slash 5 5 Miners Miners Voters Voters Evaluate Voters Voters Voting Voting Selection Selection 2 2 2 Scores Slash Multiple Rounds Global Voting and Approval Assets local model updates 3 Reward Malicious Clients Elimination Setup Finalization Stake Evaluate Evaluate Miners Miners 4 Local Training 3 global model update candidates Evaluate Evaluate Evaluate Miners Local Training Finalization and Reward Aggregate voting scores Aggregate voting scores local model updates Scores Read Read Remove Train Participants Global Voting and Refusal 6 Train Train Train Train Train Train 4 Voters 1 Proposers Revert Proposers Proposers Proposers Proposers Fig. 3. A round-based training process. In the initial state (indexed as \u2460), both honest (black) and malicious (red) clients exist in an FL system. In the final state (indexed as \u2465), all malicious clients are expected to be removed from the system. To reach the final state from the initial state, multiple rounds of training are required. Here are two possible scenarios, the proposed aggregation is either approved (the upper"}