{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_UNSSOR:_Unsupervised_Neural_Speech_Separation_by_Leveraging_Over-determined_Training_Mixtures_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the formulation suggest about the existence of a solution for separation?", "answer": " The formulation suggests that there exists a solution for separation, which is most consistent with the linear system.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " In over-determined cases, how is it possible to estimate the speaker images?", "answer": " In over-determined cases, it is possible to estimate the speaker images in an unsupervised way.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " Why is F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C used in the context of gp(c, f )?", "answer": " F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is used because gp(c, f ) is E-tap and there is one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " What is the blind deconvolution problem mentioned in the text?", "answer": " The blind deconvolution problem mentioned in the text is non-convex in nature and difficult to be solved without assuming any prior knowledge about the relative RIRs or the speaker images.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " What is the proposed system illustrated in Figure 1?", "answer": " The proposed system illustrated in Figure 1 is a DNN-based approach that models speech patterns through unsupervised learning for tackling the blind deconvolution problem.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " How is the intermediate estimate \u02c6Z(c) obtained for each speaker c?", "answer": " The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping, where the real and imaginary parts of the input mixture are stacked as features for the DNN to predict the RI parts of \u02c6Z(c).", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " What is the computation performed by the mixture-constraint loss (MC)?", "answer": " The mixture-constraint loss (MC) is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P-channel input mixture.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " What is the purpose of FCP in relative RIR estimation?", "answer": " FCP is employed to estimate the relative RIR relating the \u02c6Z(c) to the speaker image captured at each microphone p, assuming that the speakers are non-moving.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " How are the actual number of filter taps (A and B) defined in the FCP process?", "answer": " The actual number of filter taps (A and B) in the FCP process are set to I and J, both of which are hyper-parameters to tune.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}, {"question": " What is the weighting term used in balancing the importance of each T-F unit in relative RIR estimation?", "answer": " The weighting term used in balancing the importance of each T-F unit in relative RIR estimation is defined as \u03be/(max(\u00b7))P.", "ref_chunk": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}], "doc_text": "In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way. 1T \u00d7 F \u00d7 C is because there is one unknown for each X1(c, t, f ), and F \u00d7 (P \u2212 1) \u00d7 E \u00d7 C is because gp(c, f ) is E-tap and we have one such filter for each of P \u2212 1 microphone pairs for each frequency and speaker. 3 (1) (2) !! Microphone! DNNmulti-channelinput: FCP FCP FCP 1 1 \u2a01 \u2112.45/6.6\u2a01 ,3!()* ,3!()* Microphone\" + \u2026 \u2026\u2026\u2026\u2026 !&,\u2026,!!,\u2026,!'ormonauralinput:!&\"#(%) %,3!()* \"#(+)\"# ,!!\u2112\"),! \u2112#+\"+,! Figure 1: Illustration of UNSSOR (assuming P > C during training). As \u03b5 is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem: C (cid:88) C (cid:88) P (cid:88) (cid:12) gp(c, f )H (cid:101)X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) X1(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12)Yp(t, f ) \u2212 (cid:12) (cid:12) (cid:12)Y1(t, f ) \u2212 2 2 (cid:88) (cid:88) . + argmin g\u00b7(\u00b7,\u00b7),X1(\u00b7,\u00b7,\u00b7) c=1 c=1 p=2 t,f t,f (3) This is a blind deconvolution problem [57], which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem. 4 Method Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the P microphones or at the reference microphone 1 as input and produces an intermediate estimate \u02c6Z(c) for each speaker c. FCP [29] is then performed on \u02c6Z(c) at each microphone p to compute a linear-filtering result, denoted as \u02c6X FCP (c), which, we will describe, is essentially an estimate of the speaker image Xp(c). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation. p 4.1 DNN configurations The intermediate estimate \u02c6Z(c) for each speaker c is obtained via complex spectral mapping [23, 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \u02c6Z(c). For the DNN architecture, we employ TF-GridNet [23], which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details. 4.2 Mixture-constraint loss on filtered estimates Following formulation in (3), we propose mixture-constraint (MC) loss, which is computed by filtering the DNN estimate \u02c6Z(c) of each speaker c to approximate the P -channel input mixture: P (cid:88) C (cid:88) C (cid:88) (cid:88) (cid:88) \u02c6gp(c, f )H (cid:101)\u02c6Z(c, t, f )). \u02c6Z(c, t, f )) + LMC = \u03b11 F(Y1(t, f ), \u03b1p F(Yp(t, f ), (4) c=1 c=1 p=2 t,f t,f In (4), (cid:101)\u02c6Z(c, t, f ) stacks a window of T-F units around \u02c6Z(c, t, f ), and \u02c6gp(c, f ) is an estimated relative RIR computed based on \u02c6Z(c, \u00b7, f ) and the mixture Yp(\u00b7, f ) through FCP [29]. Both of them will be described in the next sub-section. \u03b1p \u2208 R is a weighting term for microphone p. Following [23], F(\u00b7, \u00b7) in (4) computes an absolute loss on the estimated RI components and their magnitude: (cid:16)(cid:12) 1 (cid:12)Re(Yp(t, f )) \u2212 Re( \u02c6Yp(t, f )) (cid:12) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| (cid:12) (cid:12) (cid:12)Im(Yp(t, f )) \u2212 Im( \u02c6Yp(t, f )) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) (cid:17) (cid:16) Yp(t, f ), \u02c6Yp(t, f ) F = (cid:80) (cid:12) (cid:12) (cid:12)|Yp(t, f )|\u2212| \u02c6Yp(t, f )| (cid:12) (cid:12) (cid:12) (cid:17) + , (5) 4 where Re(\u00b7) and Im(\u00b7) respectively extract RI components and |\u00b7| computes magnitude. The term 1/(cid:80) t\u2032,f \u2032|Yp(t\u2032, f \u2032)| balances the losses at different microphones and across training mixtures. According to the discussion in Section 3, minimizing LMC would encourage separation of speakers. We illustrate the loss surface of LMC in Appendix B. Compared to the mixture consistency term proposed in [59], our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions. 4.3 FCP for relative RIR estimation To compute LMC, we need to first estimate each of the relative RIRs, \u02c6gp(c, f ). In [29, 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \u02c6Z(c) to the speaker image captured at each microphone p (i.e., Xp(c)). Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem: \u02c6gp(c, f ) = argmin gp(c,f ) (cid:88) t 1 \u02c6\u03bbp(c, t, f ) (cid:12) (cid:12)Yp(t, f ) \u2212 gp(c, f )H (cid:101)\u02c6Z(c, t, f ) (cid:12) (cid:12) (cid:12) (cid:12) 2 , where gp(c, f ) \u2208 CI+1+J is a K-tap (with K = I + 1 + J) time-invariant FCP filter, (cid:101)\u02c6Z(c, t, f ) = [ \u02c6Z(c, t \u2212 I, f ), . . . , \u02c6Z(c, t, f ), . . . , \u02c6Z(c, t + J, f )]T \u2208 CK stacks I past and J future T-F units with the current one. Since the actual number of filter taps (i.e., A and B defined in the text below (2)) is unknown, we set them to I and J, both of which are hyper-parameters to tune. \u02c6\u03bbp(c, t, f ) is a weighting term balancing the importance of each T-F unit. Following [29], we define it as (cid:16) 1 \u02c6\u03bbp(c, t, f ) = , where \u03be (= 10\u22124 in this study) P is used to floor the weighting term and max(\u00b7) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A"}