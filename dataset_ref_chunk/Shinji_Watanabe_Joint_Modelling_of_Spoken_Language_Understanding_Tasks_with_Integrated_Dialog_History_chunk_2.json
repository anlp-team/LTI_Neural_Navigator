{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Joint_Modelling_of_Spoken_Language_Understanding_Tasks_with_Integrated_Dialog_History_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the sequence sc represent?", "answer": " The sequence sc represents a set of words from the vocabulary V with a length of Nc.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " What tags are assigned to each utterance for the NLU tasks?", "answer": " Each utterance is assigned tags for dialogue act classification, intent classification, speaker role prediction, and emotion recognition.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " How does NLU model seek to output Yda, Yic, Ysr, and Yer?", "answer": " NLU models seek to output \u02c6Yda, \u02c6Yic, \u02c6Ysr, and \u02c6Yer by maximizing the posterior distributions given S.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " What additional complexity does SLU introduce?", "answer": " SLU introduces the complexity of modeling dialog context from spoken utterances.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " How are transcripts computed in the separate E2E model according to Eq. 4?", "answer": " Transcripts are computed by passing ASR transcripts \u02c6s1:c-1 to a pretrained language model and concatenating them with the acoustic embeddings obtained from xc.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " What is proposed in the joint E2E model with dialog context discussed in section 3?", "answer": " The proposed joint E2E model extends the prior work and suggests jointly modeling all SLU tasks.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " What is the purpose of using the joint CTC-attention training in the model architecture?", "answer": " The joint CTC-attention training is used to train the attention model encoder as an auxiliary task in the model architecture.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " What issue is addressed by order agnostic training in this work?", "answer": " Order agnostic training addresses the label ambiguity problem, also known as the permutation problem, in predicting SLU tags.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " What is the role of the semantic encoder in the joint model architecture?", "answer": " The semantic encoder encodes the dialog history by processing ASR transcripts and SLU tags from previous spoken utterances.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}, {"question": " How is the joint embedding cjoin produced in the model architecture?", "answer": " The joint embedding cjoin is produced by concatenating acoustic and context embeddings and attending to them using a joint encoder.", "ref_chunk": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}], "doc_text": "sequence is represented as sc = {wcn \u2208 V|n = 1, . . . , Nc}, with length Nc and vocabulary V. Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from la- bel sets Lda, Lic,Lsr and Ler indicating dialogue act classi\ufb01cation, intent classi\ufb01cation, speaker role prediction, and emotion recogni- tion, respectively. This produces a label sequence of the same length C for each task, for instance, Y da = {yda c \u2208 Lda|c = 1, . . . ,C}. Using the maximum a posteriori theory, NLU models seek to out- put \u02c6Y da, \u02c6Y ic, \u02c6Y sr, and \u02c6Y er that maximise the posterior distribution P (Y da|S),P (Y ic|S),P (Y sr|S) and P (Y er|S) given S, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed by C spoken utterances, i.e., X = {xc|c = 1, . . . , C}. Each spoken utterance xc = {xct \u2208 Rd|t = 1, . . . , Tc} is a se- quence of d dimensional speech feature of length Tc frames. Similar to the NLU formulation, SLU systems seek to estimate the label se- quence \u02c6Y da, \u02c6Y ic, \u02c6Y sr and \u02c6Y er that maximise the posterior distribution P (Y da|X),P (Y ic|X),P (Y sr|X) and P (Y er|X) given X, respec- tively. We can model these posterior distributions as described in the subsections below. 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g., P (Y da|X), using a se- quence of transcripts S by applying the Viterbi approximation: P (Y da|X) = (cid:88) P (Y da|S,X)P (S|X) (1) S \u2248 max S P (Y da|S,X)P (S|X) (2) Their approach then assumes the conditional c |s1:c\u22121 from x1:c\u22121,yda yda 1:c\u22121 to simplify the Eq 2: independence of P (Y da|X) \u2248 max S C (cid:89) c=1 P (yda c |s1:c\u22121, xc)P (S|X) (3) Transcripts are computed using a separate ASR module that seeks to estimate \u02c6S that maximises P (S|X). Using \u02c6S, we can modify Eq 3: P (Y da|X) \u223c C (cid:89) P (yda c |\u02c6s1:c\u22121, xc) (4) c=1 Prior work [17] models P (yda c |\u02c6s1:c\u22121, xc) in Eq. 4 by passing ASR transcripts \u02c6s1:c\u22121 to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. They focus on building a sep- arate model for each of the SLU tasks, which in the above description is dialogue act classi\ufb01cation. Thus, to predict all the SLU tasks, all the separate models that estimate P (Y da|X),P (Y ic|X),P (Y sr|X), and P (Y er|X) independently need to be executed which can in- crease latency and computational cost and also does not consider the dependency between SLU tasks. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integra- tion discussed in section 2.1 and propose to jointly model all SLU tasks. We denote a single target containing all the SLU tags as DecodercjoinY CTC JointEncoder Concatccont xSContext || YContext SemanticEncodercaco AcousticEncoder Fig. 1: Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R = (Y da,Y ic,Y sr,Y er) and modify Eq. 4 with rc = (yda as shown below: c ,yic c ,ysr c ,yer c ) P (R|X) \u223c C (cid:89) P (rc|r1:c\u22121,\u02c6s1:c\u22121, xc) (5) c=1 The SLU tags predicted for the previous spoken utterances i.e. r1:c\u22121 = (yda 1:c\u22121,yer 1:c\u22121) may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda 1:c\u22121. In \u00a75.1, we con\ufb01rm experimentally whether this previous SLU tag condition is helpful. Further, by jointly modeling all the SLU tasks, we expect signi\ufb01cantly lower latency and lightweight inference. 1:c\u22121,yic 1:c\u22121,ysr c |s1:c\u22121 from yda To realize this formulation, we propose a joint model architec- ture shown in Fig. 1. The input speech signal for each utterance i.e., xc in Eq. 5, is passed through an acoustic encoder (Encoderaco) to generate acoustic embeddings caco. caco = Encoderaco(xc) (6) We concatenate ASR transcripts \u02c6s1:c\u22121 and SLU tags r1:c\u22121 for all previous spoken utterances and pass them through semantic encoder (Encodersem) like a pretrained LM to encode the dialog history: ccont = Encodersem(concat(\u02c6s1:c\u22121, r1:c\u22121)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings ccont have the same hidden dimen- sion as acoustic embeddings caco. The acoustic and context embed- dings are concatenated together (concat(caco, ccont)) and attended by a joint encoder Encoderjoin to produce the joint embedding cjoin: cjoin = Encoderjoin(concat(caco, ccont)) (8) The model is trained using joint CTC-attention training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregres- sive decoder to predict tags one by one (Eq. 11), the likelihood P (rc|xc,\u02c6s1:c\u22121, r1:c\u22121) is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. Inspired by prior work on permutation invariant training [30\u201332], we use CTC objec- tive function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order \u02c6\u03c0 is computed as: \u02c6\u03c0 = argmin \u03c0\u2208P LossCTC(z,r\u03c0 c ), (9) where P is the set of 4! possible permutations of SLU tags (da,ic,sr,er), \u03c0 is one such permutation and r\u03c0 c is the reference target with the order of SLU tags indicated by \u03c0. Later, the optimal Model DA (\u2191) IC (\u2191) SR (\u2191) ER (\u2191) RTF (\u2193) Slowest Total Latency (msec.) (\u2193) Slowest Total Parameters (\u2193) Train time (sec.) (\u2193) Separate E2E model without context* ([18]) Separate E2E"}