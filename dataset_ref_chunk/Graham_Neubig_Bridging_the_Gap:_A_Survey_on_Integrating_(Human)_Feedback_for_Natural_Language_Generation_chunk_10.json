{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Bridging_the_Gap:_A_Survey_on_Integrating_(Human)_Feedback_for_Natural_Language_Generation_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does recent comparative work suggest about RLHF?", "answer": " Recent comparative work suggests that RLHF might be outperformed by simpler, easier to leverage methods.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " What is still lacking in terms of a comprehensive study in feedback methods?", "answer": " A more comprehensive, large-scale study comparing more methods is still lacking.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " According to some recent work, what does the success of AI Feedback hint at?", "answer": " The success of AI Feedback hints that the need for human supervision can be massively reduced.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " How does some recent work question the need for feedback?", "answer": " Some recent work raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " What is the purpose of the survey mentioned in the text?", "answer": " The survey aims to help researchers understand the current state of the art and identify new and existing sources of feedback and ways of leveraging it.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " What organizations supported the work mentioned in the text?", "answer": " The work was supported by EU\u2019s Horizon Europe Research and Innovation Actions, UTTER, MAIA, and NextGenAI.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " Who are some of the authors mentioned in the references section?", "answer": " Some of the authors mentioned are Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, Dan Man\u00e9, Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, and others.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " What topic does the work of Chantal Amrhein and Rico Sennrich focus on?", "answer": " Chantal Amrhein and Rico Sennrich focus on identifying weaknesses in machine translation metrics through minimum Bayes risk decoding.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " What is the main focus of the work by Florian B\u00f6hm and colleagues?", "answer": " Florian B\u00f6hm and colleagues focus on how rewards yield better summaries by learning to summarize without references.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}, {"question": " What is the goal of the work by Tolga Bolukbasi and team?", "answer": " The goal of the work by Tolga Bolukbasi and team is debiasing word embeddings, specifically addressing gender biases.", "ref_chunk": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}], "doc_text": "merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking. 3. It\u2019s still unclear what role (human) feedback plays in improving the model\u2019s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning. Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it. 13 Acknowledgments This work was supported by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02). References Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105\u2013120. Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105\u2013120. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565. Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125\u20131141, Online only. Associ- ation for Computational Linguistics. Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Preprint Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Karl Johan \u00c5str\u00f6m and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1\u201316. 14 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Associa- tion for Computing Machinery. Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109\u2013117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics. Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110\u20133120, Hong Kong, China. Association for Computational Linguistics. Better Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam- Preprint cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang"}