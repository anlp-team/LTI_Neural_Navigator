{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_Vector_Quantized_Approach_for_Text_to_Speech_Synthesis_on_Real-World_Spontaneous_Speech_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of non-autoregressive TTS models?", "answer": " Non-autoregressive TTS models focus on better robustness, inference speed, and lower training difficulty.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is the purpose of a duration predictor in TTS?", "answer": " A duration predictor is trained to predict the text-audio alignment in TTS.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is VITS and how does it achieve near-human naturalness?", "answer": " VITS is an end-to-end model that uses a stochastic duration predictor for better diversity and achieves near-human naturalness by synthesizing speech on the VCTK dataset.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is the approach of quantization-based generative models in TTS?", "answer": " Quantization-based generative models use a two-stage approach where a quantizer encodes the speech into discrete tokens, followed by an autoregressive transformer for generation.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " How does DiscreTalk discretize speech in TTS?", "answer": " DiscreTalk discretizes speech using a VQVAE and a transformer for autoregressive generation.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is the purpose of unit-based language models in TTS?", "answer": " Unit-based language models train the transformer on discrete tokens derived from representations of self-supervised models for speech synthesis.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is the key difference between MQTTS and other TTS models?", "answer": " MQTTS differs from other TTS models in the use of multi-code and its specialized architecture designed for multi-code generation and monotonic alignment.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " How does MQTTS incorporate denoising in speech synthesis?", "answer": " MQTTS uses a denoising module trained jointly with the TTS system to reduce noise in speech synthesis.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is the training procedure of the model architecture MQTTS?", "answer": " The training of MQTTS involves two stages: quantization of raw waveform into discrete codes followed by autoregressive generation using a transformer conditioned on speaker embeddings and phoneme sequences.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}, {"question": " What is the architecture used for the quantizer in MQTTS?", "answer": " MQTTS adopts HiFi-GAN as the backbone architecture for the quantizer decoder QD and the discriminator D, with group normalization and speaker embedding added to enhance performance.", "ref_chunk": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}], "doc_text": "du- ration predictor. We will show that this technique actually degrades the performance of MQTTS. Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training dif\ufb01culty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a \ufb02ow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models. Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain suf\ufb01cient information for speech reconstruction. TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as \ufb01lled pauses (i.e., um, uh). We do not design speci\ufb01c architectures for these, but rather let the models learn implicitly. 3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the \ufb01rst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t \u2208 [1, \u00b7 \u00b7 \u00b7 , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i \u2208 [1, \u00b7 \u00b7 \u00b7 , N ] for N codebooks. In the second stage, we \ufb01x the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time. 3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD. Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi. codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the \ufb01nal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc t,i: ct,i = arg min ||zc t,i \u2212 z||2, (1) z\u2208Gi Then zq through gradient estimator: t , the input to the decoder is obtained by the straight- zq t,i = zc t,i + sg[Gi(ct,i) \u2212 zc t,i], zq t = concat[zq t,i] where sg[\u00b7] is the stop gradient operator, Gi(\u00b7) returns the embedding of the given code, and the concat[\u00b7] operator con- catenates all codes on the embedding dimension. The \ufb01nal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q: (2) Figure 2: Detailed view of the multi-output transformer dur-"}