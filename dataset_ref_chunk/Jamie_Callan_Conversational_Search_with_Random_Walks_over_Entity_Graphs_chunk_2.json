{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_Conversational_Search_with_Random_Walks_over_Entity_Graphs_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is query rewriting?", "answer": " Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " Why is using graph structures for multi-hop strategies considered a competitive approach?", "answer": " Using graph structures for multi-hop strategies is considered competitive because it helps estimate important terms during a conversation and allows exploration of neighboring levels of a knowledge base.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " What role do external knowledge bases play in conversational search?", "answer": " External knowledge bases provide additional information that may not be explicit in documents, and assist in the extraction and linking of named-entities across conversation turns.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " What are some examples of approaches that use external knowledge like Wikipedia or DBpedia in entity linking?", "answer": " Examples of approaches that use external knowledge like Wikipedia or DBpedia in entity linking include TagMe, DBPediaSpotlight, and Wikipedia2Vec.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " According to Joko et al., what was observed about the performance of different entity linkers in conversational search?", "answer": " Joko et al. observed that deep learning methods achieve a higher precision but very low recall in entity linking for conversational search.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " What is the goal of improving top precision in conversational search, according to the text?", "answer": " The goal is to improve user satisfaction by improving the top precision of conversational search to provide access to a wider set of relevant passages.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " How does the proposed lightweight approach leverage entity graphs in the context of conversational search?", "answer": " The proposed lightweight approach leverages entity graphs to encode query-passage interactions across the ranking and estimate entity centrality scores for reranking passages effectively.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " What are the two distinct stages of the lightweight reranking model proposed in the text?", "answer": " The two distinct stages are the full-text retrieval ranker and the entity centrality stage.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " What is the significance of named-entities in the entity graph construction?", "answer": " Named-entities in the entity graph construction provide a knowledge-aware view of the textual content and help in connecting queries and passages by estimating centrality over the entity graph.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}, {"question": " How does the turn-specific entity graph construction begin in the context of conversational search?", "answer": " The turn-specific entity graph construction starts with linking the entities of the query and respective retrieved passages obtained by the full-text retrieval ranker, with nodes representing entities and edges corresponding to their occurrence in the passages.", "ref_chunk": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}], "doc_text": "can encode, since they are very memory intensive. Given the limited information that novel strategies can encode it is also necessary to build better queries. Query rewriting is a competitive strategy in approximating the users\u2019 information need as the conversation evolves, and has been largely studied to sat- isfy users\u2019 information needs [23, 24, 37]. Additionally, it has been shown that using graph structures for multi-hop strategies is a competitive approach to estimate important terms during a conver- sation [5, 46, 48]. Graph-based methods allow the exploration of neighboring levels of a knowledge base, which can be used to infer the topics a conversation might follow. External knowledge bases provide additional information that may not be explicit in documents. A fundamental aspect is the extraction and linking of named-entities across the conversation turns. Before conversational search took centre stage, many entity linking works were proposed [11]. One family of approaches uses some form of external knowledge such as Wikipedia or DBpedia [1]. TagMe [11] and DBPediaSpotlight [25], are long-standing examples of such approaches. More recently, other approaches extend the external knowledge with representation learning in the form of embeddings such as Wikipedia2Vec [42]. Good examples include REL [39], BLINK [21], and GENRE [8]. Driven by the research in conversational assistants, Joko et al. [17] examined how different entity linkers behaved in this do- main. The authors observed that deep learning methods achieve a higher precision but very low recall. Overall, the best f-measure was achieved by the methods based on textual representations of Wikipedia [11]. Gustavo Gon\u00e7alves, Jo\u00e3o Magalh\u00e3es, and Jamie Callan There is a wide range of works exploring named-entities with the Transformer architecture [29, 47]. While these works have been successful in a number of tasks, there is not enough evidence that such approaches can improve the Transformer architecture in ad- hoc retrieval tasks, or conversational search tasks. It is interesting to see that previous improvements with named-entities [40, 41] have yet to be translated into the Transformer generation of ranking methods. See the experiments section of this paper for more details. 3 CONVERSATION ENTITY GRAPHS Our goal is to improve the top precision of conversational search, as this is an important factor for user satisfaction when dealing with conversational assistants. We hypothesize that the entities appearing in top-ranked passages are connected both by their co- occurrence and semantic relations, which can provide access to an extended set of relevant passages at lower-ranked positions that might be overlooked by neural rankers. We propose a lightweight approach that leverages on the top passage results of state-of-the- art neural rankers, and encodes the query-passage interactions across the ranking as an entity graph. Modern state-of-the-art ranking pipelines often start with a lexi- cal ranker, such as BM25 [33], to quickly obtain a set of documents that approximate a user\u2019s information need. Increasingly computa- tionally expensive rerankers are applied subsequently to reorder the documents and maximize relevance. Recent work uses neural language models, such as BERT, to reorder the documents obtained by the earlier ranker(s) [14]. This work proposes a lightweight reranking model that utilizes entity graphs as a representation of conversational context, and consists of two distinct stages. The first stage is a full-text retrieval ranker. In our work, the full-text retrieval ranker consists of a query likelihood ranker followed by a BERT [9] reranker, but any full-text retrieval system may be used. The full-text retrieval ranker produces a ranking of passages \ud835\udc43 = {\ud835\udc5d1, \u00b7 \u00b7 \u00b7 , \ud835\udc5d\ud835\udc58 }, an ordered collection of \ud835\udc58 candidate passages that answer a query \ud835\udc5e. The second stage analyzes \ud835\udc43 to estimate entity centrality scores, which are used to rerank passages more effectively. We refer to these two stages as the full-text retrieval and entity centrality stages throughout this paper. The following subsections explain the entity centrality stage of the reranking model, by decomposing the graph construction in Section 3.1, defining how to weight the graph edges in Section 3.2, and finally how to determine entity centrality in Section 3.3. 3.1 Nodes and Edges of the Entity Graph This work, inspired by previous competitive approaches that con- sider named-entities as a connective element between documents [10, 40, 41], infers the current conversation context by estimating centrality over an entity graph, thus connecting query and passages. Named-entities in queries and passages provide a knowledge- aware view of the textual content. Linking text to a knowledge- base is a starting point to obtain external connections that are not explicit in the query-passage text. We argue that entity occurrence is enough to provide information and reweight the full-text retrieval ranker passage scores. Thus, giving more importance to passages that contain entities central to the current query, but lack the exact query terms to be highly ranked by the full-text retrieval ranker. Conversational Search with Random Walks over Entity Graphs The turn-specific entity graph construction begins with linking the entities [17] of the query and respective retrieved passages obtained by the full-text retrieval ranker. The nodes of the graph are given by the entities in the passages, and the edges correspond to the occurrence in the passages. The set of unique entities \ud835\udc38 is computed from the current conversation query \ud835\udc5e, and the top retrieved passages \ud835\udc43. This leads to the set of unique entities \ud835\udc38 defined as: \ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc54, \u00b7 \u00b7 \u00b7 , \ud835\udc52\ud835\udc5b }, \u2200\ud835\udc52\ud835\udc54 \u2208 {\ud835\udc5e} \u222a \ud835\udc43 Given the set of \ud835\udc5b entities, \ud835\udc38, and the top \ud835\udc58 passages \ud835\udc43, we com- pute the entities-passage occurrence matrix CP as: CP \u2208 {\ud835\udc64\ud835\udc54}\ud835\udc5b\u00d7\ud835\udc58 To build the affinity matrix we consider the weighted occurrences of entities CP for each query \ud835\udc5e of the conversational search session, which will result in the entities that will be in the graph to calculate the centrality scores and rerank the top passages. The query vector CQ, Eq. (3), contains the entities mentioned in the query and we define it as a multi-hot vector: CQ \u2208 {0, 1}\ud835\udc5b\u00d71 This allows us to compute the"}