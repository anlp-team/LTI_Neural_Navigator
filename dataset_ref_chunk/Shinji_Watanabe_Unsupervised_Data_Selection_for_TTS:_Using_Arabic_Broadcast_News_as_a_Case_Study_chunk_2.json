{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Unsupervised_Data_Selection_for_TTS:_Using_Arabic_Broadcast_News_as_a_Case_Study_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some challenges related to speaker identification and linking in the text?,        answer: One of the main challenges is the inconsistency in speaker names, with many spelling mistakes in the metadata field names and attributes.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " How did the team overcome the challenges in speaker identification and linking?,        answer: They applied several iterations of automatic parsing and extraction followed by manual verification and standardization.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " What is the main purpose of selecting anchor speakers from the MGB-2 dataset for building the TTS corpus?,        answer: To have main speakers who speak clearly, record in a high-quality studio, and provide desired data for the TTS corpus.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " What are the six classes identified by investigating recordings for data selection?,        answer: Background music, wrong transcription, overlap speech, wrong speaker label, bad recording quality, and good recording.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " How did the team classify good recordings for the TTS system?,        answer: They hired a professional linguist to listen to all segments from selected anchor speakers and classified them into six classes.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " Which three methods were used for automatic data classification of good samples?,        answer: MOSNet, wv-MOS, and DNSMOS.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " What was the criteria for considering MOS scores as excellent samples in the automatic data classification?,        answer: MOS scores above 4 were considered excellent samples.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " What was the main intent behind using Arabic ASR to fix spelling mistakes in the TTS system?,        answer: To consider good samples with wrong transcriptions and achieve the goal of building a fully unsupervised TTS.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " Why is vowelization of text considered challenging?,        answer: It requires a complex system that considers linguistic rules and statistics, and there are differences in vowels for transcribed speech as compared to normal written text.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}, {"question": " Which models were used as autoregressive text-to-mel models in the architecture?,        answer: Tacotron2 and Transformer-TTS.    ", "ref_chunk": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}], "doc_text": "is conversational programs with overlapping speech and dialectal usage; and \ufb01nally metadata is not always available or standardized. 2.1. Speaker Identi\ufb01cation and Speaker Linking Majority of metadata information appears in the beginning of the \ufb01le. However, some of them are embedded inside the episode tran- scription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of meta- data \ufb01eld names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction fol- lowed by manual veri\ufb01cation and standardization. This data is pub- licly available 2. 2.2. Data Selection for TTS Corpus Our next step in building the TTS corpus is selecting anchor speak- ers from the MGB-2 dataset. We picked recordings from two an- chors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy envi- ronment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some record- ings, we can identify the following six classes and only the last one is our desired data: Background music normally happens at the beginning or at the end of each episode; Wrong transcription often happens due to the fact that broad- cast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correc- tion or rephrasing;Manual dataset classi\ufb01cation Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data (speaker linking results); Bad recording quality happens when the anchor speaker re- ports from a noisy environment, far-\ufb01eld microphone, or call- ing over the phone..etc; and Good recording, none of the above. 2.2.1. Manual dataset classi\ufb01cation To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and 2https://arabicspeech.org/qasr_tts classify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran exper- iments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part. 2.2.2. Automatic dataset classi\ufb01cation The second approach we investigated to choose the good samples is automatic data classi\ufb01cation because manual labelling is time con- suming. We use three different methods MOSNet, wv-MOS, and DNSMOS. MOSNet [18] is a deep learning-based assessment model to predict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to detect obvious distortions they trained a modern neural net- work model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjec- tive speech quality. DNSMOS [17] is a Convolutional Neural Network (CNN) based model that is trained using the ground truth human rat- ings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy. Our input to the proposed methods are the wav \ufb01les and the output is MOS score ranging from 1 to 5, with lowest score of 1 and high- est score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with man- ual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score pre- diction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wv- MOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any tran- scription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to \ufb01x the spelling mistakes. Table 1. Details of the TTS. Segments(Seg.) and Duration (Dur.) in minutes for each class Class Background music Wrong transcription Overlapped speech Wrong speaker Bad recordings Good Segments DNSMOS MOSNet wv-MOS Male # Seg. Dur. 631 134 1,860 22 2,200 1,200 687 1209 1315 70 15 200 3 240 60 53 70 63 Female # Seg. Dur. 131 1,586 62 930 260 4,028 12 292 105 1,312 70 1,094 97 1,241 87 1334 44 933 2.2.3. Vowelized Text Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23]. Furthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text di- acritizers. Additionally, the diacritized text should match the speak- ers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vow- elization on affecting the quality of the produced results. 3. ARCHITECTURE In this section, we will introduce the architecture of text-to-mel mod- els and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data. 3.1. Autoregressive models We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)- It consists of a bi-directional based sequence-to-sequence model. Long short-term memory (LSTM)-based encoder and a unidirec- tional LSTM-based decoder"}