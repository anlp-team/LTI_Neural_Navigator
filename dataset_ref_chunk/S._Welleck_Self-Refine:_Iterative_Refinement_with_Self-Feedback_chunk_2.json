{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Self-Refine:_Iterative_Refinement_with_Self-Feedback_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is SELF-REFINE according to the text?,answer: SELF-REFINE is a process that generates an initial output, receives feedback on the output, and refines the output based on the feedback using the same model.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " What does SELF-REFINE require for its operation?,answer: SELF-REFINE requires an input x, a model M, prompts for initial generation, feedback, and refinement, and a stop condition.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " How is the initial output generated in SELF-REFINE?,answer: The initial output in SELF-REFINE is generated by the model M using a specific prompt pgen and the input x.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " What is the purpose of feedback in SELF-REFINE?,answer: The feedback in SELF-REFINE is used to provide insights on the output generated by the model, focusing on aspects like efficiency, readability, and overall quality.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " How does SELF-REFINE use feedback to improve its output?,answer: SELF-REFINE uses the feedback provided by the model to refine its most recent output by incorporating the feedback into the refinement process.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " What is the role of the stopping condition in SELF-REFINE?,answer: The stopping condition in SELF-REFINE determines when to stop the iterative process, based on a specified timestep or a stopping indicator extracted from the feedback.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " What tasks are mentioned as part of the evaluation of SELF-REFINE?,answer: The evaluation of SELF-REFINE includes tasks such as Dialogue Response Generation, Code Optimization, Code Readability Improvement, Math Reasoning, Sentiment Reversal, Acronym Generation, and Constrained Generation.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " How does SELF-REFINE alternate between FEEDBACK and REFINE steps?,answer: SELF-REFINE alternates between FEEDBACK and REFINE steps until the stopping condition is met, allowing the model to learn from past mistakes and improve its outputs.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " What does the PROMPT prefine provide in SELF-REFINE?,answer: The prompt prefine provides examples of improving the output based on the feedback, assisting in the iterative refinement process in SELF-REFINE.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}, {"question": " How does SELF-REFINE rely on supervision during its operation?,answer: SELF-REFINE relies only on the supervision present in the few-shot examples provided in the prompts for generating, receiving feedback, and refining its outputs.", "ref_chunk": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}], "doc_text": "and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial output y0: y0 = M (pgen\u2225x) . 2 (1) (a) Dialogue: x, yt (b) FEEDBACK fb (c) REFINE yt+1 User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt (e) FEEDBACK fb (f) REFINE yt+1 Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. Code (refined) def sum_faster(n): return (n*(n+1))//2 return res generated by the base LLM and then passed Figure 2: Examples of SELF-REFINE: an initial output back to the same LLM to receive feedback . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. to the same LLM to refine the output Algorithm 1 SELF-REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7) 1: y0 = M(pgen\u2225x) 2: for iteration t \u2208 0, 1, . . . do 3: 4: 5: 6: 7: 8: 9: end for 10: return yt f bt = M (pfb\u2225x\u2225yt) if stop(f bt, t) then break else yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) end if \u25b7 Initial generation (Eqn. 1) \u25b7 Feedback (Eqn. 2) \u25b7 Stop condition \u25b7 Refine (Eqn. 4) Figure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x(k), y(k)\u27e9 for the task.2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own output, given a task-specific prompt pfb for generating feedback: f bt = M (pfb\u2225x\u2225yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020). 3 Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples \u27e8x(k), y(k), f b(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k). By \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the output. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \u201d. This feedback is actionable, since it suggests the action \u2018use the formula...\u2019. The feedback is specific since it mentions the \u2018for loop\u2019. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M (prefine\u2225x\u2225yt\u2225f bt) . For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x(k), y(k) , f b(k) t , y(k) t+1\u27e9. t Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M (prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) . Finally, we use the last refinement yt as the output of SELF-REFINE. Algorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating SELF-REFINE We instantiate SELF-REFINE following the high-level description"}