{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Enhancing_Speech-To-Speech_Translation_with_Multiple_TTS_Targets_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the difference between VITS and other text2Mel models mentioned in the text?", "answer": " VITS directly models the process from text to waveform (text2wav) without needing additional vocoders.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " What are the three different vocoders that are used for investigation with text2Mel models?", "answer": " Parallel WaveGAN (PWG), Hi\ufb01-GAN (HFG), and StyleMel-GAN (SMG).", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " What aspect of NAR models is investigated in the text?", "answer": " The effect of duration control by tuning the speed factor in the inference.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " What is the focus of the study in terms of TTS systems?", "answer": " The study focuses only on single-speaker TTS systems.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " What are the steps outlined for constructing an S2ST system in the text?", "answer": " 1. Target speech synthesis, 2. Discrete unit extraction, 3. S2ST system training, 4. Inference.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " What information can speech discrete units potentially disentangle according to the text?", "answer": " Speech discrete units can potentially disentangle linguistic, prosodic, and speaker-related information.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " How is the framework proposed in the text designed to capture high-level consensus over linguistic information?", "answer": " By adding separate decoder branches for speech discrete units generated from different TTS systems.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " What is the special token added at the start of target unit sequences in the proposed framework?", "answer": " The special token is defined as an indicator of the quality of synthesized speech.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " What dataset is used for S2ST experiments in the text?", "answer": " The Fisher Spanish-English dataset.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}, {"question": " What corpus is used for training TTS systems in the text?", "answer": " LJSpeech, a 24-hour single-speaker corpus.", "ref_chunk": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}], "doc_text": "a classical AR TTS text2Mel model, while Fastspeech2 is a typical NAR TTS text2Mel model. VITS, different from others (text2Mel + vocoder), directly models the process from text to waveform (text2wav), which does not need additional vocoders. For text2Mel models (i.e., TT2 and FS2), we adopt three different vocoders for investigation: Par- allel WaveGAN (PWG) [29], Hi\ufb01-GAN (HFG) [30], and StyleMel- GAN (SMG) [31]. We also investigate the effect of duration control for NAR models by tuning the speed factor in the inference. For the \ufb01rst set of our experiments, we evaluate the S2ST model with different target TTS speech. Then, we further conduct experi- ments on combining synthesized speech from different TTS systems. For this study, we focus only on single-speaker TTS systems. Overall Work\ufb02ow: Based on the previous work discussed, the over- all work\ufb02ow for constructing an S2ST system is outlined below: (1) Target speech synthesis: Target speech is synthesized using a TTS model, which can be either an acoustic model and vocoder or a direct text2wav model. (2) Discrete unit extraction: The synthe- sized target speech is converted into discrete units using a HuBERT model by clustering. (3) S2ST system training: The S2ST model is trained using source speech as input and target discrete units as out- put. (4) Inference: During inference, the S2ST model converts the source speech into a sequence of discrete units. Then, a unit-based vocoder is applied to generate the \ufb01nal waveform speech. 2.2. The Proposed Framework As in [21, 24], speech discrete units from speech SSL representa- tions can potentially disentangle linguistic, prosodic, and speaker- related information. However, at the same time, it is still noisy to use. For example, in [12], the authors have shown that the same sen- tence spoken by different speakers, could result in different speech discrete unit sequences. A similar phenomenon may happen when the same sentence is generated by different TTS systems. To ver- ify our hypothesis, we measure the Pearson correlation coef\ufb01cients of the HuBERT units\u2019 distribution between different TTS systems trained on LJSpeech [32].1 As shown in Fig. 2, it clearly indicates that different TTS systems are still different though with the same linguistic source and trained on the same corpus. The data includes 1The con\ufb01guration of HuBERT and TTS systems are discussed in Sec. 3.2 in detail. 0.98 10.970.930.9710.960.930.961 1.00 0.96 0.92 0.94 0.90 VITSFS2TT2VITSFS2TT2 Fig. 2. The Pearson correlation coef\ufb01cients between different TTS systems: the unit distribution is collected with the development set of the Fisher Spanish-English dataset [18]. For the wave generation from FS2 and TT2, we both utilize the Hi\ufb01-GAN vocoder. the development set of the Fisher Spanish-English corpus [18]. On the other hand, given the same utterance in the target language, the synthesized speech should have the same linguistic content. There- fore, it is reasonable to assume that the extracted units could have a similar consensus shared across, given the same text is employed to generate speech from different TTS systems. Following the assumption discussed above, we propose the framework as shown in Fig. 1. The framework is based on the model proposed in [7] but is additionally designed to capture the high-level consensus over linguistic information across different TTS systems. To be speci\ufb01c, we add separate decoder branches for speech dis- crete units generated from different TTS systems. For simplicity, in Fig. 1, we show the case with two targets, but it could be easily extended into three or more targets because of its parallel property. Instead of directly predicting the units in parallel, we also ap- pend a special token at the start of target unit sequences as shown in the gray blocks in Fig. 1. The special token is de\ufb01ned as an indicator of the quality of synthesized speech, which we can use to select bet- ter output during model inference. Practically, we \ufb01rst compute the character error rates (CER) at the sentence level for each utterance from different TTS systems. Then, at the training stage, we assign token [Y] to the TTS system with the best CER among candidate target speech discrete units and token [N] to other systems.2 For inference, we compute the probability of the \ufb01rst predicted special token [Y] from all the decoders and select the one with the highest probability to continue generating the sequences. Noted that since the special token is at the start of the sequence, the inference process does not need to auto-regressively generate future tokens if it already has a lower probability than other branches. Therefore, compared to the base system without multiple TTS targets, there is not much ad- ditional searching burden when doing inference. Due to the noise present in discrete units, a similar approach was explored in [12]. The authors proposed a speaker normalization method to normalize the units of different speakers to a reference speaker. However, in the case of using different TTS systems, it can be challenging to determine which system should be used as the reference, as they are all synthesized using the same text. 3. EXPERIMENTS 3.1. Datasets For S2ST, we use the Fisher Spanish-English dataset [18], which is also widely used in the previous S2ST works [7,9]. The English TTS systems are applied to synthesize target speech from the English text for training and validation. For the training of TTS systems, we use LJSpeech, a 24-hour single-speaker corpus [32]. 2For even cases, we assign [Y] to all systems with the best CER. 3.2. Experimental Settings 3.2.1. Model architectures Speech-to-unit translation model (S2UT): We follow the updated version of S2UT model described in [11], which is an extension of [7]. For speech discrete unit generation, we adopt the pre-trained multilingual HuBERT, K-Means clustering, and the unit-based HFG vocoder released in [11, 12]. The vocabulary size of the discrete unit is 1,000, corresponding to the number of clusters in the K-Means model. The generated discrete units are reduced by duplication- pooling, during the training of S2UT models. On the other hand, the reduced units are recovered to their original lengths through"}