{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Joint_Modelling_of_Spoken_Language_Understanding_Tasks_with_Integrated_Dialog_History_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the proposed model architecture?", "answer": " The main focus of the proposed model architecture is to jointly model intent classification, dialogue act prediction, speaker role identification, and emotion recognition with full integration of dialog history in spoken conversations.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " What does the joint model achieve in terms of performance compared to task-specific models?", "answer": " The joint model achieves comparable performance to task-specific models with the additional benefits of low latency and lightweight inference.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " What does the joint model successfully capture to improve the prediction performance of all SLU tasks significantly?", "answer": " The joint model successfully captures dialog context to improve the prediction performance of all SLU tasks significantly.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " What does the study recommend for future studies on jointly modeling different SLU tasks?", "answer": " The study recommends incorporating order agnostic training in the framework of future studies on jointly modeling different SLU tasks.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " What does the joint model learn to automatically predict during inference?", "answer": " The joint model learns to automatically predict in the optimal tag order during inference using order agnostic training.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " Which environment was used for the work mentioned in the acknowledgement section?", "answer": " The work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by NSF grant number ACI-1548562.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " What do the results show regarding the use of order agnostic training?", "answer": " The results show that order agnostic training can further enhance performance.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " What is the focus of the work by R. De Mori, F. Bechet, D. Hakkani-Tur, et al. as mentioned in the references?", "answer": " The focus of the work is on spoken language understanding.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " What approach does the joint model take for dialogue act prediction according to the references?", "answer": " The joint model takes a context-based approach for dialogue act prediction.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}, {"question": " What does the study experimentally confirm regarding training for performance enhancement?", "answer": " The study experimentally confirms that order agnostic training can further enhance performance.", "ref_chunk": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}], "doc_text": "with optimal tag order \u03c0 are also predicted by the joint model in the same tag order \u03c0 during inference. This \ufb01nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 6. CONCLUSION We propose a novel model architecture that can jointly model intent classi\ufb01cation, dialogue act prediction, speaker role identi\ufb01cation and emotion recognition with full integration of dialog history in spoken conversations. Our results show that the joint model achieves com- parable performance to task-speci\ufb01c models with the additional ben- e\ufb01ts of low latency and lightweight inference. Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi\ufb01cantly. We experimentally con- \ufb01rm that order agnostic training can further enhance performance. In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 7. ACKNOWLEDGEMENT This work used the Extreme Science and Engineering Discovery En- vironment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. Speci\ufb01cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). 8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al., \u201cSpoken language understanding,\u201d IEEE Signal Processing Magazine, vol. 25, no. 3, pp. 50\u201358, 2008. [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al., \u201cToward conversational human-computer interaction,\u201d AI magazine, vol. 22, no. 4, pp. 27\u201327, 2001. [3] B. Agrawal, M. M\u00a8uller, M. Radfar, et al., \u201cTie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,\u201d arXiv preprint arXiv:2011.09044, 2020. [4] S. Cha, W. Hou, H. Jung, et al., \u201cSpeak or chat with me: End-to-end spoken language understanding with \ufb02exible inputs,\u201d arXiv, 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T\u00a8ur, et al., \u201cEasy contex- tual intent prediction and slot detection,\u201d in Proc. ICASSP, 2013, pp. 8337\u20138341. [6] P. Xu and R. Sarikaya, \u201cContextual domain classi\ufb01cation in spoken language understanding systems using recurrent neural network,\u201d in Proc. ICASSP, 2014, pp. 136\u2013140. [7] P. Colombo, E. Chapuis, M. Manica, et al., \u201cGuiding attention in sequence-to-sequence models for dialogue act prediction,\u201d in Pro- ceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 34, 2020, pp. 7594\u20137601. [8] C. Bothe, C. Weber, S. Magg, et al., \u201cA context-based approach for di- alogue act recognition using simple recurrent neural networks,\u201d arXiv preprint arXiv:1805.06280, 2018. [9] V. Raheja and J. Tetreault, \u201cDialogue act classi\ufb01cation with context- aware self-attention,\u201d arXiv preprint arXiv:1904.02594, 2019. [10] S. Kim, S. Dalmia, and F. Metze, \u201cGated embeddings in e2e speech recognition for conversational-context fusion,\u201d in Proc. ACL, 2019, pp. 1131\u20131141. [11] \u2014\u2014, \u201cCross-Attention End-to-End ASR for Two-Party Conversa- tions,\u201d in Proc. Interspeech, 2019, pp. 4380\u20134384. [12] T. Hori, N. Moritz, C. Hori, et al., \u201cTransformer-based long-context end-to-end speech recognition.,\u201d in Proc. Interspeech, 2020, pp. 5011\u2013 5015. [13] Y.-N. Chen, D. Hakkani-T\u00a8ur, G. T\u00a8ur, et al., \u201cEnd-to-end memory net- works with knowledge carryover for multi-turn spoken language un- derstanding.,\u201d in Proc. Interspeech, 2016, pp. 3245\u20133249. [14] C. Sankar, S. Subramanian, C. Pal, et al., \u201cDo neural dialog systems use the conversation history effectively? an empirical study,\u201d arXiv preprint arXiv:1906.01603, 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al., \u201cSequential dialogue con- text modeling for spoken language understanding,\u201d arXiv preprint arXiv:1705.03455, 2017. [16] V. Vukoti\u00b4c, C. Raymond, and G. Gravier, \u201cA step beyond local ob- servations with a dialog aware bidirectional gru network for spoken language understanding,\u201d in Proc. Interspeech, 2016, pp. 3241\u20133244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al., \u201cIntegrating dialog his- tory into end-to-end spoken language understanding systems,\u201d arXiv preprint arXiv:2108.08405, 2021. [18] V. Sunder, S. Thomas, H.-K. J. Kuo, et al., \u201cTowards end-to-end in- tegration of dialog history for improved spoken language understand- ing,\u201d in Proc. ICASSP, 2022, pp. 7497\u20137501. [19] N. Tomashenko, C. Raymond, A. Caubri`ere, et al., \u201cDialogue history integration into end-to-end signal-to-concept spoken language under- standing systems,\u201d in Proc. ICASSP, 2020, pp. 8509\u20138513. [20] S. Arora, S. Dalmia, X. Chang, et al., \u201cTwo-pass low latency end- to-end spoken language understanding,\u201d in Proc. Interspeech, 2022, pp. 3478\u20133482. [21] T. O\u2019Malley, A. Narayanan, Q. Wang, et al., \u201cA conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\u201d in Proc. ASRU, 2021, pp. 304\u2013311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., \u201cEnd-to-end dereverber- ation, beamforming, and speech recognition with improved numerical stability and advanced frontend,\u201d in Proc. ICASSP, 2021, pp. 6898\u2013 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., \u201cJoint endpointing and decoding with end-to-end models,\u201d in Proc. ICASSP, 2019, pp. 5626\u2013 5630. [24] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for CTC- based speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6224\u20136228. [25] M. Wu, J. Nafziger, A. Scodary, et al., \u201cHarpervalleybank: A domain- speci\ufb01c spoken dialog corpus,\u201d arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in Proc. ICASSP, 2022, pp. 7167\u20137171. [27] J. Devlin, M. Chang, K. Lee, et al., \u201cBERT: pre-training of deep bidi- rectional transformers for language understanding,\u201d in Proc. NAACL- HLT, 2019, pp. 4171\u20134186. [28] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to- end speech recognition using multi-task learning,\u201d in Proc. ICASSP, 2017, pp. 4835\u20134839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., \u201cDeep clustering: Dis- criminative embeddings for segmentation and separation,\u201d in Proc. ICASSP, 2016, pp. 31\u201335. [31] D. Yu, M. Kolb\u00e6k, Z.-H. Tan, et al., \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in Proc. ICASSP, 2017, pp. 241\u2013245. [32]"}