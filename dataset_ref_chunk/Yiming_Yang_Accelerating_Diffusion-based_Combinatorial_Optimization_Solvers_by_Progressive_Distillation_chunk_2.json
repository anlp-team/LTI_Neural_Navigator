{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Accelerating_Diffusion-based_Combinatorial_Optimization_Solvers_by_Progressive_Distillation_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What loss function is applied to the predicted noise and the actual noise to optimize the predictor?,answer: Mean Squared Error (MSE) Loss", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " What model is used to obtain the predicted solution x0 in the text?,answer: Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM)", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " What is the purpose of the Progressive Distillation algorithm in the DIFUSCO framework?,answer: To accelerate the inference speed of diffusion models while preserving solution quality", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " How does Progressive Distillation update the student model in each iteration?,answer: To make its one denoising step match two denoising steps from the teacher model", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " What is the objective of iteratively training a series of student models in Progressive Distillation?,answer: To progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " What is the performance metric used in the experiments described in the text?,answer: Solution cost drop percentage on the test set", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " What noise predictor neural network is used in the text?,answer: 12-layer anisotropic Graph Neural Network", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " What diffusion scheduler is fixed in the text?,answer: Linear denoising schedule from \u03b21 = 10^-4 to \u03b2T = 0.02", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " What is the purpose of using the linear skipping strategy in the experiments described in the text?,answer: For fast inference", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}, {"question": " What percentage of performance degradation was observed in the 4x distilled student compared to the teacher when restricting the student to 1/4 of the teacher inference steps?,answer: 0.019%", "ref_chunk": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}], "doc_text": "and given the corrupted \u02c6xt and timestep t. \u02dc\u03f5t = \u03f5\u03b8(\u02c6xt, t) We apply the Mean Squared Error (MSE) Loss to \u02dc\u03f5t and \u03f5t and optimize \u03f5\u03b8 with respect to this loss, producing a performant predictor of noise that we could use to iteratively denoise and produce the predicted solution \u02c6x0. The predicted solution \u02c6x0 is obtained with the Gaussian Posterior from Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020). GuassianPosterior(\u02dc\u03f5t, \u02c6xt) = (cid:114) \u00af\u03b1t\u22121 \u00af\u03b1t (cid:0)\u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1t\u02dc\u03f5t (cid:1) + (cid:112)1 \u2212 \u00af\u03b1t\u22121\u02dc\u03f5t We finally perform quantization to obtain x0 from \u02c6x0. 2 (1) (2) (3) (4) (5) (6) (7) Algorithm 1 Progressive Distillation on DIFUSCO - training Require: Trained teacher model (cid:98)\u03f5\u03b7(\u00b7, \u00b7) Require: Data Set D, Student sampling steps N , Distillation factor K for K iterations do \u03b8 \u2190 \u03b7 while not converged do Sample { x \u223c D, xt = \u03b1tx + \u03c3t\u03f5 t\u2032 = t \u2212 0.5/N, t\u2032\u2032 = t \u2212 1/N #2 steps of teacher denoising xt\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt, t), xt] xt\u2032\u2032 = GaussianPosterior\u03b7 [ \u02c6\u03f5\u03b7(xt\u2032 , t\u2032), xt\u2032 ] #1 step of student denoising \u02dcx = GaussianPosterior\u03b8 [ \u02c6\u03f5\u03b8(xt, t), xt] L = ||xt\u2032\u2032 \u2212 \u02dcx||2 2 \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207\u03b8L \u25b7 Init student from teacher i \u223c Categorical [1, 2, . . . , N ], t = i/N, \u03f5 \u223c N (0, I) } \u25b7 MSE loss \u25b7 Gradient Update on student model parameters end while \u03b7 \u2190 \u03b8 N \u2190 N/2 \u25b7 Student becomes the new teacher \u25b7 Halve number of sampling steps end for 2.3 Progressive Distillation The DIFUSCO framework is capable of producing high-quality solutions for CO problems like TSP-50, TSP-100, etc. However, this framework is often inefficient in inference due to the need of a considerable number of inference diffusion steps to generate reasonable solutions. Aiming to accelerate this method while preserving solution quality, we propose to use progressive distillation to accelerate its inference. Progressive Distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022) is an algorithm designed to accelerate the inference speed of diffusion models. For one iteration of progressive distillation, we update the student model to make its one denoising step match two denoising steps from the teacher model. We then iteratively train a series of student models in this manner, where the teacher model in each iteration is initialized from the converged student model in the previous iteration. The objective is to progressively distill knowledge from one model to the next, gradually improving the performance of the student model with fewer inference steps. By the end of multiple distillation iterations, it becomes viable for the yielded progressively distilled student model to compress the diffusion steps required to achieve the same level of quality of solution by multiple times. We describe the progressive distillation training algorithm for DIFUSCO in 1. 3 Experiments We benchmark our results against the TSP-50 dataset, where each training sample x0 is the solution to a TSP instance with exactly 50 vertices in the problem graph. We fixed the diffusion steps to 1024 in all experiments. For the teacher model and each student model, we vary the number of inference steps as powers of 2, from 4 to 64 steps. The result of the distilled students after 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 progressive distillation iterations are shown in Figure 1. We also perform 4\u00d7 parallel sampling for all experiments in Figure 1 and report results in Figure 2. 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled students effectively have 512, 256, 128, and 64 diffusion steps during training. Model Setup Following Sun and Yang (2023), we set the noise predictor neural network \u02c6\u03f5\u03b8(\u00b7, \u00b7) to be a 12-layer anisotropic Graph Neural Network (Bresson and Laurent, 2018) with a width of 256. We fix the diffusion scheduler {\u03b2i}T i=1 to be a linear denoising schedule from \u03b21 = 10\u22124 to \u03b2T = 0.02. We use DDIM Song et al. (2020) for fast inference and use a linear skipping strategy for all experiments (Sun and Yang, 2023). Performance Metrics Per convention, we use the solution cost drop percentage on the test set as the metric of performance. Solution cost drop percentage is calculated as the the difference between yielded solution cost and ground truth cost divided by the ground truth cost. 3 Figure 1: TSP-50 results for 1\u00d7, 2\u00d7, 3\u00d7, and 4\u00d7 distilled student model performance against an optimized teacher model performance. We vary the number of inference steps and tabulate performance drop on each choice of inference step. Figure 2: We enable 4\u00d7 parallel sampling where we decode 4 samples in parallel and pick the best candidate. Same inference steps We first examine how much better our distilled student performs when the teacher and the student are allowed the same number of inference steps. As shown in Figure 1, we observed that our distilled student performs considerably better than the teacher when we allow the same number of inference steps for the student and teacher. The student is expected to compress multiple steps of the teacher into one step of its own. Therefore the student inferencing with a distill factor of k is theoretically equivalent to teacher inferencing with k times the student inference steps. When parallel sampling is enabled, we observe that performance gap between teacher and distilled students shrinks but does not vanish. Inference with 1/k of steps We also examine how minimal the performance degradation is when we restrict the student to inference with 1/k of teacher inference steps We observed that distilled students are able to retain minimal performance degradation compared to our already optimized teacher. Our 4x distilled student was able to compress 64 inference steps of teacher model into 4 steps. Comparing the result of student inferencing with 4 steps and teacher inferencing with 64 steps, we observed a performance degradation of 0.019%. 4 Related Work Prior work by Graikos et al. (2022) used image diffusion model to introduce visual inductive bias to aid solving CO problems. We would like"}