{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Grounding_Language_Models_to_Images_for_Multimodal_Generation_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " After filtering out missing images, how many examples remain in Captions?", "answer": " 33.1 million examples", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " What did the burgers and sausages in the text taste like?", "answer": " Delicious", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " What did Dad enjoy while he was manning the grill?", "answer": " A glass of wine", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " What time did people start to arrive for the cookout?", "answer": " Around 2 in the afternoon", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " What action did the author take over the summer?", "answer": " Went on a desert tour", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " What did the author mention about the desert tour environment?", "answer": " There were green plants growing there", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " In the interleaved image-and-text inputs, what model resulted in the best Recall@1 score?", "answer": " FROMAGe", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " What is one benefit of FROMAGe in processing multimodal contextual information?", "answer": " Greater sensitivity to input context", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " What is a significant gain of using image-and-text context on retrieval accuracy?", "answer": " 107% improvement on R@1", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}, {"question": " Why does FROMAGe outperform CLIP when greater context is provided?", "answer": " CLIP is unable to properly handle longer, temporally dependent sentences", "ref_chunk": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}], "doc_text": "Captions 33.1M examples remain after filtering out missing images. 4 Grounding Language Models to Images for Multimodal Inputs and Outputs \u201chigh resolution photography\u201d \u201cThe burger and sausages were cooked to perfection. The burgers were cooked on the grill and the sausages were smoked.\u201d It was good to get friends and family together for fun and food and drinks. \u201cpen drawing\u201d \u201cby the mountains\u201d This is us by the lake This is my pet gecko on the lawn + X \u201cvector icon\u201d \u201cvector icon\u201d \u201cin a forest\u201d The burgers and sausages were delicious. + X This is my cat looking very dignified + X The vegetable commonly used for Halloween \u201cpencil outline\u201d \u201chigh resolution dslr close up\u201d \u201cdetailed ink wash\u201d Dad enjoyed a glass of wine while he manned the grill. \u201cgouache painting\u201d Picture of ursus arctos horribilis \u201cdigital drawing\u201d [RET] = This is my dog \u201cat the beach\u201d \u201coil on canvas\u201d [RET] = A dish usually cooked at Thanksgiving This is it taking a bath \u201cin a showroom\u201d [RET] X = [RET] = [RET] = [RET] = [RET] = [RET] [RET] = [RET] [RET] [RET] [RET] [RET] [RET] \u201cat the lake\u201d [RET] = X = X = People started to arrive for the cookout around 2 in the afternoon. Figure 3. Selected examples from FROMAGe for various image-text tasks. FROMAGe is sensitive to context: it can generate multimodal dialogue, and rapidly learn in-context to perform various few shot image-text tasks. More examples are provided in the appendix. 5 Grounding Language Models to Images for Multimodal Inputs and Outputs I went on a desert tour over the summer. Retrieved Image Believe it or not, there were green plants growing there. This is our caravan as we left. ? Eventually we ran across a stone ridge. There was nothing but sand for quite some time. Input Context Figure 4. Contextual image retrieval conditioned on a Visual Story (Huang et al., 2016) of interleaved image-and-text inputs. Model CLIP ViT-L/14 FROMAGe CLIP ViT-L/14 FROMAGe BLIP\u2020 CLIP ViT-L/14\u2020 FROMAGe \u2020 CLIP ViT-L/14 FROMAGe \u2020 Inputs 1 caption 5 captions 5 captions 5 captions 5 captions 5 captions, 4 images 5 captions, 4 images R@1 R@5 R@10 11.9 11.3 25.5 24.6 32.2 32.1 5.9 11.9 19.5 23.8 28.0 31.7 6.2 8.8 13.2 2.4 18.2 16.8 22.3 28.5 21.3 42.7 23.4 29.8 36.7 34.0 51.8 and-text generation tasks, as few prior models are capable of this. We benchmark performance on various configurations of multimodal inputs, detailed in the following sections. Table 1. Recall@k on zero-shot contextual image retrieval of the last image in Visual Storytelling (Huang et al., 2016). Numbers in bold indicate best scores for a particular set of inputs. \u2020 indicates retrieval over images not previously seen in the story sequence. 4 4.1. Contextual Retrieval from Multimodal Inputs Prior work on image-text retrieval (Radford et al., 2021; Jia et al., 2021) typically focuses on retrieving a single im- age from a single caption (and vice versa). FROMAGe is adapted from a frozen LLM, and we find that it inherits several interesting behaviors of LLMs, such as in-context learning and greater sensitivity to input context. This ben- efits many downstream applications, such as multimodal dialogue or image-and-text generation (examples in Fig. 3). In order to evaluate the abilities of FROMAGe to process multimodal contextual information, we assess its perfor- mance in retrieving the appropriate image conditioned on a sequence of interleaved image-text inputs from the Vi- sual Storytelling (VIST) dataset (Huang et al., 2016). Each example in VIST consists of 5 images and text pairs in temporal order (Fig. 4). VIST examples are of \u201cstories\u201d, which are of a very different style compared to the image caption data FROMAGe is trained on. This allows us to evaluate our model\u2019s capability for in-context learning and zero-shot transfer. This also acts as an evaluation for dis- course capabilities, as VIST contains more free-form text. We benchmark over several different experimental setups: 1. Retrieve the last image correctly given its description. This is similar to standard image-text retrieval. the strongest open sourced and open domain image-text retrieval models available. We report Recall@k (R@k) met- rics in Tab. 1. For a single caption input, CLIP outperforms our model, which we attribute to the CLIP text encoder being a bidirectional model trained specifically for image- text retrieval,5 while our language model was trained on free-form text. However, as greater context is provided to both models, FROMAGe substantially outperforms CLIP. Given the full set of 5 captions, CLIP performance substan- tially deteriorates (as it appears to be unable to properly handle longer, temporally dependent sentences), with R@1 decreasing by 50.4% relative to the single caption setting. In contrast, FROMAGe is able use the additional descriptions to improve in retrieval accuracy (11.3 to 11.9 on R@1). FROMAGe also effectively conditions on multimodal con- text (which previous image-text retrieval models are not explicitly trained for). When both images and text inputs are provided to the model, retrieval improves significantly, increasing by 37.9% on R@1 relative to the caption-only setting (13.2 to 18.2). Similar improvements are seen on R@5 and R@10. This is a substantial gain over the baseline CLIP model with 5 captions: we achieve a relative improve- ment of 107% on R@1 (8.8 to 18.2) when image-and-text context is provided. 2. Retrieve the last image given the 5 preceding descrip- tions. Image-text pairs in VIST follow a temporal order. This tests the ability of retrieval models to con- dition on free-form temporally dependent language. 3. Retrieve the last image given the 5 preceding descrip- tions and 4 images. This tests the ability of retrieval models to process interleaved image-and-text context. Our results are presented in Table 1. We primarily com- pare against CLIP (Radford et al., 2021), as it is one of We also run an experiment to test the ability of CLIP to retrieve images conditioned on multimodal context. We embed each of the images and descriptions in the input, and average their embeddings. We find that it does substan- tially worse"}