{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_A_Gold_Standard_Dataset_for_the_Reviewer_Assignment_Problem_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What do the third and fourth columns in Table 3 investigate?", "answer": " The relative difference in performance between the non-trivial algorithms, displaying the differences in performance between TPMS and each of the more advanced algorithms.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " According to Table 3, what important observation can be made about the algorithms considered in the work?", "answer": " All algorithms considerably outperform the Trivial baseline, confirming that the content of papers is useful in evaluating expertise.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " How do Specter+MFR and Specter compare to ELMo according to the text?", "answer": " Specter+MFR and Specter outperform ELMo, with the former two relying on domain-specific embeddings while ELMo uses general-purpose embeddings.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " Which algorithm performs best under modeling choices that mimic those made in real conferences?", "answer": " The Specter+MFR algorithm performs best.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " What is a surprising finding mentioned in the text about the TPMS algorithm?", "answer": " TPMS is competitive against complex algorithms like Specter and Specter+MFR, even outperforming ELMo and ACL.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " How does TPMS differ from ELMo, Specter, Specter+MFR, and ACL in terms of complexity and performance?", "answer": " TPMS is much simpler but competitive in performance against these more complex algorithms.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " What is noted about the performance of advanced algorithms compared to the TPMS algorithm?", "answer": " The performance of the best algorithm (Specter+MFR) is comparable to the simpler TPMS algorithm.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " Why is it crucial to increase the size of the dataset mentioned in the text?", "answer": " To enable more fine-grained comparisons between the algorithms, as some confidence intervals overlap.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " What is the trade-off between providing richer representations of papers to similarity-computation algorithms?", "answer": " Richer representations are envisaged to result in higher accuracy but are also associated with an increased demand for computational power.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}, {"question": " Why is the TPMS algorithm chosen for experiments mentioned in the text?", "answer": " TPMS can work with all possible choices of paper representation and is fast to execute, enabling experiments with hundreds of configurations in a reasonable time.", "ref_chunk": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}], "doc_text": "The third and the forth columns investigate the relative di\ufb00erence in performance between the non-trivial algorithms: they display the di\ufb00erences in performance between TPMS and each of the more advanced algorithms (OpenReview algo- rithms and ACL) together with the associated con\ufb01dence intervals. We make several important observations from Table 3: First, we note that all algorithms we consider in this work considerably outperform the Trivial baseline, con\ufb01rming that content of papers is useful in evaluating expertise. Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and Specter outperform ELMo. The former two algorithms rely on domain-speci\ufb01c embeddings3 while ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be su\ufb03ciently di\ufb00erent from that in other domains. Third, we note that under modeling choices (paper representation and length of reviewers\u2019 pro\ufb01les) that mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best. \u2022 The fourth \ufb01nding is the most surprising. Observe that TPMS algorithm is much simpler than in contrast to ELMo, Specter, Specter+MFR, and ACL, it does 3Specter+MFR and Specter are initialized with SciBERT Beltagy et al. (2019) 11 Algorithm Trivial Loss 0.50 95% CI for Loss \u2206 with TPMS \u2014 \u2014 95% CI for \u2206 \u2014 TPMS 0.28 [0.23, 0.33] \u2014 \u2014 ELMo 0.34 Specter 0.27 Specter+MFR 0.24 ACL 0.30 [0.29, 0.40] [0.21, 0.34] [0.18, 0.30] [0.25, 0.35] +0.06 \u22120.01 \u22120.04 +0.01 [0.00, 0.13] [\u22120.06, 0.04] [\u22120.09, 0.01] [\u22120.03, 0.06] Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer pro\ufb01les consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of \u2206 indicates that the algorithm performs worse (respectively, better) than TPMS.\u2217 \u2217In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes the gap with Specter+MFR when using this information. not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm that can make use of the full text of the papers. In Section 6.2, we \ufb01nd that when TPMS uses the full text of all papers (including reviewers\u2019 past papers), it closes the gap with Specter+MFR. Note that the performance of advanced algorithms could be improved by \ufb01ne-tuning in a dataset-speci\ufb01c manner, or by leveraging larger pre-trained models like T5 (Ra\ufb00el et al., 2020) with added \ufb01ne-tuning in the scienti\ufb01c domain. However, our observation suggests that the o\ufb00-the-shelf performance of the best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm. Finally, we note that some of the con\ufb01dence intervals for the performance of the di\ufb00erent algorithms, as well as for the relative di\ufb00erences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more \ufb01ne-grained comparisons between the algorithms. 6.2 The role of modeling choices In the beginning of Section 6.1 we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers\u2019 pro\ufb01les used by these algorithms. In this section, we investigate these two questions in more detail. Question 1 (paper representation). Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the manuscripts (e.g., TPMS). Consequently, there is a potential trade-o\ufb00 between accuracy and compu- tation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quanti\ufb01ed. With this motivation, our \ufb01rst question is: What are the bene\ufb01ts of providing richer representations of papers to similarity-computation algorithms? Question 2 (reviewer pro\ufb01le). The second important choice is the size of reviewers\u2019 pro\ufb01les. On the one hand, by including only very recent papers in reviewers\u2019 pro\ufb01les, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is: 12 10 full text 1 title+abstract 0.3 0.2 0.1 0.0 20Number of papers in reviewer profile title 0.5Loss 15 0.4 5 Paper content Figure 2: Impact of di\ufb00erent choices of parameters on the quality of predicted similarities. Con\ufb01dence intervals are not shown (see Table 4). What is the optimal number of the most recent papers to include in the pro\ufb01les of reviewers? To investigate these questions, we choose the TPMS algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen- tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for hundreds of con\ufb01gurations in a reasonable time. With the algorithm chosen, we vary the number of papers in the reviewers\u2019 pro\ufb01les from 1 to 20. For each value of the pro\ufb01le length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer pro\ufb01les and predict similarities using the approach introduced in Section 6.1. The only exception is that we repeat the procedure for averaging out the randomness in the pro\ufb01le creation 5 times (instead of 10) to save the computation time. Papers and reviewer pro\ufb01les Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are"}