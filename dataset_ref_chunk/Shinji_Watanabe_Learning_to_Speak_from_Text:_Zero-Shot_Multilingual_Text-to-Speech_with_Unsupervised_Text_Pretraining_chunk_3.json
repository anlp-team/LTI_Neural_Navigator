{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Learning_to_Speak_from_Text:_Zero-Shot_Multilingual_Text-to-Speech_with_Unsupervised_Text_Pretraining_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of model did Saeki et al., 2022b build?", "answer": " Saeki et al., 2022b built a massively multilingual TTS model.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " What does the work by Saeki et al., 2022b achieve?", "answer": " The work achieves zero-shot TTS from ASR data.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " How does the model address the mismatch between language embeddings of seen and unseen languages?", "answer": " The model freezes the language embedding layer.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " What is the evaluation denotation for Fully zero-shot TTS and Text-seen zero-shot TTS?", "answer": " Fully zero-shot TTS performs zero-shot TTS without pretraining, while Text-seen zero-shot TTS performs zero-shot TTS with pretraining.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " What is the bottleneck layer in the model based on?", "answer": " The bottleneck layer is based on a residual network.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " How was the model trained for unsupervised text pretraining?", "answer": " The model was trained for 1.2M iterations using the Noam optimizer with specific learning rate and warm-up steps.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " What was the model architecture based on for the autoregressive TTS model?", "answer": " The model architecture was based on Transformer TTS.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " Which optimizer was used for training the TTS model?", "answer": " The Noam optimizer was used for training the TTS model.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " What is the purpose of using x-vectors in the encoder output?", "answer": " X-vectors are used for the speaker embedding and added to the encoder output through a projection layer.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}, {"question": " How many languages were selected as seen languages for evaluation?", "answer": " Seven European languages were selected as seen languages for evaluation.", "ref_chunk": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}], "doc_text": "work [Saeki et al., 2022b] has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mis- match between language embedding of seen and unseen languages. (6) (7) (8) (9) Our work attempts to only use the linguistic knowledge to im- prove the zero-shot TTS. Thus, the inference process is writ- ten as L\u2032 unseen \u2282 Ltext. In the evaluation, we denote the inference with Lunseen and L\u2032 unseen as Fully zero-shot TTS and Text-seen zero-shot TTS, respec- tively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al., 2020], which is the baseline method in our evaluations. unseen \u2229 Lpaired = \u2205 and L\u2032 2.4 Model Architecture Our model is an autoregressive TTS model based on Trans- former TTS [Li et al., 2019b], which has also been used in the previous work on byte-based multilingual TTS [He et al., 2021]. During the supervised learning described in \u00a7 2.2 and inference described in \u00a7 2, we use x-vector [Snyder et al., 2018] for the speaker embedding and add it to the encoder output through a projection layer. During supervised learn- ing, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthe- sis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with \u03b8B, we use a residual network consisting of Layer Normalization [Ba et al., 2016], down projection, ReLU [Nair and Hinton, 2010], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019]. 3 Experimental Evaluations 3.1 Experimental Setting Dataset We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each lan- guage. For the unsupervised text pretraining described in \u00a7 2.1, we used transcripts from VoxPopuli [Wang et al., 2021], M-AILABS [Munich Artificial Intelligence Labora- tories GmbH, 2017], and CSS10 [Park and Mulc, 2019], re- sulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning de- scribed in \u00a7 2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. It The paired data consisted of one speaker per language. should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the ora- cle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. Training Details The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in \u00a7 2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al., 2017] with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described in Languages Code Text-only data Text Paired data Audio Seen languages for evaluation Lseen German French Dutch Finnish Hungarian Russian Greek de fr nl fi hu ru el 359MB 372MB 336MB 308MB 104MB 4.9MB 0.39MB 0.73MB 0.94MB 0.75MB 0.47MB 0.51MB 1.5MB 0.39MB 16.13h 19.15h 14.10h 21.36h 10.53h 10.00h 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 English Estonian Croatian Italian Lithuanian Polish Romanian Slovak Slovenian en et hr it lt pl ro sk sl 338MB 87MB 2.0MB 334MB 89MB 102MB 67MB 94MB 81MB Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. \u00a7 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017] and a 6-block Transformer decoder, with a post- net consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of atten- tion heads were set to 512 and 8, respectively. For the bot- tleneck layer described in \u00a7 2.4, we set the hidden dimen- sion after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation func- tion [Hendrycks and Gimpel, 2016], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al., 2018] to improve the training efficiency. For the supervised learn- ing described in \u00a7 2.2, we trained the models for 2.47M it- erations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al., 2020] for 2M iterations with LibriTTS [Zen et al., 2019], VCTK [Veaux et al., 2017], and CSS10. For the x-vector described in \u00a7 2.4, we used a model trained on VoxCeleb1 and VoxCeleb2 [Na- grani et al., 2017] published in SpeechBrain [Ravanelli et al., 2021]. We used ESPnet2-TTS [Watanabe et al., 2018; Hayashi et al., 2021] for the implementation. Baselines We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5 and could not synthesize in- telligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al., 2018] only for the monolingual models, as in the original pa- per of the dataset [Park and Mulc, 2019]. Multilingual w/o LIDs: We trained a multilingual Transformer TTS model us- ing the paired data shown in Table 1 without language IDs 5The original paper [Li et al., 2019b] also reports the instability. (LIDs). Multilingual w/ LIDs: We trained"}