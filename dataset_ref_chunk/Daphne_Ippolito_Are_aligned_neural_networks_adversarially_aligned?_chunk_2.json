{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Are_aligned_neural_networks_adversarially_aligned?_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two primary research areas studied in the text?", "answer": " AI alignment and adversarial examples", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " What are some examples of tasks that large language models are commonly used for?", "answer": " Question answering, translation, and summarization", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " What are some limitations of large pretrained language models when deployed in user-facing applications?", "answer": " They do not follow user instructions and reflect biases, toxicity, and profanity present in the training data", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " What are some techniques used to align base models with certain desired principles?", "answer": " Instruction tuning and reinforcement learning via human feedback (RLHF)", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " What do adversarial examples refer to in the context of the text?", "answer": " Inputs designed by an adversary to cause a neural network to perform incorrect behaviors", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " Why do researchers study adversarial examples in machine learning systems?", "answer": " To evaluate the robustness of machine learning systems against real adversaries and to understand the worst-case behavior of the system", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " What is the purpose of adversarial robustness in machine learning?", "answer": " To prevent real attacks by designing robust classifiers and to understand the worst-case behavior of a system", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " What are some examples of textual tasks for which adversarial examples have been studied?", "answer": " Question answering, document classification, sentiment analysis, and triggering toxic completions", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " What is the difference between the focused adversarial examples studied in the text and recent social engineering attacks on language models?", "answer": " The focused adversarial examples are optimized to produce specific unwanted outcomes, while social engineering attacks may try to trick the model into playing a harmful role", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}, {"question": " Why are adversarial examples important in the verification of high-stakes neural networks?", "answer": " They serve as a lower bound of error when formal verification is not possible", "ref_chunk": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}], "doc_text": "machine learning models of today already face practical security risks [Brundage et al., 2018, Greshake et al., 2023]. Our work suggests that eliminating these risks via current alignment techniques\u2014which do not specifically account for adversarially optimized inputs\u2014is unlikely to succeed. 2 2 Background Our paper studies the intersection of two research areas: AI alignment and adversarial examples. Large language models. As large language model parameter count, training dataset size, and training duration have been increased, the models have been found to exhibit complex behaviors [Brown et al., 2020, Wei et al., 2022b, Ganguli et al., 2022]. In this work, we focus on models trained with causal \u201cnext-word\u201d prediction, and use the notation s \u2190 Gen(x) to a language model emitting a sequence of tokens s given a prompt x. Many applications of language models take advantage of emergent capabilities that arise from increased scale. For instance, language models are commonly used to perform tasks like question answering, translation, and summarization [Brown et al., 2020, Chowdhery et al., 2022, Rae et al., 2022, Anil et al., 2023, Liang et al., 2022, Goyal et al., 2022]. Aligning large language models. Large pretrained language models can perform many useful tasks without further tuning [Brown et al., 2020], but they suffer from a number of limitations when deployed as is in user-facing applications. First, these the models do not follow user instructions (e.g., \u201cwrite me a sorting function in Python\u201d), likely because the model\u2019s pretraining data (e.g., Internet text) contains few instruction-answer pairs. Second, by virtue of faithfully modeling the distribution of Internet text, the base models tend to reflect and even exacerbate biases [Abid et al., 2021], toxicity, and profanity [Welbl et al., 2021, Dixon et al., 2018] present in the training data. Model developers thus attempt to align base models with certain desired principles, through tech- niques like instruction tuning [Wei et al., 2022a, Ouyang et al., 2022] and reinforcement learning via human feedback (RLHF) [Christiano et al., 2023, Bai et al., 2022]. Instruction tuning finetunes a model on tasks described with instructions. RLHF explicitly captures human preferences by su- pervising the model towards generations preferred by human annotators [Christiano et al., 2023]. Multimodal text-vision models. Increasingly, models are multimodal, with images and text being the most commonly combined modalities [OpenAI, 2023, Pichai, 2023, Liu et al., 2023, Zhu et al., 2023]. Multimodal training allows these models to answer questions such as \u201chow many people are in this image?\u201d or \u201ctranscribe the text in the image\u201d. While GPT-4\u2019s multimodal implementation has not been disclosed, there are a number of open- source multimodal models that follow the same general protocol [Gao et al., 2023, Liu et al., 2023, Zhu et al., 2023]. These papers start with a standard pre-trained language model that tokenizes and then processes the embedding layers. To process images, they use a pretrained vision encoder like CLIP [Radford et al., 2021] to encode images into an image embedding, and then train a projection model that converts image embeddings into token embeddings processed by the language model. These visual tokens may be passed directly as an input to the model [Zhu et al., 2023, Liu et al., 2023], surrounded by special templates (e.g., \u201c<img> . . . <\\img>\u201d) to delineate their modality, or combined internal to the model via learned adaptation prompts [Gao et al., 2023]. Adversarial examples. Adversarial examples are inputs designed by an adversary to cause a neu- ral network to perform some incorrect behavior [Szegedy et al., 2014, Biggio et al., 2013]. While primarily studied on vision classification tasks, adversarial examples also exist for textual tasks such as question answering [Jia and Liang, 2017, Wallace et al., 2019], document classification [Ebrahimi et al., 2017], sentiment analysis [Alzantot et al., 2018], or triggering toxic completions [Jones et al., 2023, Wallace et al., 2019]. Prior work on textual tasks has either applied greedy attack heuris- tics [Jia and Liang, 2017, Alzantot et al., 2018] or used discrete optimization to search for input text that triggers the adversarial behavior [Ebrahimi et al., 2017, Wallace et al., 2019, Jones et al., 2023]. In this paper, we study adversarial examples from the perspective of alignment. Because aligned language models are intended to be general-purpose\u2014with strong performance on many different tasks\u2014we focus more broadly on adversarial examples that cause the model to produce unwarranted harmful behavior, rather than adversarial examples that simply cause \u201cmisclassification\u201d. Our inputs are \u201cadversarial\u201d in the sense that they are specifically optimized to produce some tar- geted and unwanted outcome. Unlike recent \u201csocial-engineering\u201d attacks on language models that induce harmful behavior by tricking the model into playing a harmful role (for example, taking on the persona of a racist movie actor [Reddit, 2023]), we make no effort to ensure our attacks are semantically meaningful, and they often will not be. 3 3 Threat Model There are two primary reasons researchers study adversarial examples. On the one hand, researchers are interested in evaluating the robustness of machine learning systems in the presence of real adver- saries. For example, an adversary might try to construct inputs that evade machine learning models used for content filtering [Tram`er et al., 2019, Welbl et al., 2021] or malware detection [Kolosnjaji et al., 2018], and so designing robust classifiers is important to prevent a real attack. On the other hand, researchers use adversarial robustness as a way to understand the worst-case behavior of some system [Szegedy et al., 2014, Pei et al., 2017]. For example, we may want to study a self-driving car\u2019s resilience to worst-case, adversarial situations, even if we do not believe that an actual attacker would attempt to cause a crash. Adversarial examples have seen extensive study in the verification of high-stakes neural networks [Wong and Kolter, 2018, Katz et al., 2017], where adversarial examples serve as a lower bound of error when formal verification is not possible. 3.1 Existing Threat Models Existing attacks assume that a model developer creates the model and uses some alignment technique (e.g., RLHF) to make the model"}