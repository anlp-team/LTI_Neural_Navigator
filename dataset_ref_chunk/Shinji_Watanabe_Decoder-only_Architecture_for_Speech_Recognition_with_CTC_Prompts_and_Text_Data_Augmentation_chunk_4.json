{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Decoder-only_Architecture_for_Speech_Recognition_with_CTC_Prompts_and_Text_Data_Augmentation_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What method was found to be the best choice for compressing prompts in the experiments?", "answer": " Removing frames of blank predictions (P3)", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " Why did the decoder-only architecture using only audio-text pair data struggle to outperform the Baseline EncDec results?", "answer": " Training the decoder-only model from scratch was slightly worse than the conventional encoder-decoder model", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " How many epochs were the models trained for in the experiments?", "answer": " 30 epochs", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " What advantage did the proposed method show in terms of computational speed?", "answer": " Advantage in computational speed with 0.29 of RTF against 0.54", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " In the Switchboard and Fisher experiments, what type of data augmentation was used to reduce WERs significantly?", "answer": " Text augmentation using an external text-only corpus (D1)", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " What approach was evaluated involving pre-training the CTC and decoder with paired data and unpaired text data?", "answer": " Fine-tuning using the paired data (F1)", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " What did the proposed model successfully outperform in both test subsets of the Switchboard models?", "answer": " Baseline EncDec model", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " What is the main advantage of the decoder-only architecture proposed for ASR tasks?", "answer": " To enhance the model with LM training without any modification", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " Which method with both CTC and LM fusion achieved the best performance in the experiments?", "answer": " Fusing both CTC and LM (D3)", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}, {"question": " What is planned for future work based on the conclusion in the text?", "answer": " Scaling up the decoder to have more capacity for LM while maintaining the advantage in inference speed", "ref_chunk": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}], "doc_text": "the ST tasks; however, they did not use it for the models trained from scratch. We also found that the averaging approach (P2) struggled in training for ASR tasks. Thus, we con\ufb01rmed that removing frames of blank predictions was the best choice for compressing the prompts (P3), and we used this method for the following experiments. However, with only audio\u2013 text pair data (P3), the decoder-only architecture could not outper- form the Baseline EncDec results (B4), as [16] reported that training the decoder-only model from scratch was slightly worse than the conventional encoder\u2013decoder model. We also evaluated a full 960h pair set of LibriSpeech and its ex- ternal text-only corpus. We trained the models for 30 epochs and used the same beam size and fusion weights as in Sec. 3.2 for infer- ence. The results are listed in Table 3. The decoder-only model suc- cessfully re\ufb01ned CTC transcription, particularly in the test-other set, from 7.0% to 6.6%. We sampled an utterance from the test-other set to show how the decoder re\ufb01nes CTC predictions in Table 2. How- ever, compared with the Baseline EncDec, our method did not reach its performance. We assume that the capacity of the decoder is rela- tively small because it is a 6-block transformer while a transformer LM generally has 12 blocks or more [26]. However, our proposed method still showed an advantage in the computational speed; 0.29 of RTF against 0.54. 3.4. Switchboard and Fisher experiments Next, we trained models with proposed text augmentation (D1\u2013 3). In contrast to pair-data-only training, text augmentation using an external text-only corpus (D1) signi\ufb01cantly reduced the WERs from 9.5% and 23.0% to 9.3% and 19.9% for the test-clean and test-other sets, respectively, compared to (P3). As discussed in Sec. 2.3, be- cause the structure of decoder is identical to autoregressive LMs, it is an advantage of the decoder-only architecture to enhance the model with LM training without any modi\ufb01cation. Compared to the Base- line EncDec fused with CTC and LM (B5), the proposed method with only CTC fusion (D2) achieved comparable WERs. The prompt compression contributed to speeding up the computation; it was less than half of the RTF of the Baseline EncDec (B5)1. The external LM is also effective and complementary to the text augmentation, as fusing both CTC and LM (D3) achieved the best performance; 7.0% and 17.5% for test-clean and test-other. It was also compa- rably fast with the Baseline CTC with LM fusion (B2), in which LM computation was costly in time-synchronous decoding. As in [15], we also evaluated the pre-training of the CTC and decoder with the paired data and unpaired text data, respectively, followed by \ufb01ne-tuning using the paired data (F1), as described in Sec. 2.3. However, the \ufb01ne-tuning approach was slightly worse than training from scratch (D3), presumably because the \ufb01ne-tuning approach was We evaluated the Switchboard models using Hub5\u201900 with the Switchboard and CallHome subsets. We fused CTC and LM with weights of 0.4 and 0.4, respectively, with a beam size of 10. Table 4 lists the results. We observed a similar tendency as in Section. 3.2; the proposed model successfully outperformed the Baseline EncDec model in both the test subsets, owing to effective text augmentation. 4. CONCLUSION We proposed a decoder-only architecture for ASR tasks that effec- tively applies text augmentation using text-only additional data. Au- dio information was provided as CTC prompts, which mapped the output of the encoder module compressed by CTC predictions to the embedding space of the decoder. Although the model was trained us- ing an ASR task, the decoder was simultaneously trained for an LM task using augmented text data. We experimentally con\ufb01rmed that the proposed methods re\ufb01ned the baseline CTC using the decoder and outperformed conventional RNN-T and encoder\u2013decoder mod- els with approximately half the computational cost in LibriSpeech 100h and Switchboard setups. 1The number of frames was reduced down to 14.55% on average. Future work will include scaling up the decoder to have more ca- pacity for LM while maintaining the advantage in inference speed. Extending the streaming approach can also be considered by exploit- ing its compactness and ef\ufb01ciency of the proposed approach. 5. REFERENCES [1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end- to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based mod- els for speech recognition,\u201d in Proc. of NIPS, 2015, pp. 577\u2013 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, \u201cInternal language model estimation for domain-adaptive end-to-end speech recognition,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243\u2013250. [4] Albert Zeyer, Andr\u00b4e Merboldt, Wilfried Michel, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLibrispeech transducer model with inter- nal language model prior correction,\u201d in Proc. of Interspeech, 2021, pp. 2052\u20132056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, \u201cResidual language model for end-to- end speech recognition,\u201d in Proc. of Interspeech 2022, 2022, pp. 3899\u20133903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, \u201cMulti-modal data augmentation for end- to-end asr,\u201d Proc. of Interspeech 2018, pp. 2394\u20132398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., \u201cIndependent language modeling architecture for end-to-end ASR,\u201d in Proc. of ICASSP, 2020, pp. 7059\u20137063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, \u201cMultitask training with text data for end-to-end speech recognition,\u201d in Proc. of Interspeech, 2021, pp. 2566\u20132570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, \u201cMAESTRO: Matched speech text representations through modality matching,\u201d in Proc. of Interspeech, 2022, pp. 4093\u2013 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., \u201cLan- guage models are few-shot learners,\u201d"}