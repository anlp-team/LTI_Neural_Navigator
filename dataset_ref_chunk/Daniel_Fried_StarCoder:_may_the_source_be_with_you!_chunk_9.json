{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_StarCoder:_may_the_source_be_with_you!_chunk_9.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What library was used to train the byte-level Byte-Pair-Encoding tokenizer?,answer: Hugging Face Tokenizers", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " What is the vocabulary size of the tokenizer used in the model?,answer: 49,152 tokens", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " What type of model architecture is described in the text?,answer: Decoder-only Transformer with Multi-Query-Attention", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " What technique was used to speed up attention computation?,answer: FlashAttention", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " How many iterations was the model trained for?,answer: 250k iterations", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " What optimizer was used during training?,answer: Adam", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " How many GPUs were used in the GPU cluster to train the model?,answer: 512 A100 80 GB GPUs", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " What was the carbon footprint of training StarCoderBase in tonnes of CO2eq?,answer: 16.68 tonnes", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " How many epochs was the model fine-tuned for in the Python variant?,answer: 2 epochs", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}, {"question": " What is the purpose of the Code LM Evaluation Harness mentioned in the text?,answer: To enable reproducible and centralized evaluation of Code LLMs", "ref_chunk": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}], "doc_text": "Jupyter text cell start of Jupyter code cell start of Jupyter output cell output cell without content code snippet before commit commit message code snippet after commit Table 10: Overview of the sentinel tokens. 5.3 Tokenizer The model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization step includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer. 5.4 Model Architecture We trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a decoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional embeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training data, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and reduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work with MQA during training, we simply expand the key and value before calling the attention kernel. The architecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of SantaCoder(Ben Allal et al., 2023) for comparison. 5.5 Training details StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight decay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of 2,000 iterations. StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for 8,500 steps. 16 Published in Transactions on Machine Learning Research (12/2023) Hyperparameter SantaCoder StarCoder Hidden size Intermediate size Max. position embeddings Num. of attention heads Num. of hidden layers Attention 2048 8192 2048 16 24 Multi-query 6144 24576 8192 48 40 Multi-query Num. of parameters \u2248 1.1B \u224815.5B Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community). 5.6 Multi-Node GPU Setup We trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We partitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism rank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used 32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a micro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to 4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher throughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16 leads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities. Except for a few restarts, we did not experience significant training instabilities. 5.7 CO2 emissions StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the carbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average Power Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted. StarCoder The fine-tuned model adds 3.5% of training time, which translates to an additional estimated emission of 0.58 tonnes of CO2eq. 6 Evaluation In this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we report on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation using a variety of benchmarks and tasks. A Code LM Evaluation Harness To enable reproducible and centralized evaluation of StarCoder and other Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM Evaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks, including HumanEval, MultiPL-E, and DS-1000. Other Models Evaluated We compare StarCoder and StarCoderBase to the following models. 1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained on the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript, and Python from the GitHub BigQuery dataset (Smith, 2016). 17 Published in Transactions on Machine Learning Research (12/2023) Model Size HumanEval MBPP Open-access LLaMA LLaMA SantaCoder CodeGen-Multi LLaMA CodeGeeX LLaMA-65B CodeGen-Mono StarCoderBase StarCoder 7B 13B 1.1B 16B 33B 13B 65B 16B 15.5B 15.5B 10.5 15.8 18.0 18.3 21.7 22.9 23.7 29.3 30.4 33.6 17.7 22.0 35.0 20.9 30.2 24.4 37.7 35.3 49.0 52.7 Closed-access LaMDA PaLM code-cushman-001 code-davinci-002 137B 540B 12B 175B 14.0 26.2 33.5 45.9 14.8 36.8 45.9 60.3 Table 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several other models. StarCoder and StarCoder base obtain the highest performance of open-access models, and comparable performance to the code-cushman-001 closed access model. 2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python code from GitHub, though the dataset is not publicly available. 3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming languages"}