{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Structured_Pruning_of_Self-Supervised_Pre-Trained_Models_for_Speech_Recognition_and_Understanding_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the method described in the text?", "answer": " The main focus of the method is structured pruning of SSL speech models.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " Why is careful tuning of separate target sparsities required in the method?", "answer": " Careful tuning of separate target sparsities is required to meet a particular budget, which is computationally expensive.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " How is the pruning target able to regularize the training according to the text?", "answer": " The pruning target is able to regularize the training by achieving similar or better performance as the original model at low sparsities.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " What is the purpose of the HJ-Pruning-MAC method described in the text?", "answer": " The purpose of HJ-Pruning-MAC is to prune the entire model to directly meet a computational budget measured by MACs.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " What role does the computation of MACs play in the method?", "answer": " The computation of MACs helps in calculating the percentage of MACs that are pruned, which is important for the method.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " How do the proposed pruning methods compare to the baseline in terms of performance?", "answer": " The proposed pruning methods consistently outperform the baseline by reducing computation without degradation in performance.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " Why does HJ-Pruning-MAC have similar performance with HJ-Pruning-SepSize in the method?", "answer": " HJ-Pruning-MAC and HJ-Pruning-SepSize have similar performance because the CNN has much fewer parameters than Transformer.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " What is required in order to select the best combination of target sparsities in the method?", "answer": " To select the best combination of target sparsities, a grid search is performed to identify the Pareto frontiers, which requires much more computation.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " How is the performance and complexity of the HJ-Pruning-MAC method described in the text?", "answer": " The HJ-Pruning-MAC method is considered the best in terms of performance and complexity among the proposed methods.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}, {"question": " What is the similarity between the results of intent classification on SLURP and the ASR task based on the text?", "answer": " The overall trend in the results of intent classification on SLURP is very similar to that of ASR, with joint pruning methods outperforming.", "ref_chunk": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}], "doc_text": "two terms: size(\u03b1)) and Transformer (strans size, ttrans 2 gsize = \u03bbcnn + \u03bbtrans 1 size) + \u03bbcnn size ) + \u03bbtrans size(\u03b1) \u2212 tcnn 1 (scnn size (\u03b1) \u2212 ttrans (strans 2 (scnn size(\u03b1) \u2212 tcnn size) (strans size (\u03b1) \u2212 ttrans size ) 2 2 . As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive. (7) (8) (9) 3.2. Joint pruning based on the MACs The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed \ufb02ops pro\ufb01ler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows: MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint, where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by MACcnn = T outC outC inK, where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of \u03b1. We de\ufb01ne the percentage of MACs that are pruned as the MACs-based macs(\u03b1). 2 It is differentiable w.r.t. parameters \u03b1. Hence, sparsity sall we can still train the model using Eq. (5). 4. EXPERIMENTS 4.1. Experimental setup We focus on task-speci\ufb01c structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace\u2019s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and \ufb01ne-tune the entire model to obtain an unpruned model. Then, we prune this \ufb01ne-tuned model to reach a speci\ufb01c sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments. ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We \ufb01ne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the \ufb01rst 5 epochs. The learning rate of \u03b1 and \u03bb is selected from {0.02, 0.05}. The pruned model is \ufb01ne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and \ufb01ne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classi\ufb01cation. A pre-trained SSL model is \ufb01ne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The \ufb01nal \ufb01ne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and \ufb01ne-tuning, respectively. Other con\ufb01gs are the same as ASR. 1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a \u201cvirtual\u201d length used only for computing MACs. We do not modify any training utterances. (10) (11) (12) ) \u2193 ( e t a R r o r r E d r o W % 11 9 7 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 5.77 20 30 40 MACs (\u00d7109) 50 60 70 Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline. 86.1 85 ) \u2191 ( y c a r u c c A % 80 Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC 75 10 20 30 50 MACs (\u00d7109) 40 60 70 Fig. 2: Intent Classi\ufb01cation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline. 4.2. Main results Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity. Fig. 2 shows the results of intent classi\ufb01cation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform"}