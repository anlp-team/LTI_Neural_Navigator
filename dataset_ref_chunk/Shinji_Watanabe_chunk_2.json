{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the architecture proposed in the first text for low-level feature representation learning?", "answer": " frontend network", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " What are the three task-specific structured pruning methods proposed in the first text?", "answer": " Three task-specific structured pruning methods", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " Which tasks were selected for the experiments in the second text related to the web-scale speech model Whisper?", "answer": " audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST)", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " In the third text, what encoder architecture has become the de facto for speech processing?", "answer": " Conformer", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " What type of architecture does the Token-and-Duration Transducer (TDT) introduce?", "answer": " TDT architecture", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " What is the focus of the speech summarization system proposed in the fifth text?", "answer": " Improving memory efficiency of speech/text encoders", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " What condition is required for UNSSOR to realize unsupervised speech separation?", "answer": " Over-determined conditions", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " Which conference is mentioned in the second and third texts in relation to their research work?", "answer": " Interspeech", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " What is the major benefit of the proposed TDT models in the fourth text compared to conventional Transducers?", "answer": " Significantly faster inference", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}, {"question": " What is the main focus of the fourth text regarding sequence-to-sequence tasks?", "answer": " Jointly predicting tokens and durations", "ref_chunk": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}], "doc_text": "frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']] ['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']] ['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']] ['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']] ['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']] ['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es."}