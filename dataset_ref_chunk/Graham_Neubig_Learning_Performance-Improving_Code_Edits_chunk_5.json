{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Learning_Performance-Improving_Code_Edits_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What dataset was used to train GPT-3.5?", "answer": " The smaller, high-quality dataset (HQ) in Section 3.2.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " What results were observed when fine-tuning on the smaller, high-quality dataset?", "answer": " Substantially stronger results were seen.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " What is reflected by the observation that a small set of high-quality examples can elicit strong performance?", "answer": " To adapt LLMs.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " What did the gains highlighted in the study show about the fine-tuned models?", "answer": " The gains showed how the models can distinguish optimal and sub-optimal solutions, leading to more effective optimizations.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " How did the additional synthetic data impact the performance of CODELLAMA and GPT-3.5?", "answer": " The additional data improved both %OPT and often SPEEDUP, particularly with BEST@1.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " What did the results demonstrate regarding CODELLAMA and GPT-3.5?", "answer": " The results demonstrated that openly available models such as CODELLAMA can be competitive with GPT-3.5.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " How did dynamic retrieval impact the performance of CODELLAMA 34B?", "answer": " It improved the performance from 20.07% %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " What was found to be incredibly effective for achieving good performance according to the study?", "answer": " Performance-conditioned generation.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " What kind of adaptors performed significantly worse than end-to-end in the study?", "answer": " Low-rank adaptors (LoRA).", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}, {"question": " What did the manual analysis of program pairs show about the kind of changes responsible for performance gains?", "answer": " They can be broadly categorized into Algorithmic changes, Input/Output operations, Data Structure modifications, and Miscellaneous adjustments.", "ref_chunk": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}], "doc_text": "PIE dataset. Due to the cost of fine-tuning and sampling models through the OpenAI API, we were only able to train GPT-3.5 on the smaller, high-quality dataset (HQ) in Section 3.2. The top of Table 3 shows results for traditional fine-tuning on all models. We see substantially stronger results when fine-tuning on the smaller, high-quality dataset. These results reflect the observation that to adapt LLMs, a small set of high-quality examples can elicit strong performance (Zhou et al., 2023a; Chen et al., 2023). Performance-conditioned training outperforms fine-tuning. Table 3 shows results for performance- conditioned (perf-cond) generation (Section 3.2). Both fine-tuned CODELLAMA models (7B and 13B) show significant improvements in %OPT and SPEEDUP. These gains highlight how the performance improvement information (Figure 2) can enable models to distinguish optimal and sub-optimal solutions, leading to more effective optimizations. Synthetic data from self-play marginally improves generalization. Next, we fine-tuned both CODEL- LAMA and GPT-3.5 using our PIE dataset augmented with our synthetic examples. We show results at the bottom of Table 3. For CODELLAMA and GPT-3.5, compared to using no synthetic data, the additional data improves both %OPT and often SPEEDUP, particularly with BEST@1. We believe the small set of synthetic examples helped generalize the fine-tuned model, as evidenced by the higher %OPT. 3 4.3 DISCUSSION AND KEY TAKEAWAYS CODELLAMA vs. GPT-3.5 . Our results demonstrate that openly available models such as CODELLAMA can be competitive with GPT-3.5. For prompting, CODELLAMA 34B with dynamic retrieval (42.16% %OPT, 3For GPT-3.5, to be sure the increases came from the type of data and not the quantity of data, we performed an ablation by fine-tuning on the top 5,793 examples from PIE with a maximum of 8 duplicates (instead of the 5,570 pairs that included synthetic programs), and we saw BEST@1 performance degrade %OPT to 36.66% and SPEEDUP to 2.67\u00d7, and BEST@8 performance degrade %OPT to 83.63% and SPEEDUP to 6.03\u00d7. 8 Correct 75.25% 75.56% 87.68% 85.13% 95.42% 63.85% 71.08% 87.47% 86.25% 95.11% Preprint. Under review. 2.57\u00d7 SPEEDUP for BEST@8) roughly matched the performance of GPT-3.5 with dynamic retrieval (48.16% %OPT, 2.14\u00d7 SPEEDUP for BEST@8). With fine-tuning, CODELLAMA 13B with performance-conditioned generation (66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8) approached the performance of GPT-3.5 with synthetic data (87.68% %OPT, 6.86\u00d7 SPEEDUP for BEST@8); indeed, we may expect that fine-tuning CODELLAMA 34B using the same strategy would further bridge this gap. These results demonstrate that with the right adaptation strategies, open models can be competitive with closed ones. Prompting vs. fine-tuning. Our results demonstrate that while prompting can be an effective way to adapt models (with retrieval), fine-tuning significantly outperforms prompting for models of the same size. Effectiveness of retrieval-based few-shot learning. Our results show that dynamic retrieval provides enor- mous gains over all other prompting approaches; for instance, it improved the performance of CODELLAMA 34B from 20.07 %OPT, 1.61\u00d7 SPEEDUP to 42.16% %OPT, 2.57\u00d7 SPEEDUP for BEST@8. Effectiveness of performance-conditioned generation. We find that performance-conditioned generation is incredibly effective for achieving good performance; in particular, it improved the performance of CODEL- LAMA 13B from 47.86% %OPT, 3.43\u00d7 SPEEDUP to 66.60% %OPT, 5.65\u00d7 SPEEDUP for BEST@8. Ineffectiveness of LoRA. We also experimented with low-rank adaptors (LoRA) (Hu et al., 2021), but they performed significantly worse than end-to-end; see Appendix A.7 for results. We hypothesize that this gap may be because performance optimization examples do not occur naturally in the training data. 4.4 ANALYSIS OF GENERATED CODE EDITS Next, we study the kinds of edits LLMs make that lead to our performance gains, focusing on our best- performing model, GPT-3.5 fine-tuned with synthetic data. We manually analyze a randomly sampled set of 120 (source, optimized) program pairs to understand the algorithmic and structural changes responsible for the performance gains. We find that the transformations can be broadly categorized into four kinds: Algorith- mic changes, Input/Output operations (IO), Data Structure modifications, and Miscellaneous adjustments. Algorithmic changes (complex modifications, such as changing recursive methods to dynamic programming, and unexpected ones, such as omitting Binary Indexed Trees for simpler constructs) are most common, com- prising ~34.15% of changes; Input/Output operations (e.g., changing \u2018cin/cout\u2018 to \u2018scanf/printf\u2018, efficiently reading strings) comprised ~26.02%; Data Structures (e.g., switching from vectors to arrays) comprised ~21.14%, and Miscellaneous (e.g., code cleanups and constant optimizations) comprised ~18.70%. These findings show the LLM\u2019s capability to perform sophisticated optimizations while preserving functionality. See Appendix A.1 for details. In Appendix A.1, we show several examples to demonstrate the nature of optimizations made by our model. In these examples, we highlight the removal of a wasteful nested loop (Figure 4), eliminating the need to sort (Figure 3), avoiding unnecessary precomputations (Figure 5), use of simple modular arithmetic properties for optimization (Figure 6), and restructuring loops to improve performance (Figure 7). Finally, while our analysis showcases a variety of optimizations, it is essential to address certain speedup sources that may be considered spurious. Specifically, in 10 out of the 120 cases we examined, the speedup stemmed from reducing the constants used to allocate arrays. These speedups might not always reflect genuine algorithmic improvements, and indicate that the test cases may not perfectly cover all the cases, an open problem in code synthesis (Li et al., 2021). Thus, while they contribute to the overall speedup metrics, they should be interpreted with caution. Nevertheless, our analysis shows that the vast majority of speedups do not suffer from this issue, supporting our strong empirical results. 9 Preprint. Under review. 5 CONCLUSION Our work is an initial step towards unlocking the potential of LLMs in leveraging the opportunities at the \u201ctop\u201d of the computing stack. In particular, we improve algorithmic efficiency and, given a correctness oracle, enable automatic code optimization beyond optimizing compilers. Our results pave an exciting path for improving computing efficiency post Moore\u2019s law. ACKNOWLEDGEMENTS This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200), Army Research Office Award W911NF-20-1-0080, and NSF Award CCF- is authorized to reproduce and distribute reprints for Governmental purposes 1917852. The U.S."}