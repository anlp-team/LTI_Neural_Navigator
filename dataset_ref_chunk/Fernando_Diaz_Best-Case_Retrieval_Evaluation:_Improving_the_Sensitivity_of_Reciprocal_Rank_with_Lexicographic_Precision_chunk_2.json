{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Best-Case_Retrieval_Evaluation:_Improving_the_Sensitivity_of_Reciprocal_Rank_with_Lexicographic_Precision_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the text discuss about the probability of ties amongst system rankings?", "answer": " The text discusses computing the probability of ties between system rankings, based on the position of the first relevant item and a random second ranking.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " How does the probability of ties change as the number of relevant items increases?", "answer": " The probability of ties increases as the number of relevant items increases, skewing towards a higher probability of tie when the first relevant item is closer to the top.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " What does the text suggest about the sensitivity of reciprocal rank as the number of relevant items increases?", "answer": " The text indicates a lack of sensitivity of reciprocal rank as the number of relevant items increases.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " Why is it mentioned that RL1 metrics have a fundamental limitation for evaluation?", "answer": " RL1 metrics, such as reciprocal rank and ESL1, have a fundamental limitation for evaluation due to the skew in the distribution of ties when rankings are drawn from real systems.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " What is the significance of using the logarithmic transform in Figure 3?", "answer": " The logarithmic transform in Figure 3 is used to show the empirical probability of ties with a second ranking for different benchmarks, as the probability values are highly skewed.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " How is Lexicographic Precision related to RL1 evaluation?", "answer": " Lexicographic Precision emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items in RL1 evaluation.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " Why is RL1 evaluation described as having ceiling effects?", "answer": " RL1 evaluation is described as having ceiling effects because it only considers the position of the top-ranked relevant item, which limits its sensitivity.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " How does RL1 evaluation aim to address limitations in existing metrics like expected reciprocal rank and average precision?", "answer": " RL1 evaluation aims to develop an evaluation method that preserves the ordering of rankings and generates a sensible order when RL1 is tied.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " What is the significance of best-case retrieval evaluation in relation to RL1 metrics?", "answer": " Best-case retrieval evaluation aims to improve the sensitivity of reciprocal rank by considering lexicographic precision and providing a justified ordering of rankings when RL1 is tied.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}, {"question": " How does the text suggest uncertainty over recall requirements affects rankings?", "answer": " The text suggests that uncertainty over recall requirements can be evaluated using RL1 to assess the quality of a ranking for users with different recall levels.", "ref_chunk": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}], "doc_text": "ties amongst system rankings for these \ud835\udc5b values as predicted by theoretical properties of reciprocal rank. Specifically, we want to compute, for a given position of the first relevant item \ud835\udc5d1 and a random second ranking, the probability that we will observe a tie. For any \ud835\udc5d1, there are (cid:0)\ud835\udc5b\u2212\ud835\udc5d1 (cid:1) tied arrangements of positions of relevant items amongst all \ud835\udc5a\u22121 of the possible arrangements \ud835\udc5d\u2032 from a second system. If we sample an arrangement of relevant items uniformly at random, then the probability of a tie with \ud835\udf0b is \ud835\udc43\ud835\udc5f (\ud835\udc5d1 = \ud835\udc5d\u2032 (\ud835\udc5b\u2212\ud835\udc5d1 \ud835\udc5a\u22121 ) ( \ud835\udc5b \ud835\udc5a) 1|\ud835\udc5d1) = . We plot this probability in Figure 2. We can observe that, when we have few relevant items (i.e. small \ud835\udc5a), we have a relatively small and uniform probability of ties across all values of \ud835\udc5d1. However, as we increase the number of relevant items, the distribution begins to skew toward a higher probability of a tie as \ud835\udc5d1 is smaller. This means that, if we have a ranking where the first relevant item is close to the top, even if the second ranking is drawn uniformly at random, we will be more likely to find a tie than if the first relevant item were lower in the ranking. While our analysis indicates a lack of sensitivity of reciprocal rank for \ud835\udc5d\u2032 drawn uniformly at random as \ud835\udc5a increases, we are also interested in the probability of ties when \ud835\udc5d\u2032 is drawn from rank- ings produced by real systems. We collected runs associated with multiple public benchmarks (see Section 4.1 for details) and com- puted the the empirical distribution of ties conditioned \ud835\udc5d1 (Figure 3). Because of the highly skewed distribution, we plot the logarith- mic transform of the probability of a rank position. As we can see, across both older and newer benchmarks, the probability of a tie for rankings when the top-ranked relevant item is at position 1 is substantially larger than if we assume \ud835\udc5d\u2032 is drawn uniformly at random. The 2021 TREC Deep Learning track data in particular demonstrates higher skew than others, confirming observations previously made about saturation at top rank positions [21]. Taken together, these results demonstrate a fundamental limita- tion of RL1 metrics (i.e., reciprocal rank and ESL1) for evaluation. 1In lieu of an isolated \u2018Related Work\u2019 section, we have included discussion of relevant literature when necessary. This helps make connections explicit to our work. Fernando Diaz 0100200300400500 0.0000.0010.0020.0030.0040.005 p1Pr(p1=p1') m1050100250 Figure 2: Given a ranking where the highest-ranked relevant item is at position \ud835\udc5d1, the probability of a tie with a second ranking sampled uniformly from all arrangements of rel- evant items for a corpus size of \ud835\udc5b = 50000. This figure and others are best rendered or printed in color. p1Pr(p1=p1') robustweb 2013DL 2021 (docs)ml-1m 0.0050.0100.0200.0500.1000.2000.500 12510 Figure 3: Empirical probability of a tie with a second ranking for several benchmarks (see Section 4.1 for details). Horizon- tal and vertical axes are on a logarithmic scale for clarity. As retrieval and other scenarios where reciprocal rank is used be- gin to attract highly performant systems, we need to extend our evaluation approaches to address these issues. 3 LEXICOGRAPHIC PRECISION RL1 evaluation emphasizes precision by considering the position of the top-ranked relevant item and ignoring the positions of other relevant items. However, only ever looking at the position of the top- ranked relevant item results in the ceiling effects described in the previous section. Our goal is to develop an evaluation method that preserves the ordering of a pair of rankings by RL1 (i.e., agree with RR1 when RR1 (\ud835\udf0b) \u2260 RR1 (\ud835\udf0b \u2032)) and provides a justified ordering Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision of a pair of rankings when RL1 is tied (i.e., generate a sensible order when RR1 (\ud835\udf0b) = RR1 (\ud835\udf0b \u2032)). Although metrics like expected reciprocal rank [6] and average precision include reciprocal rank as a component in their computation, they are not guaranteed to preserve the ordering of reciprocal rank when there is one. In this section, we will interpret RL1 metrics as best-case retrieval evaluation, allowing us to derive a preference-based evaluation method based on social choice theory. determine an item\u2019s psychological relevance to any particular user. From this perspective, there are 2\ud835\udc5a \u2212 1 possible non-empty sets of relevant items for a specific request, each representing psycho- logical relevance to a possible user. Nevertheless, amongst these possible users, if they are interested in precisely one relevant item, there are \ud835\udc5a unique utilities. Again, since RL1 monotonically de- creases in rank, the best-case utility is RR1, followed by RR2 until we reach RR\ud835\udc5a. 3.1 Best-Case Retrieval Evaluation When a user approaches a retrieval system, there is a great deal of uncertainty about their information need. While a request such as a text query provides information about which items in the corpus might be relevant, it says much less about the user\u2019s appetite for relevant information. As a result, there is an implicit population of possible users issuing any particular request, each of whom may have a different utility for any particular ranking. In this section, we explore two types of uncertainty and demonstrate that, from both perspectives, RL1 evaluation represents the best-case utility over that population. We first consider uncertainty over recall requirements. Robert- son [15] presented a model for evaluating rankings based on the diverse set of recall requirements that a user might have. Given a request and its associated relevant items, users may be interested in one relevant item, a few relevant items, or the complete set of all relevant items. We can assess the quality of a ranking for any particular user recall requirement with what Cooper [7] refers to as the Type 2 expected search length: the number of items a user with requirement \ud835\udc56 has to scan before finding \ud835\udc56 relevant items. So, each information need has \ud835\udc5a recall levels and RL1 is the evaluation measure associated with users requiring exactly one"}