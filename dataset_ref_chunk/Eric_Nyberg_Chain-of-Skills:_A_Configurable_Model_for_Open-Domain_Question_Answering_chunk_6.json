{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_Chain-of-Skills:_A_Configurable_Model_for_Open-Domain_Question_Answering_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of experiments were conducted by the authors in this study?", "answer": " End-to-end question-answering experiments on NQ, OTT-QA, and HotpotQA.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " What models were used for NQ and OTT-QA experiments?", "answer": " The Fusion-in-Encoder (FiE) model was used for NQ and OTT-QA experiments.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " How many passages does the model read for NQ and OTT-QA experiments?", "answer": " Top-100 passages for NQ and top-50 evidence chains for OTT-QA.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " What approach was followed for HotpotQA experiments?", "answer": " A separate reader model was trained to predict supporting sentences in addition to the answer span.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " What are the top skills learned by the modular model COS for ODQA?", "answer": " COS learns five reusable skills for ODQA via multi-task learning.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " What are some limitations of the current COS model mentioned in the text?", "answer": " One limitation is that the reranking expert only learns to rerank single-step results. Another limitation is that the current pretraining setup only includes three bi-encoder tasks.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " How does the COS model outperform previous baselines on OTT-QA?", "answer": " The model, coupled with the FiE, outperforms previous baselines by large margins on OTT-QA.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " What datasets were used to train separate models independently?", "answer": " Separate models were trained independently for each dataset, NQ, OTT-QA, and HotpotQA.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " What is the main reason for the superior performance of the model on OTT-QA?", "answer": " The superior performance of the model on OTT-QA is mainly due to COS.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}, {"question": " How does the COS model achieve competitive QA performance against previous SOTA?", "answer": " The COS model achieves competitive QA performance against previous SOTA with improved exact match accuracy.", "ref_chunk": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}], "doc_text": "(Huang et al., 2022) CORE (Ma et al., 2022a) 10.3 28.1 33.2 37.1 49.0 13.0 32.5 38.6 42.8 55.7 9.7 27.2 32.5 37.3 47.3 12.8 31.5 38.5 43.1 54.1 CORE + FiE COS + FiE 51.4 56.9 57.8 63.2 54.9 61.5 Table 8: End-to-end QA results on OTT-QA. 6 Question Answering Experiments Here, we conduct end-to-end question-answering experiments on NQ, OTT-QA and HotpotQA, using retrieval results from COS. Following the literature, we report exact match (EM) accuracy and F1 score. For NQ and OTT-QA, we re-implement the Fusion-in-Encoder (FiE) model (Kedia et al., 2022) because of its superior performance on NQ. For NQ, the model reads top-100 passages returned by COS, and for OTT-QA, the model reads top-50 evidence chains, in order to be comparable with previous work. Here, separate models are trained for each dataset independently. Due to space con- straints, we only present the results on OTT-QA and leave the NQ results to Table A2. The OTT- QA results are summarized in Table 8. Our model, when coupled with the FiE, is able to outperform the previous baselines by large margins on OTT- QA, and we can see that the superior performance of our model is mainly due to COS. Finally, for HotpotQA, since the task requires the model to predict supporting sentences in addi- tion to the answer span, we follow Zhu et al. (2021) to train a separate reader model to learn answer pre- diction and supporting sentence prediction jointly. Due to space constraints, we leave the full results to Table A3. Overall, our method achieves compet- itive QA performance against the previous SOTA with improved exact match accuracy. 7 Related Work Dense retrievers are widely used in recent litera- ture for ODQA (Lee et al., 2019; Karpukhin et al., 2020). While most previous work focuses on sin- gle retrieval (Xiong et al., 2021a; Qu et al., 2021), some efforts have also been made towards better handling of other query types. Xiong et al. (2021b) propose a joint model to handle both single retrieval and expanded query retrieval. Chen et al. (2021b) train a dense model to learn salient phrase retrieval. Ma et al. (2022a) build an entity linker to handle multi-hop retrieval. Nevertheless, all those mod- els are still customized for specific datasets, e.g., only a subset of query types are considered or sep- arate models are used, making them un-reusable and computationally intensive. We address these problems by pinning down a set of functional skills that enable joint learning over multiple datasets. Mixure-of-expert models have also become pop- ular recently (Fedus et al., 2021b). Methods like gated routing (Lepikhin et al., 2020) or stochastic routing of experts (Zuo et al., 2021) do not differ- entiate the knowledge learned by different experts. Instead, our work builds expert modules that learn reusable skills which can be flexibly combined for different use cases. Another line of work focus on unsupervised dense retrievers using self-supervised data con- structed from the inverse-cloze-task (Lee et al., 2019), random croppings (Izacard et al., 2021), truncation of passages with the same span (Ram et al., 2022), hyperlink-induced passages (Zhou et al., 2022) or synthetic QA pairs (Oguz et al., 2022). Other model architecture adjustments on Transformer for retrieval are proposed (Gao and Callan, 2021, 2022). Our work can be viewed as a synergy of both. Our multi-task pretrained model can perform better zero-shot retrieval. Our modular retriever can be further fine-tuned in a multi-task fashion to achieve better performance. 8 Conclusions In this work, we propose a modular model Chain-of-Skills (COS) learns five reusable skills for ODQA via multi-task learning. To reduce task interference, we design a new pa- rameterization for skill modules. We also show that skills learned by COS can be flexibly chained to- gether to better fit the target task. COS can directly perform superior zero-shot retrieval using multi- task self-supervision on Wikipedia. When fine- tuned on multiple datasets, COS achieves SOTA results across the board. For future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (Tal- mor et al., 2022) and biomedical retrieval (Nentidis et al., 2020; Zhang et al., 2022b). that Acknowledgements We would like to thank Aman Madaan, Sheng Zhang, and other members of the Deep Learning group at Microsoft Research for their helpful dis- cussions and anonymous reviewers for their valu- able suggestions on this paper. Limitations We identify the following limitations of our work. Our current COS\u2019s reranking expert only learns to rerank single-step results. Thus it can not model the interaction between documents in case of multi- passage evidence chains, which might lead to sub- optimal performance, e.g., when we need to rerank the full evidence path for HotpotQA. At the same time, we hypothesize that the capacity of the small model used in our experiments is insufficient for modeling evidence chain reranking. We leave the exploration of learning a full path reranker for fu- ture work. Also, our current pretraining setup only includes the three bi-encoder tasks, and thus we can not use the pretrained model out-of-box to solve tasks like end-to-end entity linking. Consequently, the learned skills from self-supervision can not be chained together to perform configurable zero-shot retrieval. It would be interesting to also include the entity span proposal skill in the pretraining stage, which could unleash the full potential of the Chain-of-Skills inference for zero-shot scenarios. References Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin- ton. 2016. Layer normalization. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544, Seattle, Wash- ington, USA. Association for Computational Linguis- tics. Wenhu Chen, Ming wei Chang, Eva Schlinger, William Wang, and William Cohen. 2021a. Open question answering over tables and"}