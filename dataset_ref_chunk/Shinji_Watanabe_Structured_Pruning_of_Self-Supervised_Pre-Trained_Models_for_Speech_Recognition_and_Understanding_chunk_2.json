{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Structured_Pruning_of_Self-Supervised_Pre-Trained_Models_for_Speech_Recognition_and_Understanding_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the training data in this context?", "answer": " The training data contains D samples, denoted as {(xi, yi)}D i=1.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " What is the training loss used in this case?", "answer": " The training loss is L, which is CTC loss for ASR and cross entropy loss for SLU.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " What reparameterization trick was proposed by Louizos et al. to make the loss differentiable?", "answer": " Louizos et al. proposed a reparameterization trick to make the loss differentiable.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " What distribution did Louizos et al. adopt to model the gates?", "answer": " Louizos et al. adopted the Hard Concrete Distribution.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " What function is used to stretch the output of the sigmoid function to the interval [l, r]?", "answer": " The function used is \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " How is the first term in Eq. (1) made differentiable?", "answer": " The first term in Eq. (1) becomes differentiable w.r.t. all parameters through reparameterization.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " How is the sparsity control implemented in training the sparse model?", "answer": " The sparsity control is implemented by optimizing the training loss subject to an explicit equality constraint on sparsity.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " What method is used to control the sparsity of the pruned model?", "answer": " An adversarial game according to the augmented Lagrangian method is used to control the sparsity of the pruned model.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " What does the joint pruning method proposed in Sec. 3.1 aim to optimize?", "answer": " The joint pruning method aims to optimize the overall computation by jointly pruning the CNN and Transformer.", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}, {"question": " What is the first proposed method for joint pruning based on the model size?", "answer": " The first proposed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size).", "ref_chunk": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}], "doc_text": "{(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and \u03bb > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their \ufb01nal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z: 1 \u03b2 u 1 \u2212 u + log \u03b1 u \u223c U(0, 1), v(\u03b1) = \u03c3 log (cid:18) (cid:19)(cid:19) \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), (cid:18) where U(0, 1) is a uniform distribution over the interval [0, 1], \u03c3(\u00b7) is the sigmoid function and \u03b2 is a temperature constant. The actual parameters are \u03b1. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is \ufb01nally recti\ufb01ed to [0, 1]. It is proven that the \ufb01rst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2): Ez k\u02dc\u03b8k0 h i = n j=1 X P (zj 6= 0) = n j=1 X \u03c3 (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r which is also differentiable. P (\u00b7) denotes the probability. Now we can train a sparse model using Eq. (1). However, it is dif\ufb01cult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer \u03bbk\u02dc\u03b8k0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez s.t. s(\u03b1) = t, min \u03b8,\u03b1 # \" i=1 X where s(\u03b1) is the current sparsity and t is a pre-speci\ufb01ed target spar- sity. The sparsity is de\ufb01ned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters \u03b1, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(\u03b1). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]: D 1 D L(f (xi; \u02dc\u03b8), yi) Ez + g(\u03bb, \u03b1), max \u03bb min \u03b8,\u03b1 \" # i=1 X 2 g(\u03bb, \u03b1) = \u03bb1(s(\u03b1) \u2212 t) + \u03bb2(s(\u03b1) \u2212 t) (6) where \u03bb1, \u03bb2 \u2208 R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satis\ufb01ed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value. , , (cid:19) (1) (2) , (3) (4) (5) 2.3. Structured pruning of Transformer layers A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model\u2019s hidden size (768). We de\ufb01ne a gate for each pruning unit. Given an input sequence X \u2208 RT \u00d7d of length T and feature size d, the MHA and FFN at a particular layer are the following: h MHA(X) = (zhead k ATT(X; Watt k )), k=1 X FFN(X) = GeLU(XWffn 1 ) \u00b7 diag(zint) \u00b7 Wffn 2 , where ATT(\u00b7; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(\u00b7) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 \u2208 Rd\u00d7dint 2 \u2208 Rdint\u00d7d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we de\ufb01ne a gate zhid of size d and share it across layers as in [23]. k , Wffn 3. PROPOSED METHODS 3.1. Joint pruning based on the model size As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to de\ufb01ne the model sparsity s(\u03b1). Our \ufb01rst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speci\ufb01cally, given the current distribution parame- ters \u03b1, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model\u2019s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall size(\u03b1). However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (\u03b1)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have"}