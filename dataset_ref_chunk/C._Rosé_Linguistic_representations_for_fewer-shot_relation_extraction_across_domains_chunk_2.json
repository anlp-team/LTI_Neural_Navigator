{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/C._Rose\u0301_Linguistic_representations_for_fewer-shot_relation_extraction_across_domains_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some techniques mentioned in the text that seek to reduce the need for expensive human-annotated training data?", "answer": " Leveraging additional information in the form of dependency parses, translated texts for multilingual RE, or distantly supervised instances", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " How has supplementing training data with explicit linguistic structure, such as syntactic and semantic parses, impacted performance on several NLP tasks?", "answer": " Led to substantial improvements in in-domain performance on several NLP tasks", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " According to the text, what kind of parses have shown to be beneficial for Information Extraction (IE)?", "answer": " Semantic parses, in the form of AMRs", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " What is the goal of generating a complete graph representation of a specified procedure?", "answer": " To find salient entities in the procedural text and correctly identify the appropriate relations between them", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " What is the justification provided in the text for restricting the focus to the RE task?", "answer": " Entity recognition results vary widely and entity recognition accuracy imposes an upper bound on end-to-end relation classification. RE presents a common way to frame the tasks in each of the datasets", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " Why is entity recognition accuracy significant in the context of relation classification?", "answer": " It imposes an upper bound on end-to-end relation classification", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " What linguistic representations are used to annotate each context entity in the text?", "answer": " AMR (Abstract Meaning Representation) and dependency parses", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " What problem related to token alignment is mentioned in the context of using AMR parses?", "answer": " Nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " What tool is used for annotating each context span with dependency parses in the text?", "answer": " Stanza dependency parser", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}, {"question": " What is the purpose of AMR alignment as mentioned in the text?", "answer": " Producing token-level features from an AMR representation", "ref_chunk": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}], "doc_text": "graph, (Qu et al., 2020), or entity types (Peng et al., 2020), and (iii) leveraging additional information in the form of dependency parses (Yu et al., 2022), translated texts for multilingual RE (Nag et al., 2021), or distantly supervised instances (Zhao et al., 2021; Ye and Ling, 2019a). All of these techniques seek to alleviate the need of using expensive human- annotated training data. In this work, we question whether incorporating linguistic structure on exist- ing models can aid learning robust representations which can be transferred to other domains. 2.2 Linguistic frameworks for NLP Supplementing training data with explicit linguis- tic structure, in the form of syntactic and seman- tic parses has led to substantial improvements in the in-domain performance on several NLP tasks. Sachan et al. (2021) challenges the utility of syntax trees over pre-trained transformers for IE and ob- served that one can only obtain meaningful gains with gold parses. Semantic parses, in the form of AMRs, have shown to be beneficial for IE (Zhang et al., 2021; Zhang and Ji, 2021; Xu et al., 2022), even when the parses employed are not human- annotated. In this work, we raise the question of the utility of either kind of parse for few-shot RE in a cross-domain setting. 3 Methodology We design our methodology to test whether the inclusion of AMRs and dependency parses can im- prove the few-shot RE performance across datasets, by incorporating features from linguistic represen- tations. We show an overview of our architecture in Figure 1, and go into further detail in Section 3.4. Our three datasets have their goal of generating a complete graph representation of a specified pro- cedure. This graph is constructed by first finding salient entities in the procedural text, and then cor- rectly identifying the appropriate relations between them. While this joint task is both challenging and useful, we restrict ourselves to the RE task for two reasons. Firstly, entity recognition results, as measured by baselines proposed in each of the dataset papers, vary widely, and entity recognition accuracy imposes an upper bound on end-to-end relation classification. Secondly, RE presents a common way to frame the tasks in each of these datasets. Parser ...season with rosemary and thyme... Classi\ufb01er rosemary BERT thyme Add :and season-01 R-GCN Figure 1: Model architecture. Yellow tokens denote BERT special tokens. Dotted lines indicate using BERT embeddings to seed the graph for the R-GCN. 3.1 Dataset Preprocessing In order to simplify our dataset tasks into relation extraction, we begin by identifying tuples of (en- tity1, relation, entity2), where each entity refers to a span of text in the original document, and relation refers to the flow graph edge label from the dataset. We format each triple into an instance that contains the triple and its context. We consider the context to be the shortest set of contiguous sentences that span both entity text spans. To segment sentences, we use the en-core-sci-md model with default set- tings provided in SciSpacy (Neumann et al., 2019), to account for the scientific text in the MSCorpus dataset. So that our models do not learn shallow heuristics to predict relations based on entity type, as observed in Rosenman et al. (2020), we exclude the entity types from the original datasets. 3.2 Parsing We then annotate each context entity with two lin- guistic representations: AMR (Banarescu et al., 2013) and dependency parses. We choose AMR pri- marily for the quality of parsers available relative to other semantic formalisms: AMR parsing is a relatively popular task, and state-of-the-art parsers are often exposed to scientific text in their training. However, despite the quality of parses, AMR as a formalism presents several challenges to its use in downstream applications. Foremost among these is the problem of token alignment: nodes and edges in AMR graphs do not have links back to the words in the text that they are generated from. As a con- trast, we choose to use dependency parses as our syntactic framework, which are straightforward in their correspondence to the original text: each node corresponds to a word. For the dependency parses, we annotate each context span using the Stanza dependency parser (Qi et al., 2020), which produces a dependency graph per sentence. We then create a \"top\" node for the graph to link the individual trees for relations that span sentences. For the AMR parses, we use the SPRING model (Bevilacqua et al., 2021) as implemented in AMR- Lib 2 We additionally verified that the model did not perform significantly differently than the origi- nal implementation. In contrast to the dependency parser, we found SPRING to occasionally be brittle. Because of its sequence-to-sequence architecture which cannot enforce that the produced output is a valid parse, the model sometimes failed to produce a parse altogether. These errors were non-transient, and did not display a pattern we could discern. In the interest of evaluating the impact of off-the-shelf tools as they were, we chose to include instances without AMR parses in our datasets. Because of the brittleness of the SPRING model, we parsed sentences in the datasets individually. We then compose the graph representations of each context instance by joining the graphs of its constituent sentences. We follow the same procedure as with dependency parsing, joining all of the sentence- level AMR graphs with a top node. 3.3 AMR Alignment Because AMR nodes are not required to point back to the tokens that generated them, extracting token- level features to incorporate into our RE model relied on the task of AMR aligment. AMR align- ment is usually treated as a post-hoc task that relies on rule-based algorithms. We experimented with algorithms based on the common JAMR (Flanigan et al., 2014) and ISI (Pourdamghani et al., 2014) aligners. These were implemented in AMRLib as the RBW and FAA aligners, respectively. Both aligners perform poorly, especially on the scien- tific text in the MSCorpus dataset. Because align- ments are necessary to producing token-level fea- 2https://github.com/bjascob/amrlib tures from an AMR representation, we developed heuristics as a"}