{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Large_Language_Models_Enable_Few-Shot_Clustering_2024-02-27_01-10-55_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the overall accuracy of pairwise clustering?", "answer": " 86.7", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " Why is keyphrase clustering using GPT-3.5 in the instruction-only setting better than Instructor?", "answer": " GPT-3.5 has very detailed prompts compared to Instructor, which has brief prompts.", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " How many parameters does GPT-3 contain?", "answer": " 175B", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " Why does Instructor-XL perform poorly compared to GPT-3.5?", "answer": " Instructor-XL does not handle long prompts well.", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " What is the cost of collecting LLM feedback using Keyphrase Correction approach on the OPIEC59k dataset?", "answer": " $10.66", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " How many human labels can be generated for the same cost as 20K GPT-3.5 labels on the OPIEC59k dataset?", "answer": " Less than 700 human labels", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " According to Figure 5, which approach is more cost-effective than a true oracle pairwise constraint oracle for entity canonicalization?", "answer": " Querying an LLM", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " What conclusion is drawn about the use of LLMs in text clustering tasks in the study?", "answer": " LLMs can provide consistent improvements to the quality of clusters.", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " Who supported the work mentioned in the study?", "answer": " NEC Research Laboratories", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}, {"question": " Who are the individuals acknowledged for their guidance in the study?", "answer": " Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman", "ref_chunk": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}], "doc_text": "55.0 4500 149 57.0 2472 78 89.7 Overall Accuracy of Pairwise 86.7 95.0 96.8 Pseudo-Oracle Table 4: When re-ranking the top 500 points in each dataset, the LLM rarely disagrees from the original clus- tering, and when it does, it is frequently wrong. leads to a larger set of keyphrases with less con- sistency, while providing demonstrations without any instructions leads to a more focused group of keyphrases that sometimes fail to reflect the desired aspect (e.g. topic vs. intent). Why is keyphrase clustering using GPT-3.5 in the instruction-only (\u201cwithout demonstrations\u201d) set- ting better than Instructor, which is an instruction- finetuned encoder? While GPT-3.5\u2019s size is not published, GPT-3 contains 175B parame- ters, Instructor-base/large/xl contain 110M, 335M parameters, and 1.5B parameters, respec- tively. The modest scaling curve suggests that scale is not solely responsible. Our prompts for Instructor are brief (e.g. \u201cRep- resent utterances for intent classification\u201d), while our prompts for GPT-3.5 (in Appendix B) are very detailed. Instructor-XL does not handle long prompts well; in the bottom row of Table 3, we see that Instructor-XL performs poorly when given the same prompt that we give to GPT-3.5. We spec- ulate that today\u2019s instruction-finetuned encoders are insufficient to support the detailed, task-specific prompts that facilitate few-shot clustering. 5.3 The limitations of LLM post-correction LLM post-correction consistently provides small gains on datasets over all metrics \u2013 between 0.1 and 5.2 absolute points of improvement. In Ta- ble 4, we see that when we provide the top 500 most-uncertain cluster assignments to the LLM to reconsider, the LLM only reassigns points in a small minority of cases. Though the LLM pairwise oracle is usually accurate, the LLM is dispropor- tionately inaccurate for points where the original clustering already had low confidence. 5.4 How much does LLM guidance cost? We\u2019ve shown that using an LLM to guide the clus- tering process can improve cluster quality. How- PCKMeans Correction Keyphrase Method Data Size Cost in USD OPIEC59k ReVerb45k Bank77 CLINC Tweet 2138 12295 3080 4500 2472 $42.03 $33.81 $10.25 $9.77 $11.28 $12.73 $10.24 $3.38 $2.80 $3.72 $2.24 $10.66 $1.23 $0.95 $0.99 Table 5: We compare the pseudo-labeling costs of dif- ferent LLM-guided clustering approaches. We used OpenAI\u2019s gpt-3.5-turbo-0301 API in June 2023. ever, large language models can be expensive; us- ing a commercial LLM API during clustering im- poses additional costs to the clustering process. In Table 5, we summarize the pseudo-labeling cost of collecting LLM feedback using our three ap- proaches. Among our three proposed approaches, pseudo-labeling pairwise constraints using an LLM (where the LLM must classify 20K pairs of points) incurs the greatest LLM API cost. While PCKMeans and LLM Correction both query the LLM the same number of times for each dataset, Keyphrase Correction\u2019s cost scales linearly with the size of the dataset, making this infeasible for clustering very large corpora. 5.5 Using an LLM as a pseudo-oracle is cost-effective Using large language models increases the cost of clustering. Does the improved performance jus- tify this cost? By employing a human expert to guide the clustering process instead of a large lan- guage model, could one achieve better results at a comparable cost? Since pseudo-labeling pairwise constraints re- quires the greatest API cost in our experiments, we take this approach as a case study. Given a sufficient amount of pseudo-oracle feedback, we see in Figure 5 that pairwise constraint K-means is able to yield an improvement in Macro F1 (suggest- ing better purity of clusters) without dramatically reducing Pairwise or Micro F1. Is this cost reasonable? For the $41 spent on the OpenAI API for OPIEC59k (as shown in Table 5), one could hire a worker for 3.7 hours of labeling time, assuming an $11-per-hour wage (Hara et al., 2017). We observe that an annotator can label roughly 3 pairs per minute. Then, $41 in worker wages would generate <700 human labels at the same cost as 20K GPT-3.5 labels. Based on the feedback curve in Figure 5, we see Macro F1 Pair F1 k 9 5 C E I P O 0.8 0.7 0.6 PCKMeans (LLM oracle) PCKMeans (True oracle) KMeans 0.95 0.9 0.85 0.5 0 10K 20K 0.8 0 10K 20K 0.8 1 k 5 4 b r e v e r 0.75 0.7 0.65 0.9 0.8 0.6 0 10K 20K 0 10K 20K # of Constraints # of Constraints Figure 5: Collecting more pseudo-oracle feedback for pairwise constraint K-Means on OPIEC59k improves the Macro F1 metric without reducing other metrics. Compared to the same algorithm with true oracle con- straints, we see the sensitivity of this algorithm to a noisy oracle. that GPT-3.5 is remarkably more effective than a true oracle pairwise constraint oracle at this price point; unless at least 2500 pairs labeled by a true oracle are provided, pairwise constraint KMeans fails to deliver any value for entity canonicaliza- tion. This suggests that if the goal is maximizing empirical performance, querying an LLM is more cost-effective than employing a human labeler. 6 Conclusion We find that using LLMs in simple ways can pro- vide consistent improvements to the quality of clus- ters for a variety of text clustering tasks. We find that LLMs are most consistently useful as a means of enriching document representations, and we be- lieve that our simple proof-of-concept should moti- vate more elaborate approaches for document ex- pansion via LLMs. 7 Acknowledgements This work was supported by a fellowship from NEC Research Laboratories. We are grateful to Wiem Ben Rim, Saujas Vaduguru, and Jill Fain Lehman for their guidance. We also thank Chenyang Zhao for providing valuable feedback on this work. References Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining Text Data. David Arthur and Sergei Vassilvitskii. 2007. means++: the advantages of careful seeding. ACM-SIAM Symposium on Discrete Algorithms. Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. 2013. Local algorithms for interactive clustering. Journal of Machine Learning Research, 18:3:1\u20133:35. Juhee Bae, Tove Helldin, Maria Riveiro, S\u0142awomir Nowaczyk, Mohamed-Rafik Bouguelia, and G\u00f6ran Falkman. 2020. Interactive clustering: A"}