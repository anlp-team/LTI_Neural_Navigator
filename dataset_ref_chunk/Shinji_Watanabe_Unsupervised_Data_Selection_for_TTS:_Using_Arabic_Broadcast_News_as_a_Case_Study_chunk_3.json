{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Unsupervised_Data_Selection_for_TTS:_Using_Arabic_Broadcast_News_as_a_Case_Study_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the PostNet in the Transformer-TTS model?", "answer": " The PostNet refines the predicted sequence by predicting a residual component of the target sequence.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " How does Transformer-TTS differ from Tacotron2 in terms of the self-attention mechanism?", "answer": " Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with a parallelizable self-attention structure, resulting in faster and more efficient training while maintaining high perceptual quality.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " How did the authors address the issue of limited training data in the model learning alignment between input and target sequences?", "answer": " They performed pre-training on the LJSpeech dataset, which consists of 24 hours of single-female English speech, before fine-tuning the model using a small amount of Arabic training data.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " What is the key difference between autoregressive (AR) models and non-autoregressive (non-AR) models in this study?", "answer": " The autoregressive models predict the target sequence iteratively, while the non-autoregressive models predict the target sequence in parallel without the need for iterative steps.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " How does FastSpeech2 differ from Transformer-TTS in terms of model architecture?", "answer": " FastSpeech2 is based on Conformer and uses a Conformer block instead of the Transformer block for the text-to-mel model.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " What is the role of the duration predictor in the FastSpeech2 model?", "answer": " The duration predictor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " What technique was used to generate speech from the predicted mel-spectrogram in this study?", "answer": " The study used the Griffin-Lim algorithm (GL) and Parallel WaveGAN (PWG) to generate speech from the predicted mel-spectrogram.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " How was the audio quality of the generated speech evaluated in the experiments?", "answer": " The audio quality was evaluated by human testers, and the word error rate (WER) and character error rate (CER) of the generated speech were also calculated using pre-trained ASR models.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " What is the importance of vowelization in the input transcription according to the study?", "answer": " Vowelization of the input transcription is important to solve the mismatch between the input text and the output pronunciation.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}, {"question": " What role does the reduction factor play in text-to-mel models?", "answer": " The reduction factor is a common parameter that decides the number of output frames at each time step, which is crucial for stable training especially with limited training data.", "ref_chunk": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}], "doc_text": "with location sensitive attention [26]. After the decoder, the convolutional PostNet re\ufb01nes the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention struc- ture. This enables faster and more ef\ufb01cient training while maintain- ing the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models. Since our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at \ufb01rst, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pre- training, we \ufb01ne-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages. 3.2. Non-autoregressive models We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a du- ration predictor, a pitch predictor, an energy predictor, a length regu- lator, a Conformer-based decoder, and PostNet. The duration predic- tor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respec- tively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-\ufb01tting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the up- sampled hidden representation into the target mel-spectrogram, and PostNet re\ufb01nes it. We did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model. 3.3. Synthesis We used the Grif\ufb01n\u2013Lim algorithm (GL) [30] and Parallel Wave- GAN (PWG) [31] to generate speech from the predicted mel- spectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference. 4. EXPERIMENTS We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then \ufb01ne- tuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of gener- ated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experi- mented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus. 4.1. Experimental Conditions Our training process requires \ufb01rst training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the \ufb01rst network. We investigated the per- formance with the combinations of the following three conditions: Model architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we \ufb01ne-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the ground- truth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models. Vowelization: Vowelization of the input transcription is impor- tant to solve the mismatch between the input text and the output pro- nunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness. Reduction factor: The reduction factor [5] is a common pa- rameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition. 3https://zenodo.org/record/4925105 Table 2. Objective evaluation results for the male speaker, where \u201cR\u201d repre- sents the reduction factor, \u201cVowel.\u201d represents whether to vowelize the input text, and the name in paraphrases (\u00b7) represents the teacher model used for the extraction of ground-truth durations. ID 1V 1 2V 2 3V 3 4V 4 5V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 5 6V FastSpeech2 (Taco2) FastSpeech2 (Taco2) 6 7V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 7 8V FastSpeech2 (Trans.) FastSpeech2 (Trans.) 8 Ground-truth - Model Tacotron2 Tacotron2 Tacotron2 Tacotron2 Transformer-TTS Transformer-TTS Transformer-TTS Transformer-TTS R 1 1 3 3 1 1 3 3 1 1 3 3 1 1 3 3 N/A Vowel. WER[%] CER[%] MCD[dB] 10.1 \u00b1 2.4 8.5 \u00b1 1.4 10.5 \u00b1 2.9 8.9 \u00b1 1.6 8.6 \u00b1 1.7 9.0 \u00b1 1.9 8.8 \u00b1 0.9 8.6 \u00b1 1.7 8.3 \u00b1 1.6 8.5 \u00b1 1.4 8.3 \u00b1 1.1 8.9 \u00b1 1.6 8.8 \u00b1 0.9"}