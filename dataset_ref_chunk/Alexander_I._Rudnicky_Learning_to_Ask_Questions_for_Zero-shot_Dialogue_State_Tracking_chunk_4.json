{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Learning_to_Ask_Questions_for_Zero-shot_Dialogue_State_Tracking_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What models were compared in Table 2?", "answer": " RoBERTa, RoBERTa-squad, RoBERTa-self, T5, T5-squad, T5-self, D3ST, and SDT.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " Why do sequence-to-sequence models generally outperform encoder-based approaches?", "answer": " Due in part to span-based models not supporting categorical slots directly, which make up a significant number of slots.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " What is the impact of different pretraining strategies according to Table 2?", "answer": " In-domain pretraining method outperforms models pretrained on SQuAD. Both strategies show strong improvements when compared to the vanilla backbone models.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " Why is pretraining in the QA task useful for zero-shot slot-filling?", "answer": " It indicates that pretraining in the QA task is useful for zero-shot slot-filling because it shows strong improvements when compared to the vanilla backbone models.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " What can more pretraining be done with according to the text?", "answer": " More pretraining can be done by combining other QA datasets besides SQuAD, and even using other DST data when applying the pretraining step.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " What does Table 3 display in relation to T5-self performance?", "answer": " Table 3 displays the impact of different question generation strategies: Generative LLM, Pronouns, Template-based What-is, Simple, and Manual.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " What does the performance improvement of LLM versus Pronouns highlight?", "answer": " It highlights how small differences in questions can make a difference, showing that using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " What is the impact of different question generation approaches according to Table 3?", "answer": " The impact is that LLM-based strategies outperform manual question generation, while template-based approaches perform comparably to question generation without requiring access to a LLM or prompt.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " Why did the train domain experience a steep performance drop with the LLM strategy?", "answer": " Due to the LLM misrepresenting the slots train-arriveby and train-leaveat, and the semantic differences in the train-leave day question compared to pretraining questions.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}, {"question": " What does the study conclude about the zero-shot DST method presented?", "answer": " It concludes that the zero-shot DST method presented requires no human input or domain knowledge through QA. The questions are posed by a generative LLM, allowing for easy addition of new slots and domains without extra manual workload.", "ref_chunk": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}], "doc_text": "TRADE [23] MA-DST [13] T5-DST (QA) [16] TransferQA [15] T5-self (ours) 13.7 16.3 19.8 22.7 29.4 11.5 13.6 21.8 26.3 54.0 60.6 59.3 64.4 61.9 65.5 19.9 22.5 32.5 31.3 46.5 22.4 22.8 32.6 36.7 35.6 Zero-shot with extra information Li et al. [14] T5-DST [16] D3ST [26] SDT [9] 24.4 21.2 21.8 33.9 26.2 21.7 38.2 72.0 59.6 64.6 78.4 86.4 31.3 33.1 56.4 74.4 29.1 35.4 38.7 62.9 Table 2: We experiment with all classes of the models. Model nomenclature consists of the backbone model name (RoBERTa or T5), then the pretraining strategy. Model Hotel Rest. Taxi Attr. Train RoBERTa RoBERTa-squad RoBERTa-self 27.8 29.4 29.8 30.0 35.5 47.3 63.1 61.6 64.1 38.1 38.5 40.8 24.3 23.7 28.3 T5 T5-squad T5-self 27.5 28.8 29.4 35.4 37.7 54.0 66.1 65.3 65.5 43.6 44.1 46.5 38.9 39.2 35.6 However, we under-perform when compared to D3ST and SDT. This is expected, as these models utilize descriptions or full examples, alongside listing all possible slot values when applicable. Our model obtains comparable performance while using substantially less information. In Table 2, we show both classes of models and each pretrain- ing step. Generally, the sequence-to-sequence models outperform encoder-based approaches. This is due in part to span-based models not supporting categorical slots directly, which make up a signifi- cant number of slots [25]. Impact of pretraining. Table 2 also displays the impact of 4.2.1 different pretraining strategies. Our in-domain pretraining method outperforms models pretrained on SQuAD. Nevertheless, both strategies show strong improvements when compared to the vanilla backbone models. This indicates that pretraining in the QA task is useful for zero-shot slot-filling. Note that although our pretraining strategy doesn\u2019t use any DST annotations, it still interacts with dia- logues whose domains we evaluate on the cross-domain adaptation task. This situation, however, can be reflected in the real world, where users of a dialogue agent may attempt to invoke features before they are present, and the features are later added. In this situation, our in-domain pretraining methodology is viable\u2014the dialogues happen, but there are no annotations\u2014so we argue that it is comparable to the other approaches. This may also indicate that more pretraining can be done: we can combine other QA datasets besides SQuAD [15], and even use other DST data when applying our pretraining step. This is left as future work. Avg. 25.6 26.9 34.3 35.8 46.2 34.1 35.2 46.7 65.9 Avg. 36.7 37.7 42.1 42.3 43.0 46.2 Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Table 3: T5-self performance with different question genera- tion strategies. Hotel Rest. Taxi Attr. Train Generative LLM Pronouns 29.4 29.2 54.0 54.3 65.5 65.4 46.5 47.5 35.6 32.5 Template-based What-is Simple 28.5 28.9 47.0 44.3 63.7 64.5 44.4 46.0 39.9 40.7 Manual 29.0 45.5 65.0 41.9 41.9 Impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 . Table 3 displays the impact of different ques- 4.2.2 tion generation approaches, with each row referring to a unique strategy. The performance improvement of LLM versus Pronouns highlights how small differences in questions can make a differ- ence: using third person pronouns whenever possible contrasts with the questions present in both pretraining strategies. Ques- tions with the natural pronouns of a two-person dialogue (i.e. \"you\" and \"I\") generally outperformed these. Forcing natural pronouns causes the performance of LLM-based \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategies to outper- form manual question generation. On the other hand, we find that simpler, template-based approaches, perform comparably to ques- tion generation, without requiring access to a LLM or prompt. The semantic similarities between slots, while not as explicit, are still in display when using these questions. It\u2019s worth noting that the per- formance improvement of the LLM strategy isn\u2019t consistent across domains. We see a steep performance drop in the train domain, largely due to the LLM misrepresenting the slots train-arriveby and train-leaveat. The train-leave day question is also worth noting: although semantically correct, it is considerably different from the pretraining questions. Both these factors could explain its\u2014and the train domain\u2019s\u2014lower overall performance. In the Manual strat- egy, where train-arriveby and leaveat are fixed, the performance is considerably higher. This indicates that there is space for manual, domain-specific, tuning when generating \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , which goes beyond our zero-shot DST premise. 5 CONCLUSIONS We presented a zero-shot DST method that requires no human input or domain knowledge, through QA. The questions are posed by a generative LLM, and as such, new slots and domains can be easily added without any extra manual workload. We experimented with two pretraining methodologies, both of which outperform non- pretrained models, and as such, may be used to bootstrap models throughout the early data collection processes, to train more robust, full-shot, approaches. For future work, we plan to explore other approaches to automatically generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 , introduce pretraining methods which focus on more, varied, data, and explore methods for automatically correcting misrepresented slot questions. 6 ACKNOWLEDGEMENTS This work was partially funded by the FCT Scholarship PRT/BD/ 152803 /2021, the NOVA LINCS project (UIDP/04516/2020), and the CMU Portugal project iFetch (LISBOA-01-0247-FEDER-045920). Avg. 46.2 45.8 44.7 44.9 44.6 Learning to Ask Questions for Zero-shot Dialogue State Tracking REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. [2] Guan Lin Chao and Ian Lane. 2019. BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2019-September (7 2019), 1468\u20131472. https://doi.org/ 10.48550/arxiv.1907.03040 [3] Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z Pan, and Huajun Chen. 2021. Knowledge-aware Zero-Shot Learning: Survey and"}