{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Neural_Machine_Translation_for_the_Indigenous_Languages_of_the_Americas:_An_Introduction_chunk_16.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What technique is mentioned in the text that involves training an initial translation model with synthetic data generated via back-translation (BT) and then fine-tuning it with gold data?", "answer": " Training an initial translation model with synthetic data generated via BT and then finetuning it with gold data is a technique mentioned in the text.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " According to the text, what could happen if back-translation (BT) is trained without a tagging strategy for high-resource languages (HRLs)?", "answer": " The text mentions that using back-translation (BT) without a tagging strategy for high-resource languages (HRLs) can degrade performance.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " How can back-translation (BT) be further improved according to the text?", "answer": " The text explains that using back-translated data from multiple sources or optimizing the ranking of back-translated data can yield further gains in performance.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " What is the benefit of using back-translation (BT) with naturally occurring text parallel corpora instead of translationese corpora?", "answer": " The text states that using back-translation (BT) with naturally occurring text parallel corpora instead of translationese corpora improves automatic n metrics.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " How can sentence modification be achieved according to the text?", "answer": " The text mentions that sentence modification can be achieved by replacing a randomly chosen word in a sentence with a soft-word.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " What is one way to augment machine translation (MT) data mentioned in the text?", "answer": " One way to augment MT data mentioned in the text is by paraphrasing, which can increase the number of training instances.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " What are some methods mentioned in the text for augmenting existing data?", "answer": " The text mentions augmenting existing data by adding continuous or discrete noise, paraphrasing, and using minimum risk training to optimize BLEU.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " What challenges do pivot-based approaches face in neural machine translation (NMT) according to the text?", "answer": " The text states that pivot-based approaches in NMT suffer from error propagation challenges.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " What shared task is highlighted in the text that explores the case of language pairs with no parallel data between them?", "answer": " The LoResMT 2020 shared task is highlighted in the text, which explores language pairs with no parallel data.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}, {"question": " How does transfer learning help low-resource tasks according to the text?", "answer": " The text explains that transfer learning helps low-resource tasks by allowing training with a lower amount of data.", "ref_chunk": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}], "doc_text": "an in- dividual task. A.3 Data augmentation Back-translation Caswell et al. (2019) shows that adding a special tag to the synthetic data im- proves performance. A technique that exploits this idea is training an initial translation model with synthetic data generated via BT and then finetune it with gold data (Abdulmumin et al., 2019). This simple yet effective training algorithm improves NMT for LRLs; however, it can also degrade per- formance on HRLs if trained without a tagging strategy (Marie et al., 2020). Multiple improvements of BT have been pro- posed. Edunov et al. (2018) shows that sampling or noisy beam search can generate more effective pseudo-parallel data. However, for LRLs an op- timal beam search and greedy decoding are bet- ter. A factor that influences BT\u2019s effectiveness is the quality of the initial MT systems (Hoang et al., 2018). Using back-translated data from mul- tiple sources (Poncelas et al., 2019) or optimizing the ranking of back-translated data yields further gains (Soto et al., 2020). BT results in gains when the parallel corpora are naturally occurring text and not translationese, as the latter would only improve automatic n metrics (Toral et al., 2018; Graham et al., 2020). ? shows that BT produces more fluent text and is preferred by humans. Additionally, translationese and origi- nal data can be modeled as separate languages in a multilingual model (Riley et al., 2020). BT is also a central part of unsupervised MT (UMT; cf. \u00a76.4) and zero-shot MT (Gu et al., 2019). Sentence modification Zhu et al. (2019) pro- poses to replace a randomly chosen word in a sen- tence with a soft-word. That means that, instead of sampling a word from the lexical distribution of a LM like Kobayashi (2018), the authors use the hidden state vector of the LM directly. Wu et al. (2019) substitutes the RNN LMs from pre- vious work and use BERT (Devlin et al., 2019) \u2013 a transformer trained with a masked language modeling objective \u2013 instead. The authors finetune BERT with a conditional masked language mod- eling objective that tries to avoid the prediction of words that do not correspond to the original sen- tence meaning. Another way to augmented MT data is by para- phrasing. If a good paraphrase system exists, this can increase the number of training instances (Hu et al., 2019). Paraphrasing can also be used at training time by sampling paraphrases of the refer- ence sentence from a paraphraser and training the MT model to predict the distribution of the para- phraser (Khayrallah et al., 2020). This helps the model to generalize. Wieting et al. (2019) propose a similar approach, using minimum risk training to optimize BLEU. To avoid BLEU\u2019s constraints to a specific reference, they use paraphrasing to diver- sify the given reference. Finally, existing data can be augmented by adding noise. This noise can be continuous or dis- crete. In the case of applying continuous noise, noise vectors are added to the word embeddings (Cheng et al., 2018; Sano et al., 2019). Discrete noise is realized by inserting, deleting, or replac- ing words, BPE tokens, or characters to expand the training set in an adversarial fashion (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng et al., 2019, 2020). Pivoting While it is simple to implement and effective, pivot-based approaches suffer from er- ror propagation. To overcome that for NMT, joint training Zheng et al. (2017); Cheng (2019) and round-trip training (Ahmadnia and Dorr, 2019) have been proposed. Pivoting with NMT systems has been used for translating Japanese, Indonesian, and Malay into Vietnamese (Trieu et al., 2019), translation of related languages (Pourdamghani and Knight, 2019), multilingual zero-shot MT (Lakew et al., 2018), and UMT (cf. \u00a76.4) between distant lan- guage pairs (Leng et al., 2019). A.4 Recent low-resource Shared Tasks First, the LoResMT 2020 shared task (Ojha et al., 2020) explores the case of language pairs which have no parallel data between them (Hindi\u2013 Bhojpuri, Hindi\u2013Magahi, and Russian\u2013Hindi). The winning system (Laskar et al., 2020) uses a MASS model in a zero-shot fashion with ad- ditional monolingual data (see \u00a76.4). Second, the WMT 2020 shared tasks on UMT and very low-resource supervised MT (Fraser, 2020) pro- vide text and 60k aligned phrases for German\u2013 Upper Sorbian., The most important technique in all tracks is transfer learning, achieving surpris- ingly good results. For the AmericasNLP 2021 shared task on open MT (Mager et al., 2021), 10 indigenous language languages were paired with Spanish, resulting in an extreme low-resource set- ting (4k to 125k paired sentences), with challenges out as domain, dialectical, and orthographic mis- matches between splits and datasets. The best systems shows that data cleaning and collection (\u00a7??) as well as multilingual approaches (\u00a76.1) result in the best performance in this conditions. Finally the shared task on MT in Dravidian lan- guages (Chakravarthi et al., 2021) features 3 lan- guages paired with English as well as Tamil\u2013 Telugu. Again, the winning system uses a mul- tilingual approach. The best performing systems use BT (\u00a76.3) and BPE word segmentation (\u00a72.1). The results from these challenges indicate that the optimal selection and combination of meth- ods differs between cases (i.e., amount of mono- lingual, parallel data, cleanness of data, domain mismatch, linguistic closeness of languages). This implies that data analysis and linguistic knowl- edge are needed to improve a final system\u2019s per- formance. A.5 Transfer learning This helps low-resource tasks as a lower amount of data can be used for training. One application of transfer learning to MT is the usage of a pre- trained RNN LM (Gulcehre et al., 2015) as the de- coder in an NMT system. Zoph et al. (2016) is the first work that uses pretrained models to improve NMT systems. The authors perform two experi- ments with an RNN encoder\u2013decoder architecture with an attention mechanism: the model is first pretrained on a high-resource language pair This works even better if related languages are used during pretraining (Nguyen and Chiang, 2017). Using pretrained LMs at decoding time"}