{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Cuttlefish:_Low-Rank_Model_Training_without_All_the_Tuning_chunk_19.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of model is described as having a stable rank of 85.46\u00b10.60?,answer: CUTTLEFISH", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " What is the stable rank value for WideResNet-50 vanilla model?,answer: 76.86\u00b10.01", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " Which model has a higher rank: ResNet-50 vanilla or ResNet-50 scaled?,answer: ResNet-50 vanilla", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " What does BERT stand for in the context of the text?,answer: Bidirectional Encoder Representations from Transformers", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " What is the number of parameters for Cuttlefish BERTLARGE model?,answer: 249", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " What is the meaning of the abbreviation MLM in the text?,answer: Masked Language Modeling", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " On which datasets were Vanilla and CUTTLEFISH BERT pre-trained?,answer: Wikipedia and Bookcorpus", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " What training method is compared to CUTTLEFISH in Table 18?,answer: PUFFERFISH", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " What is the Top-5 validation accuracy for GraSP (60%) method?,answer: 91.86%", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}, {"question": " Where were the benchmarks for CUTTLEFISH and other baselines conducted?,answer: Single EC2 p3.2xlarge instance", "ref_chunk": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}], "doc_text": "rank 12.5 64.80\u00b10.82 85.46\u00b10.60 CUTTLEFISH DeiT-base scaled stable rank 58.3 81.52\u00b10.03 95.59\u00b10.04 CUTTLEFISH WideResNet-50 vanilla stable rank 29.1 76.86\u00b10.01 93.50\u00b10.03 CUTTLEFISH WideResNet-50 scaled stable rank 37.4 78.0\u00b10.06 94.04\u00b10.09 CUTTLEFISH ResNet-50 vanilla stable rank 11.9 74.96\u00b10.01 92.39\u00b10.07 CUTTLEFISH ResNet-50 scaled stable rank 14.7 76.44\u00b10.16 93.21\u00b10.03 Table 17. Vanilla and CUTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets. Model # Params. (M ) MLM Loss Vanilla BERTLARGE 345 1.58 Cuttle\ufb01sh BERTLARGE 249 1.6 Layer-0 Layer-9 Layer-3 Layer-6 0.4Rank Ratio 10.0 7.5 2.5 5.0 0.0 12.5% of the Total Training Epochs 0.2 Layer-15 Layer-12 0.0 Layer-13 0 Layer-1 0.0 0.4 20 Layer-7 Layer-10 0.6 Layer-16 60 100 120Epochs Layer-4 0.2 80 40 Layer-11 20 Layer-5 0.6 0 0.4 Layer-8 40 0.0 120Epochs Layer-2 60 0.2 80 100 Layer-14 150 100 0.4Rank Ratio 0.0 Layer-15 250Epochs 0.2 Layer-0 0 Layer-12 Layer-6 Layer-9 200 50 Layer-3 Layer-13 Layer-7 Layer-16 0.2 0.4 200 50 250Epochs Layer-4 Layer-1 0.0 0 150 100 Layer-10 100 Layer-8 0 0.2 50 0.4 Layer-14 Layer-2 200 150 Layer-5 Layer-11 0.0 250Epochs Figure 10. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-10 using stable rank. 250Epochs 0 100 150 Layer-12 0.4Rank Ratio 200 Layer-9 50 Layer-0 0.2 Layer-6 Layer-3 Layer-15 50 150 200 Layer-1 0.0 0 Layer-13 0.6 Layer-7 Layer-16 0.2 Layer-4 250Epochs 100 Layer-10 0.4 Layer-14 0.2 0.6 100 150 Layer-5 0.4 50 Layer-11 250Epochs Layer-8 200 Layer-2 0 0.0 100 Layer-12 200 Layer-6 0 150 250Epochs 0.6Rank Ratio Layer-15 Layer-0 0.4 0.2 50 Layer-3 Layer-9 200 Layer-1 0 100 150 Layer-10 Layer-13 0.6 Layer-7 0.4 Layer-4 0.2 250Epochs 50 Layer-14 0.2 200 Layer-11 Layer-5 0.4 150 Layer-8 100 0.0 250Epochs 0 50 Layer-2 Figure 11. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on CIFAR-100 using stable rank with batch size 1, 024. CUTTLEFISH: Low-rank Model Training without All The Tuning 0.4Rank Ratio 200Epochs 0.0 150 100 0 0.2 Layer-12 50 Layer-0 Layer-6 Layer-9 Layer-3 Layer-15 50 Layer-16 Layer-10 125 25 0.6 150 100 Layer-4 175 0.4 0 Layer-1 75 0.0 0.2 200Epochs Layer-13 Layer-7 150 200Epochs 0 0.2 0.4 0.6 25 100 0.0 Layer-8 Layer-14 Layer-2 50 125 175 Layer-11 75 Layer-5 Layer-9 50 0.2 Layer-3 0.0 0 Layer-6 Layer-15 Layer-12 0.4Rank Ratio 200Epochs 150 100 Layer-0 200Epochs 25 50 Layer-13 175 0.2 150 Layer-7 0.0 Layer-4 Layer-1 0 Layer-10 125 100 75 0.4 Layer-2 100 150 25 0 0.2 200Epochs 0.4 50 Layer-14 125 Layer-11 0.0 75 175 Layer-5 Layer-8 Figure 12. The stable ranks for various layers in ResNet-18 (the top row) and VGG-19 (the bottom row) trained on SVHN using stable rank with batch size 1, 024. Layer-44 Layer-47 0.0 60 0.2 40 Layer-35 0.3 Layer-38 80Epochs Layer-41 20 Layer-32 0 0.1 0.4Rank Ratio 10 80Epochs Layer-33 Layer-45 20 0.4 Layer-42 60 0.2 0.1 40 Layer-48 Layer-36 30 0 50 Layer-39 0.3 70 Layer-37 20 80Epochs 0 0.05 Layer-49 Layer-46 Layer-40 0.20 Layer-34 0.15 40 0.10 60 Layer-43 Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256. 37 0.15 1 1 21 33 0.05 49 0.20 29 9 9 53 73 45 77 13 17 17 Epochs 65 85 0.10 25 57 Layer Index 3 69 5 89 61 15 11 5 13 41 7 81 (a) ResNet-18 25 Layer Index 33 29 17 Epochs 45 49 15 69 57 61 5 5 65 0.1 53 7 9 9 21 37 81 1 0.2 73 13 13 11 41 3 17 77 (b) VGG-19 Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank. Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH, EB Train (30%, 50%) and GraSP (30%, 60%) over the task of ResNet-50 on ImageNet. Model # Params. Val. Acc. Val. Acc. ResNet-50 (M ) Top-1(%) Top-5(%) Full-rank 25.6 75.99 92.98 PUFFERFISH 15.2 75.62 92.55 EB Train (30%) 16.5 73.86 91.52 EB Train (50%) 15.1 73.35 91.36 GraSP (30%) 17.9 74.64 92.08 GraSP (60%) 10.2 74.02 91.86 CUTTLEFISH 14.7 75.80 92.70 CUTTLEFISH: Low-rank Model Training without All The Tuning 61 13 41 77 73 45 37 7 21 1 15 5 89 57 69 33 Layer Index 25 5 85 0.4 9 65 49 29 0.2 17 17 13 Epochs 1 81 11 3 53 0.3 0.1 9 (a) ResNet-18 9 11 9 13 41 29 53 49 1 15 61 57 69 65 81 3 Epochs 17 0.2 13 5 5 33 Layer Index 25 77 73 45 37 7 21 0.1 (b) VGG-19 Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank. 13 17 17 69 57 89 61 15 5 5 7 45 73 77 85 0.10 25 Layer Index 0.15 21 9 1 53 9 65 49 0.05 33 37 Epochs 3 11 81 1 29 41 13 (a) ResNet-18 61 15 5 5 0.4 57 65 49 77 37 45 73 Layer Index 25 33 69 9 0.2 17 3 11 1 21 7 29 9 0.3 53 13 41 13 Epochs 0.1 (b) VGG-19 Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank. 9 50 46 53 0.05 21 44 42 Layer Index 36 37 45 13 Epochs 41 29 0.15 57 1 17 34 73 48 61 65 69 33 0.20 25 49 0.10 38 77 5 32 40 Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank. CUTTLEFISH: Low-rank Model Training without All The Tuning Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted on a single EC2 p3.2xlarge instance. Model: ResNet-18 Params. (M ) Val. Acc. (%) Time"}