{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_HomeRobot:_Open-Vocabulary_Mobile_Manipulation_chunk_12.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many objects will be randomly placed in a scene with a total receptacle surface area of 10m2?,answer: Between 15-20 objects", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " Name two of the receptacles included in the simulation set.,answer: Bathtub and bed", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " What does the Diversity in Receptacle Instances section discuss?,answer: The variability of instances within each receptacle category", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " What improvements were made in scene visuals according to the text?,answer: Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support were added", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " What is the cost of the improvements in scene rendering in terms of FPS?,answer: Only a 3% drop in training FPS", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " What are the two choices of action space discussed for navigation agents?,answer: Discrete and continuous action spaces", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " What are the three different repositories within the open-source HomeRobot library mentioned?,answer: home_robot, home_robot_sim, home_robot_hw", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " How does the controller operate in the Low-Level Control for Navigation section?,answer: By producing continuous velocity commands that move the robot to an input goal pose", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " Where is the global robot pose obtained from in the Pose Information section?,answer: From Hector SLAM on the Hello Robot Stretch", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}, {"question": " What is the main functionality provided by Agents and Environments in the HomeRobot implementation?,answer: Agents contain code to execute policies while Environments provide common logic and observations to the Agent", "ref_chunk": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}], "doc_text": "if the total receptacle surface area for a scene is 10m2, then 15-20 objects will be placed. The exact number of objects will be randomly selected per episode to be in this range. The full set of included receptacles in simulation is: bathtub, bed, bench, cabinet, chair, chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart, shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, washer_dryer. D.3 Diversity in Receptacle Instances The instances within each receptacle category exhibit substantial variability. Figure 11 shows a few different receptacles from our dataset belonging to the \"table\" category. D.4 Scene Clutter Complexity Our procedural placement of target and distractor objects creates diverse and interesting scenarios that require reasoning over which direction to approach receptacles, stable placement in clutter, open vocabulary object detection under occlusion which makes the task quite challenging. Figure 12 shows a few examples of clutter surrounding target objects in our scenes. 22 & Figure 12: A few examples of clutter surrounding the target object in our simulation settings. Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient Occlusion (HBAO) and expanded Physics-based Rendering (PBR) material support added to the Habitat renderer. The top row shows images from the default renderer whereas the bottom row shows the improved renderings. D.5 Improved scene visuals We rewrote and expanded the existing Physically-Based Rendering shader (PBR) and added Horizon- based Ambient Occlusion (HBAO) to the Habitat renderer, which led to notable improvements in viewing quality which were necessary for using the HSSD [19] dataset. Rewrote PBR and Image Based Lighting (IBL) base calculations. Added multi-layer material KHR_materials_clearcoat, KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy for both direct and indirect (IBL) lighting. support covering Added tangent frame synthesis if precomputed tangents are not provided. Added HDR Environment map support for IBL. We present comparisons between default Habitat visuals and improved renderings in Figure 13. We also benchmark the ObjectNav training speeds of a DDPPO-based RL agent with and without the improved rendering and present the results in 14. We see that the improvement in scene lighting and 23 Figure 14: Minor drop in FPS with improved scene rendering: Here, we benchmark the training speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and PBR-based improved scene visuals. We observe that the improved rendering leads to a very small drop in FPS from around 340 to 330 (3 % drop). rendering comes at the cost of only a 3% dip in training FPS (decreasing from around 340 to around 330). D.6 Action Space Implementation We look at two different choices of action space for our navigation agents, either making discrete or continuous predictions about where to move next. Our expectation from prior work might be that the discrete action space would be notably easier for agents to work with. Discrete. Previous benchmarks often operate in a fully discrete action space [20, 6], even in the real world [2]. We implement a set of discrete actions, with fixed in-place rotation left and right, and translation of steps 0.25m forward. Continuous. Our continuous action space is implemented as a teleporting agent, where the robot needs to move around by predicting a local waypoint. Our robot\u2019s low-level controllers are expected to be able to get the robot to this location, in lieu of simulating full physics for the agent. In simulation, this is implemented as a check against the navmesh - we use the navmesh to determine if the robot will go into collision with any objects if moved towards the new location, and move it to the closest valid location instead. E HomeRobot Implementation Details Here, we describe more specifics for how we implemented the heuristic policies provided as a baseline to accelerate home assistant robot research. Although there exists a considerable body of prior research looking at learning specific grasping [101, 98, 99, 97] or placement [102, 17] skills, we found that it was easiest to implement heuristic policies with low CPU/GPU requirements and high interpretability. Other recent works have similarly used heuristic grasping and placement policies to great affect (e.g. TidyBot [62]). There are three different repositories within the open-source HomeRobot library: home_robot: Shared components such as Environment interfaces, controllers, detection, and segmentation modules. home_robot_sim: Simulation stack with Environments based on Habitat. home_robot_hw: Hardware stack with server processes that run on the robot, client API that runs on the GPU workstation, and Environments built using the client API. 24 Most policies are implemented in the core home_robot library. Within HomeRobot, we also divide functionality between Agents and Environments, similar to how many reinforcement learning benchmarks are set up [20]. Agents contain all of the necessary code to execute policies. We implement agents that use a mixture of heuristic policies and policies learned on our scene dataset via reinforcement learning. Environments provide common logic; they provide Observations to the Agent and a function that allows them to apply their action to the (real or simulated) environment. E.1 Pose Information We get the global robot pose from Hector SLAM [103] on the Hello Robot Stretch [22], which is used when creating 2d semantic maps for our model-based navigation policies. E.2 Low-Level Control for Navigation The Hello Stretch software provides a native interface for controlling the linear and angular velocities of the differential-drive robot base. While we do expose an interface for users to control these velocities directly, it is desirable to have desired short-term goals as a more intuitive action space for policies, and to make them updateable at any instant to allow for replanning. Thus, we implemented a velocity controller that produces continuous velocity commands that move the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the robot so that it faces the goal position at all times while moving towards the goal position, and then rotating to reach the goal orientation once the goal position is reached. The velocities to induce these motions are inferred with a trapezoidal velocity profile and some conditional checks to"}