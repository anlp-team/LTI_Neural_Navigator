{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_12.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the fundamental challenge in Partial Label Learning (PLL)?,answer: Label ambiguity", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " What is the most straightforward approach to handling Partial Label Learning (PLL)?,answer: The average-based method", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " What is a significant drawback of average-based methods in Partial Label Learning (PLL)?,answer: False positive labels can mislead them", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " In contrast to average-based methods, what type of methods have researchers explored for Partial Label Learning (PLL)?,answer: Identification-based methods", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " How do identification-based methods for Partial Label Learning (PLL) differ from average-based methods?,answer: Identification-based methods treat all candidate labels differently", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " What is one of the techniques incorporated into identification-based methods for Partial Label Learning (PLL) according to the text?,answer: Deep learning techniques", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " What assumption do methods incorporating deep learning techniques into Partial Label Learning (PLL) rely on?,answer: The ground-truth label is present in the candidate set", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " What are some of the recent developments mentioned for improvement in generating pseudo-labels in semi-supervised learning?,answer: MixMatch, ReMixMatch, UDA, FixMatch, Dash, FlexMatch, FreeMatch, SoftMatch, Label Propagation methods, Meta Pseudo Labels", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " How do consistency regularization and self-training contribute to semi-supervised learning?,answer: They encourage the network to generate similar predictions for inputs under varying perturbations", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}, {"question": " What is a common issue faced by self-training techniques in semi-supervised learning according to the text?,answer: Confirmation bias", "ref_chunk": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}], "doc_text": "we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. B.1 PARTIAL LABEL LEARNING Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). The most straight- forward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, H\u00fcllermeier & Beringer (2006) employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample\u2019s neighborhood equally and predicting the ground-truth label through voting strategies. However, a significant drawback of average-based methods is that false positive labels can mislead them. To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based 19 19 19 19 20 21 22 22 23 23 23 23 24 24 25 26 27 28 Preprint Notation Table 6: Notation Table Definition x y [\u03b9] X = {xi}i\u2208[N ] Data. A set of data instances x of size N X Y = {yi}i\u2208[N ] Y I = {[\u03b9]i}i\u2208[N ] f g h f \u25e6 g \u03b8 p(y|x; \u03b8) f \u25e6 h D L Aw As zw zs M s S xl yl xu yu X L Y L X U Y U \u02c6pu \u02c6yu \u03c4 \u02c6y \u02c6yoh \u02c6Y u, v, m \u03b1(i, y) \u03b2(i, y) T (\u02c6y|y; \u03c9) \u03c9 A training instance A class index label An imprecise label, which might contain multiple class indices Input space where x is drawn from Ground-truth labels. A set of label indices y of size N Label space where y is drawn from Imprecise labels. A set of imprecise labels [\u03b9] of size N Model backbone Model classifier Model multi-layer perceptron Model mapping X \u2192 Y Learnable parameters of f \u25e6 g Output probability from model f \u25e6 g Model mapping X \u2192 Z, where Z is a projected feature space Dataset Loss function Weak data augmentation, usually is HorizontalFlip Strong data augmentation, usually is RandAugment (Cubuk et al., 2020) Projected features from f \u25e6 h on weakly-augmented data Projected features from f \u25e6 h on strongly-augmented data Memory queue in MoCo (He et al., 2020) A partial label, with ground-truth label contained A set of partial labels A labeled training example A labeled class index A unlabeled training example A unknown class index for unlabeled data A set of labeled data instances A set of labels for labeled data instances A set of unlabeled data instances A set of unknown labels for unlabeled data instances The maximum predicted probability on unlabeled data max(p(y|xu; \u03b8)) The pseudo-label from the predicted probability on unlabeled data arg max(p(y|xu; \u03b8)) The threshold for confidence thresholding A corrupted/noisy label An one-hot version of the corrupted/noisy label A set of noisy labels Noise model related parameters in SOP (Liu et al., 2022) The forward score in forward-backward algorithm The backward score in forward-backward algorithm The simplified noise transition model in ILL The parameters in the simplified noise model methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion (Nguyen & Caruana, 2008; Zhang et al., 2016b) or the maximum likelihood criterion (Liu & Dietterich, 2012). Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN (Lv et al., 2020) proposed a self-training strategy that disambiguates candidate labels using model outputs. CC (Feng et al., 2020b) introduced classifier- consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS (Wen et al., 2021) relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wang et al. (2022a) incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios. B.2 SEMI-SUPERVISED LEARNING Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying 20 Preprint perturbations (Tarvainen & Valpola, 2017; Samuli & Timo, 2017; Miyato et al., 2018). Self-training (Lee et al., 2013; Arazo et al., 2020; Sohn et al., 2020) is a widely-used approach for leveraging unlabeled data. Pseudo Label (Lee et al., 2013), a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias (Arazo et al., 2020), where the model struggles to rectify its own errors when learning from inaccurate pseudo labels. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch (Berthelot et al., 2019b), for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020a), and FixMatch (Sohn et al., 2020) adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash (Xu et al., 2021), FlexMatch (Zhang et al., 2021b), and FreeMatch (Wang et al., 2023) dynamically adjust these thresholds following a curriculum learning approach. SoftMatch (Chen et al., 2023) introduces a novel utilization of pseudo-labels through Gaussian re-weighting. Label Propagation methods (Iscen et al., 2019) assign pseudo labels based on the local neighborhood\u2019s density. Meta Pseudo Labels (Pham et al., 2021) proposes generating pseudo labels with a meta learner. SSL learning has also seen improvements through the incorporation of contrastive loss (Li et al., 2021a; Zheng et al., 2022). Furthermore, MixUp (Zhang et al., 2017) has shown its effectiveness in a semi-supervised learning (SSL) (Berthelot et al., 2019b;a; Cai et al., 2022). B.3 NOISY LABEL LEARNING Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like"}