{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_HomeRobot:_Open-Vocabulary_Mobile_Manipulation_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main objective of the task described in the text?", "answer": " To move an object from a start_receptacle to a goal_receptacle", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " What is the purpose of the simulation version of the Open-Vocabulary Mobile Manipulation problem?", "answer": " For reproducibility, training, and fast iteration", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " What type of robot platform is used in the study?", "answer": " Hello Robot Stretch with DexWrist", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " Why is the chosen robot platform considered safe to use in labs and homes?", "answer": " Because it is human safe and human-sized", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " What types of objects are included in the dataset?", "answer": " Seen vs. unseen categories and instances", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " What are receptacles in the context of the study?", "answer": " Common household receptacles like tables, chairs, and sofas", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " How is success computed in the task?", "answer": " Success is computed for finding object on start_receptacle, picking up object, finding goal_receptacle, and placing object on the goal", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " What is the Habitat Synthetic Scenes Dataset (HSSD) comprised of?", "answer": " 200+ human-authored 3D home scenes with over 18k 3D models of real-world objects", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " How are training episodes different from validation and test episodes?", "answer": " Training episodes consist of objects from seen instances of seen categories, while validation and test episodes use unseen instances of seen and unseen categories", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}, {"question": " Where are real-world experiments conducted?", "answer": " In a controlled 3-room apartment environment", "ref_chunk": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}], "doc_text": "the language names of start_receptacle, object, and goal_receptacle, pick up an object that is known to be on a start_receptacle and move it to any valid goal_receptacle. start_receptacle is always available, to help agents know where to look for the object. task is set up as instructions of the form: \u201cMove (object) The agent is successful if the specified object is indeed moved from a start_receptacle on which it began the episode, to any valid goal_receptacle. We give partial credit for each step the robot accomplishes: finding the start_receptacle with the object, picking up the object, finding the goal_receptacle, and placing the object on the goal_receptacle. There can be multiple valid objects that satisfy each query. Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile Manipulation problem, for reproducibility, training, and fast iteration, and (2) a real-robot stack with a corresponding real-world benchmark. We compare the two in Fig. 2. Our simulated environments allow for varied, long-horizon task experimentation; our real-world HomeRobot stack allows for 4 experimenting with real data, and we design a set of real-world tests to evaluate the performance of our learned and heuristic baselines. The Robot. We use the Hello Robot Stretch [22] with DexWrist as the mobile manipulation platform, because it (1) is relatively affordable at $25, 000 USD, (2) offers 6 DoF manipulation, and (3) is human safe and human-sized, making it safe to test in labs [24, 11] and homes [2], and can reach most places a human would expect a robot to go. For a breakdown of hardware choices, see Sec. H.1. Objects. These are split into seen vs. unseen categories and instances. In particular, at test time we look at unseen instances of seen or unseen categories; i.e. no seen manipulable object from training appears during evaluation. Agents must pick and place any requested object. Receptacles. We include common household receptacles (e.g. tables, chairs, sofas) in our dataset; unlike with manipulable objects, all possible receptacle categories are seen during training. Scenes. We have both a simulated scene dataset and a fixed set of real-world scenes with specific furniture arrangements and objects. In both simulated and real scenes, we use a mixture of objects from previously-seen categories, and objects from unseen categories as the goal object for our Open-Vocabulary Mobile Manipulation task. We hold out validation and test scenes, which do not appear in the training data; while some receptacles may re-appear, they will be at previously unseen locations, and target object instances will be unseen. Scoring. We compute success for each stage: finding object on start_receptacle, successfully picking up object, finding goal_receptacle, and placing object on the goal. Overall success is true if all four stages were accomplished. We compute partial success as a tie-breaker, in which agents receive 1 point for each successive stage accomplished, normalized by the number of stages. More details in Appendix C. 3.1 Simulation Dataset The Habitat Synthetic Scenes Dataset (HSSD) [19] consists of 200+ human-authored 3D home scenes containing over 18k 3D models of real-world objects. Like most real houses, these scenes are cluttered with furniture and other objects placed into realistic architectural layouts, making navigation and manipulation similarly difficult to the real world. We used a subset of HSSD [19] consisting of 60 scenes for which additional metadata and simulation structures were authored to support rearrangement 4. For our experiments, these are divided into train, validation, and test splits of 38, 12, and 10 scenes each, following the splits in the original HSSD paper [19]. Objects and Receptacles. We aggregate objects from AI2- Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob- jects [61] and the HSSD [19] dataset to create a large and diverse dataset of real-world robot problems. In total, we annotated 2,535 ob- jects from 129 total categories.We identified 21 different categories of receptacles which appear in the HSSD dataset [19]. Figure 3: HSSD scenes. We construct our final set of furniture receptacle objects by first automatically labeling stable areas on top of receptacles, then manually refining and processing these in order to remove invalid or in- accessible receptacles. In addition, collision proxy meshes were automatically generated and in many cases manually corrected to support physically ac- curate procedural placement of object arrangements. SC, SI SC, UI UC, UI Total 85 Cats Insts 1,363 64 748 44 424 129 2,535 Table 2: # of objects in the sim for each split of (S)een and (U)nseen (I)nstance and (C)ategory. 4All 200+ scenes with rearrangement support will be released soon. 5 HomeRobot is a simple, easy-to-set-up library that works in multiple environments and Figure 4: requires only relatively affordable hardware. Computationally intensive operations are performed on a desktop PC with a GPU, and a dedicated consumer-grade router provides a network interface to a robot running low-level control and SLAM. Episode Generation. We generate episodes consisting of varying object arrangements and particular values for object, start_receptacle, and goal_receptacle, which allow our agent to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile Manipulation, this task is particularly challenging because we have to place objects in locations that are navigable, meaning that the robot can get to them, reachable, meaning its arm can make it to these locations, and from which we can navigate to a navigable, reachable goal receptacle. For full episode generation details see App. D.2. Training and Validation Split. Training episodes consist of objects from the large pool of seen instances of seen categories (SC,SI). In contrast, we use unseen instances of seen object categories (SC,UI) and unseen instances of unseen categories (UC,UI) for validation and test episodes. Two- thirds of the categories were randomly designated as seen, and two-thirds of the objects in the seen category were randomly marked as seen instances. Splits are in Table 2 and the distribution of objects across categories is in App. Fig. 6. 3.2 Real World Benchmark Real-world experiments are performed in a controlled 3-room apartment environment, with a sofa, kitchen table, counter with bar, and"}