{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_AV-SUPERB:_A_Multi-Task_Evaluation_Benchmark_for_Audio-Visual_Representation_Models_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of AV-SUPERB?", "answer": " AV-SUPERB specializes in audio-visual tasks from speech and audio processing.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " How many evaluation tracks are set up in AV-SUPERB according to the text?", "answer": " Three evaluation tracks are set up in AV-SUPERB.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " What are the three evaluation tracks in AV-SUPERB referred to as?", "answer": " The three tracks are referred to as audio-only, video-only, and audio-visual fusion features.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " What is the goal of the benchmark mentioned in the text?", "answer": " The goal of the benchmark is to provide insight on the generalization capabilities of pretrained representations.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " What is the name of the upstream model referred to in the text?", "answer": " The upstream models referred to in the text are AV-HuBERT, RepLAI, AVBERT, MAViL, and HuBERT.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " What type of network architecture is used for the downstream models in the benchmark?", "answer": " The downstream models consist of a two-layer fully-connected network.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " What is the purpose of using handcrafted features in the baselines?", "answer": " Handcrafted features are used in the baselines to make a fairer comparison between audio and audio-visual features.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " Why is a weighted sum of representations extracted over different Transformer layers used for downstream tasks?", "answer": " Using a weighted sum of representations helps improve the performance of downstream tasks.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " What tasks are focused on in the utterance-level classification tasks for audio processing?", "answer": " The tasks focused on are audio event classification (AEC) and action recognition (AR).", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}, {"question": " Which model excels at audio processing tasks according to the text?", "answer": " MAViL excels at audio processing tasks.", "ref_chunk": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}], "doc_text": "visual-and-language domains [29, 30] or gen- eral multimodal learning[31]. In contrast, AV-SUPERB specializes in audio-visual tasks from speech and audio processing, allowing for more holistic assessment of representation models of audio and video alike. 3. BENCHMARK DETAILS As shown in Figure 1, audio-visual models typically consist of two separate unimodal encoders followed by multimodal fusion layers. Based on this design, we setup three evaluation tracks in AV- SUPERB to benchmark representations from the two encoder and fusion layers, referred as audio-only, video-only, and audio-visual fusion features. This also allows for easy comparison with previous unimodal representation models. Instead of striving for best possible performance for each task, the goal of our benchmark is to provide insight on the generaliza- tion capabilities of pretrained representations; therefore, we freeze the parameters of the task-invariant pretrained representation model (hereby referred as upstream model), and only finetune the param- eters of the task-specific model (hereby referred as downstream model), following previous work [7]. Downstream models are designed to be simple and lightweight in order to purely evalu- ate representation abilities. Following the spirit of representation evaluation, we also limit hyperparameter tuning for downstream tasks. Although, we recognize that different representations may have vastly different loss landscapes, hence we search for the best performing learning rate from 10\u22121 to 10\u22125 in log-scale. 3.1. Downstream Task Selection To keep computational costs reasonable, we mainly focus on utterance-level classification tasks in speech and audio processing, with the addition of ASR. For audio processing, we select two audio classification tasks that highlight the relevance of different modalities, audio event clas- sification (AEC) and action recognition (AR). Since audio events are often directly caused by actions, these tasks are complementary, and utilizing both audio and visual information can lead to better repre- sentations. This enables the possibility of learning better representa- tions from multimodal input compared to unimodal baselines. For speech processing, we select three audio-visual speech pro- cessing tasks where visual information is known to be beneficial [32, 33, 34], automatic speech recognition (ASR), automatic speaker verification (ASV), and emotion recognition (ER), in order to assess model capabilities on three fundamental aspects of speech: content, speaker, and paralinguistic information. In designing the architecture for the downstream models, we generally follow the setup used for utterance-level tasks in the SU- PERB benchmark. Specifically, the downstream model consists of a two-layer fully-connected network. This network takes the mean of features extracted from the frozen upstream model as input, and outputs class probabilities. However, as we also include the frame- level ASR task, we employ a two-layer BiLSTM model that takes the whole representation sequence as input and outputs characters. 3.2. Pretrained Upstream Models To showcase the utility of our benchmark, we opt for the base version of four audio-visual upstream models, AV-HuBERT [35], RepLAI [36], Lee et al.\u2019s model [37] (hereby referred as AVBERT through- out this paper), and MAViL [38]. These models were specifically chosen because they each excel at different tasks, underscoring the current gap in multi-tasking capabilities within existing audio-visual models. They vary substantially in terms of architecture, training objectives, and preprocessing techniques. We also conduct exper- iments on the base HuBERT [2] model, an unimodal speech repre- sentation model with similar design as AV-HuBERT, to make a fairer comparison between audio & audio-visual features. Additionally, we incorporate two baselines that use handcrafted features as input for downstream models. Specifically, we employ log mel filterbank (FBANK) for audio and histogram of oriented gra- dients (HoG) for video, respectively. 4. EXPERIMENTAL RESULTS AND DISCUSSION Previous work has shown that simply using representations extracted at the last layer of a frozen self-supervised model often results in sub- optimal performance [39, 7]. Hence, we take a learnable weighted- sum of representations extracted over different Transformer layers as the final representation for each downstream task. For the audio-only and video-only tracks, only unimodal input and the relevant layers are used for extracting representations. For the audio-visual fusion track, both of the unimodal encoders plus fusion layers are used. As the size of representations extracted from fusion Transformer layers differ from those of unimodal layers, we take the weighted-sum for fusion Transformer layers only. Audio-Visual Speech-Visual Representation Type Params. Overall Score AEC AS-20K VGGSound (mAP \u2191) (Acc. \u2191) Kinetics- Sounds (Acc. \u2191) AR ASR ASV ER UCF101 LRS3-TED VoxCeleb2 IEMOCAP (Acc. \u2191) (CER \u2193) (EER \u2193) (Acc. \u2191) Audio-only FBANK HuBERT AV-HuBERT* RepLAI AVBERT MAViL 0 95M 90M 5M 10M 86M 36.69 53.66 53.20 39.70 44.81 54.11 2.8 14.3 12.6 12.3 20.5 21.6 5.12 30.21 31.14 27.01 37.67 39.91 24.73 51.46 49.02 45.90 55.28 57.28 19.91 36.06 38.58 33.85 43.26 45.68 20.07 2.96 3.01 66.09 80.23 24.43 27.16 15.58 14.45 32.58 23.74 20.71 51.52 62.14 58.54 57.53 60.94 59.46 Video-only HoG AV-HuBERT* RepLAI AVBERT MAViL 0 103M 15M 37M 87M 25.39 33.48 36.40 47.69 49.70 1.5 2.4 5.5 11.5 18.0 3.81 5.90 13.5 28.73 32.08 18.70 24.73 46.68 62.67 74.01 25.67 37.55 56.69 77.42 79.37 71.46 50.91 71.33 72.29 74.03 36.32 11.90 36.95 20.00 24.58 35.83 26.59 40.72 45.8 43.03 Audio-visual fusion AV-HuBERT AVBERT MAViL 103M 43M 187M 53.42 54.85 62.36 13.3 22.9 26.7 32.69 44.54 47.22 52.23 71.31 79.51 41.46 71.76 77.98 2.75 70.12 30.18 9.46 18.31 19.67 46.45 61.87 54.94 In order to fairly compare HuBERT & AV-HuBERT, we set features of the opposing modality to 0 and extract features from the 12-layer fusion Transformer for audio-only and video-only tracks. Table 1. Main results. Best results for each track are highlighted in bold. Second-best results are underlined. We observe that MAViL excels at audio processing tasks, while HuBERT and AV-HuBERT are better for speech processing tasks. 4.1. Downstream Datasets and Training Details Evaluation results for the three tracks are given in Table 1. For AEC, we evaluate on AudioSet [40] and VGGSound [41], and for AR, we select Kinetics-Sounds [15] and UCF101 [20]. Notably, in VGGSound and Kinetics-Sounds, audio and visual information are more correlated. This is reflected in our results, as audio-visual fu- sion improves representations more for the"}