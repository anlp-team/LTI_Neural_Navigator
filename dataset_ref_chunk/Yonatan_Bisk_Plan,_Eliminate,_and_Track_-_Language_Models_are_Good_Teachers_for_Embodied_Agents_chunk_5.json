{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_Plan,_Eliminate,_and_Track_-_Language_Models_are_Good_Teachers_for_Embodied_Agents_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How does PET compare to GPT in terms of performance on human goal specifications?", "answer": " PET outperforms GPT by 25% on seen and 5% on unseen splits for human goal specifications.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " Why does GPT require fine-tuning on fully text-based expert trajectory?", "answer": " GPT requires fine-tuning on fully text-based expert trajectory to adapt to different environment settings.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " What is one advantage of the Plan module of PET in comparison to GPT?", "answer": " The Plan module of PET generalizes to variations in human goal specifications without being specifically trained on the task.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " How does the performance drop compare between GPT and PET when transferring from template to human-goal specifications?", "answer": " GPT suffers from a 50% performance drop, while PET incurs only a 15 \u223c 25% drop when transferring from template to human-goal specifications.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " What is the closest setting to PET and how does it perform in comparison?", "answer": " The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC), which performs poorly compared to PET.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " Which LLM achieves the overall best performance on all dataset splits for the Plan module?", "answer": " MT-NLG with 530B parameters achieves the overall best performance on all dataset splits for the Plan module.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " What does the Macaw QA model demonstrate in its zero-shot receptacle/object masking performance on the three splits of AlfWorld?", "answer": " The Macaw QA model demonstrates consistent masking performance on all three splits of AlfWorld, even on the unseen split.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " How does adding Eliminate without sub-task tracking impact the performance when compared to adding Plan and Track together?", "answer": " Adding Plan and Track together greatly improves the completion rate by 60%, while adding Eliminate without sub-task tracking results in a relatively insignificant 3% improvement.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " What are the precision and recall values for the sub-task trackers Macaw-11B and Macaw-large?", "answer": " Macaw-11B has a precision of 0.99 and a recall of 0.78, while Macaw-large has a precision of 0.96 and a recall of 0.96.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}, {"question": " What are the types of failure examples mentioned for sub-task generation in the Plan Module?", "answer": " The two types of failure examples are generating synonyms of the ground truth and generating inaccurate sub-tasks.", "ref_chunk": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}], "doc_text": "4.4. Automated Analysis of PET modules Goals We compare the performance of action attention as- sisted by PET with BUTLER (Shridhar et al., 2020b) and \ufb01ne-tuned GPT (Micheli & Fleuret, 2021) in Ta- ble 1. For human goal speci\ufb01cations, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal speci\ufb01cations, GPT requires \ufb01ne-tuning on fully text- based expert trajectory and thus loses adaptability to di\ufb00erent environment settings. Qualitatively, on human goal speci\ufb01cation tasks, where the goal speci\ufb01cations are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal speci\ufb01cations as shown in Section 4.5. Quantitatively, GPT su\ufb00ers from a relative 50% perfor- mance drop transferring from template to human-goal speci\ufb01cations, whereas PET incurs only a 15 \u223c 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per- forms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outper- forms BUTLER with DAgger by more than 2x while being much more e\ufb03cient. (Section 4.1) Plan Module We experiment with di\ufb00erent LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo- 2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal speci\ufb01cations, where there is no variation in sentence structures. For human goal speci\ufb01cation, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform signi\ufb01cantly worse. Eliminate module We evaluate the zero-shot recep- tacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accu- racy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by 40% on average. 4.3. Ablations for Plan, Eliminate, and Track In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sam- pled from the training set. The data set size is chosen to match the size of the seen validation set, for an e\ufb03cient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately. Adding Plan and Track greatly improves the comple- tion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks step- by-step reduces the complexity. We observe a relatively insigni\ufb01cant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% rela- tive improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the perfor- mance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as \u201c\ufb01nished\u201d if and only if the environment is \u201cfully solved\u201d by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we \ufb01nd that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure exam- ples for sub-task generation in Table 4. The \ufb01rst type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu- Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) GPT-Neo-2.7B (Black et al., 2021) MT-NLG (Smith et al., 2022) 94.29 (0.97) 99.29 (1.00) 98.57 (0.99) 87.31 (0.94) 96.27 (0.98) 100 (1.00) 10.07 (0.62) 4.70 (0.82) 40.04 (0.94) 7.98 (0.58) 9.16 (0.80) 49.3 (0.94) Table 2. Evaluation of di\ufb00erent LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal speci\ufb01cation. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. Figure 6. Plot of AUC scores of zero-shot relevance identi\ufb01cation across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identi\ufb01cation. Bottom: Object relevance identi\ufb01cation. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. Model Ablations seen unseen Action Attention Action Attention + Eliminate Action Attention + Plan &"}