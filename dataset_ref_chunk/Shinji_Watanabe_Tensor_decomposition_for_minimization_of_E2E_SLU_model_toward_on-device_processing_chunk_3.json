{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Tensor_decomposition_for_minimization_of_E2E_SLU_model_toward_on-device_processing_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How is the rank of the Tucker decomposition calculated?", "answer": " The rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current compression ratio is under the given one.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " What technique is mainly used to reduce the parameters of the convolution layer in the model?", "answer": " The Tucker decomposition is mainly used to reduce the parameters of the convolution layer.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " What is the reason for adopting the Tucker decomposition in the model?", "answer": " The Tucker decomposition is adopted because it is expected to represent the convolution weight tensor with fewer parameters than CP decomposition.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " How can the Tucker decomposition be inferred without reconstructing the original tensor?", "answer": " The Tucker decomposition can be inferred without reconstructing the original tensor by swapping the order of computation as in SVD.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " What is a similarity between CP decomposition and SVD?", "answer": " CP decomposition, similar to SVD, can uniquely determine rank from compression ratio.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " What dataset was the system evaluated on?", "answer": " The system was evaluated on the STOP dataset.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " What evaluation criteria were used in the system?", "answer": " The evaluation criteria used were performed by exact match accuracy (EM).", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " What feature extractor was used in the teacher model?", "answer": " The teacher model used a HuBERT-based feature extractor.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " How many attention blocks did the encoder in the Conformer-based middle model have?", "answer": " The encoder in the Conformer-based middle model had 12 attention blocks.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}, {"question": " What optimization technique was used in the training of the system?", "answer": " Adam optimization with warmup scheduling was used in the training of the system.", "ref_chunk": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}], "doc_text": "core tensor shape cannot be uniquely determined from the given compression ratio (eq. (4)), the rank of the Tucker decomposition is calculated by iteratively halving all dimensions until the current ratio is under the given one as in Algorithm 1. In line 4, all dimensions are halved, and each time the compression ratio is recalculated in line 14. Lines 5\u201313 compensate so that each dimension is not less than 1. For comparison, the Tucker decomposition is replaced by CP and Tensor-Train decomposition. CP decomposition, sim- ilar to SVD, can uniquely determine rank from compression ratio (eq. (6)). However, Tensor-Train decomposition cannot be uniquely determined from the given compression ratio (eq. (8)). Therefore, the dimension is iteratively reduced, as Tucker decomposition. In our model, the Tucker decomposition is mainly used to re- duce the parameters of the convolution layer. The reasons for adopting the Tucker decomposition are that it is expected to be able to represent the convolution weight tensor with fewer pa- rameters than CP decomposition and that it contributes not only to parameter reduction but also to speed up [25]. The tucker decomposition can be inferred without reconstructing the orig- inal tensor by swapping the order of computation as in SVD. This technique is very compatible with on-device processing. On the other hand, Tensor-Train decomposition cannot acceler- ate inference because the original tensor must be reconstructed during inference. 4. Experimental evaluation 4.1. Experimental settings We evaluated our system on the STOP dataset [29]. STOP dataset consists of over 200,000 audio files from over 800 speakers and text and semantic parses. They are divided into train, valid, and test sets. Evaluation criteria were performed by exact match accuracy (EM), which is the percentage of perfect match of the label sequences. The rank of the decompositions is determined by the spec- ified compression ratio. We apply SVD to the linear layers and Tucker decomposition to the convolution layers in the models. The teacher model was based on the Conformer model. The model used a HuBERT-based feature extractor and convolution layers to reduce the input feature length. The encoder had 12 attention blocks, each with 512 dimensions with 8 heads. The (9) decoder had 6 attention blocks, and each block also had 512 dimensions with 8 heads. The number of parameters of the teacher model was 431M. The Conformer-based middle model had 47M parameters. Its encoder had 10 attention blocks, and each block had 384 dimensions with 6 attention heads. The decoder had 3 blocks with the same dimensions and attention heads. On the other hand, the E-Branchformer-based middle model had 53M parameters. Its encoder had 10 attention blocks, each with 384 dimensions with 6 heads. The decoder had 3 blocks with the same dimensions and heads. For the training of the middle and small models, we used speed perturbation. The teacher and ground truth labels were both used with speed perturbation for the distillation. On the other hand, the validation data consisted of ground truth text, even in the sequential distillation. The input feature was log mel-filter bank having 80 bins. Furthermore, we used SpecAug- ment technique [32]. After that, we applied utterance-level mean normalization. The target semantic parse labels were di- vided using the byte-pair encoding (BPE) model with 500 to- kens. Adam optimization with warmup scheduling was used in our training. Finally, we used an averaged model of the top 10 accuracy checkpoints. 4.2. Comparison of Conformer and E-Branchformer Table 1 shows the experimental results of our system on the STOP dataset with 15M parameter limitation. In this exper- iment, we used tucker decomposition (Sec 2.2) to reduce the convolution parameters. For tensor decomposition, we set the compression ratio of the encoder and decoder as 0.3 and 0.295, respectively, in Conformer. In the case of the E-Branchformer, the compression ratios were set to 0.25 and 0.3 for the encoder and decoder. By using tensor decompositions from the middle Conformer, the small Conformer achieved 65.4 EM. Further- more, the E-Branchformer encoder significantly improved the performance. Our system, E-Branchformer-based E2E SLU, achieved 70.1% EM for the test set with a parameter count of 15M, which was better than the previous study [7]. This result was a higher performance than the teacher model (69.4%). This was due to that the teacher model was based on a Conformer encoder and ground truth labels were also used for sequential distillation. Furthermore, under this condition, it appeared that the small model had not fully converged, so we continued train- ing it for an additional 200 epochs. As a result, the accuracy was increased to 70.9%. In addition we made comparisons with 30M parameters. Table 2 shows the experimental results of our system on the STOP dataset with 30M parameter limitation. In this limitation, we set the compression ratios of the encoder and decoder as 0.65 and 0.65 in Conformer. In the E-Branchformer, the encoder and decoder compression ratios were set as 0.55 and 0.6, respec- tively. The performance of the E-Branchformer was higher than Conformer but lower than the deliberation model [7]. The com- parison with 15M indicates that the E2E model is more advan- tageous when constructing the smaller model. Our E2E model does not explicitly separate ASR and NLU, so unlike [7], we can reduce the parameters in a balanced manner so that SLU performance remains high. 4.3. Comparison of different compression ratios Consequently, we investigated the performance changes with various compression ratios. Figure 2 shows the EM with vary- ing compression ratio in E-Branchformer with Tucker decom- position. We used the same compression ratio for both encoder Figure 2: EM of our system varying compression ratio with Tucker decomposition. Table 3: Experimental results (token accuracy and EM) on STOP dataset within 15M changing decomposition type. Tucker decomposition (Sec. 2.2) CP decomposition (Sec. 2.3) Tensor-Train decomposition (Sec. 2.4) TAcc. 91.8 34.8 90.7 EM 70.1 0.1 70.2 and decoder in this experiment. According to the figure, The EM drops sharply when the compression ratio falls under 0.3. Therefore, it is important to set an"}