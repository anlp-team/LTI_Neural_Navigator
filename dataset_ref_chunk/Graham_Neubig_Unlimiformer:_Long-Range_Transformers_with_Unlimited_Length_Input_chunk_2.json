{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Unlimiformer:_Long-Range_Transformers_with_Unlimited_Length_Input_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How does the given model encode an input sequence that is longer than the model\u2019s context window?", "answer": " By using the encoder to encode overlapping chunks of the input and keeping only the middle half of the encoded vectors from each chunk.", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " What library is used to index the encoded inputs in a kNN index?", "answer": " Faiss (Johnson et al., 2019).", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " In retrieval-augmented Cross-Attention, what is retrieved from the kNN index for each cross-attention head?", "answer": " The top-k hidden states.", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " How is the approach of attending to top-k hidden states from the kNN index different from standard cross-attention?", "answer": " It allows retrieval from the entire input sequence instead of truncating.", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " What is the advantage of retrieving the top-k hidden states in terms of computation and GPU-memory?", "answer": " It is cheaper than attending to all input tokens.", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " What is the goal of retrieving a set of keys Kbest?", "answer": " To maximize the attention score QK T best.", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " Why is the exact na\u00efve approach taken by Memorizing Transformers not preferred for querying the kNN index?", "answer": " It would require constructing separate indexes for the keys and values at each layer and each head, resulting in time-intensive and space-intensive operations.", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " How is the retrieval step reformulated to achieve space efficiency in the attention reformulation method?", "answer": " By choosing the encoder hidden states he that maximize (hdWqW \u22a4 ).", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " How does the reformulated index store hidden states efficiently?", "answer": " It stores only a single vector per input token.", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}, {"question": " What is the advantage of Unlimiformer\u2019s index structure in terms of memory usage for a large number of input tokens?", "answer": " It requires only 2GB of memory for 1,000,000 input tokens due to storing single vectors per input token.", "ref_chunk": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}], "doc_text": "To encode an input sequence that is longer than the model\u2019s context window, we use the given model\u2019s encoder to encode overlapping chunks of the input, following Ivgi et al. (2022). We keep only the middle half of the encoded vectors from each chunk, to ensure that the encodings have sufficient context on both sides. Finally, we index the encoded inputs in a kNN index, using a library such as Faiss (Johnson et al., 2019), using dot-product as the index\u2019s nearest-neighbor similarity metric. 2.2 Retrieval-augmented Cross-Attention In standard cross-attention, a transformer decoder attends to the encoder\u2019s top-layer hidden states, where the encoder usually truncates the input and encodes only the k first tokens in the input sequence. Instead of attending only to this k-token prefix of the input, we retrieve the top-k hidden states from the kNN index for each cross-attention head, and attend only to these top-k. This allows retrieval from the entire input sequence instead of truncating. Our approach is also cheaper, in computation and GPU-memory, than attending to all input tokens; and because softmax is dominated by the largest values, retrieving the most-attended tokens preserves the vast majority of attention mass. Figure 2 illustrates our generic changes to any sequence-to-sequence transformer\u2019s architecture. The full input is encoded using the encoder in chunks and indexed in a kNN index; then, the index of encoded hidden states is queried at each decoding step. The kNN search step is non-parametric and can be injected into any pretrained seq2seq transformer. The search step reformulates attention for space efficiency, as detailed below. 2.3 Attention reformulation Let hd be the decoder hidden state and he be an encoder\u2019s last layer hidden state. The standard cross-attention computation for a single head in a transformer is: Attn(Q, K, V ) = softmax (cid:18) QK T \u221a dk (cid:19) V where Q = hdWq is the product of the decoder states hd and the query weight matrix Wq; the keys K = heWk are the product of the last encoder hidden states he with the key weight matrix Wk; and V = heWv is similarly the product of he with the value weight matrix Wv. Our goal is to retrieve a set of keys Kbest that maximize QK T best, with the size of Kbest fixed to the size of the model\u2019s context window, and then compute the standard attention over Kbest only. Note that the linear layers Wq, Wk, and Wv are layer-specific and head-specific. Thus, na\u00efvely creating an index from the keys K = heWk and querying this index using the query vectors will require constructing separate indexes for the keys and values at each layer and each head, for a total of 2 \u00d7 L \u00d7 H indexes, where L is the number of decoder layers and H is the number of attention heads. In fact, this exact na\u00efve approach was taken by Memorizing Transformers (Wu et al., 2022), who pioneered the use of a kNN index for previously encoded inputs.3 A separate index for each attention head in each decoder layer is both time-intensive to create and space-intensive to store. So, not surprisingly, Wu et al. (2022) apply their memory layer to only a single decoder layer. Instead, we present a different order of computing the well-known transformer attention formula, which allows us to store a single index across all attention heads and all decoder layers, without changing the mathematical definition of the transformer\u2019s standard dot-product attention. The 3See Memorizing Transformers\u2019 official implementation at https://github.com/google-research/meliad/ blob/main/transformer/memory_factory.py#L78-L79 and https://github.com/google-research/meliad/blob/main/ transformer/memory_layer.py#L334-L339 3 (1) dot-product part of the transformer\u2019s attention computation can be rewritten as follows:4 QK T = (hdWq) (heWk)\u22a4 \u22a4 = (hdWq) W \u22a4 = (cid:0)hdWqW \u22a4 k he (cid:1) he k \u22a4 Thus, the retrieval step can be formulated as choosing the encoder hidden states he that maximize (cid:0)hdWqW \u22a4 \u22a4. This rewriting has two major advantages: first, there is no need to index the keys for each head and layer separately: we can create a single index of the hidden states he only, and just project the queries to hdWqW \u22a4 k using head-specific and layer-specific Wq and Wk; second, the values can be calculated trivially given he, so there is no need to store the values in a separate index from the keys before decoding. Thus, instead of constructing 2 \u00d7 L \u00d7 H indexes and retrieving from all indexes during each decoding step, we construct a single index from he and retrieve from it by just projecting the decoder hidden states to per-head per-layer hdWqW \u22a4 k . Using our reformulation, the index stores only a single vector per input token. Using 16-bit floats and hidden states of size 1024, this requires only 2GB of memory for 1,000,000 input tokens. Since indexes can be offloaded to the CPU memory, Unlimiformer\u2019s input length is practically unlimited. (cid:1) he k 3 Training Unlimiformer Unlimiformer can be used, at test time, with an already-trained model, and lead to gains without further training, as we show later in Table 3. Next, we turn our focus to training approaches to further improve the performance of Unlimiformer. Table 1 summarizes and contrasts the training approaches described below, and Appendix A contains further implementation details. Method name Training input total # tokens in example seen at training time Validation input (early stopping) Test input Baseline +test Unlimiformer +early stop w/ Unlimiformer Train chunked +test Unlimiformer SLED (Ivgi et al., 2022) Longformer (Beltagy et al., 2020) Random-encoded training Retrieval training Alternating training 1024 1024 1024 1024 16k 16k 8-16k 8-16k 8-16k 1024 1024 1024 all 16k 16k 8-16k 8-16k 8-16k 1024 1024 unlimited unlimited 16k 16k unlimited unlimited unlimited 1024 unlimited unlimited unlimited 16k 16k unlimited unlimited unlimited Table 1: A comparison of the training approaches using BART (context window size 1024) as a running example. The dashed line separates methods that are approximately the same training-time cost as the baseline, from those that require significant additional compute. 3.1 Low (additional-) Cost Training Methods: Applying Unlimiformer at"}