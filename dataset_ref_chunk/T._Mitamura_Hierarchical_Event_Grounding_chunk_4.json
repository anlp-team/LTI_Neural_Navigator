{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/T._Mitamura_Hierarchical_Event_Grounding_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of loss is added to the bi-encoder training to incorporate hierarchy?", "answer": " Hierarchy aware loss", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " How is the scoring function s(ep, ec) parameterized in the bi-encoder?", "answer": " Based on the ComplEx embedding", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " What has the ComplEx been shown to be effective in scoring?", "answer": " Asymmetric, transitive relations", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " How is the score s(ep, ec) calculated in the bi-encoder?", "answer": " s(ep, ec) = Im(ep) \u22c5 (Re(ec) \u2299 Im(r)) \u2212 Re(ep) \u22c5 (IM(ec) \u2299 Im(r))", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " What is the Re(e) in the bi-encoder implementation?", "answer": " Biased linear projection of event embedding into the real part of the complex space", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " In the training process, how is the bi-encoder optimized to minimize the loss?", "answer": " By minimizing the in-batch BCE loss", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " What are the three strategies explored for incorporating hierarchy prediction in learning the bi-encoder?", "answer": " Pretraining, Joint Learning, Pretraining + Joint Learning", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " What methodology is proposed in addition to mention-linking?", "answer": " Hierarchical relation extraction", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " How is the child-parent relation score computed in the hierarchical relation extraction process?", "answer": " h(ei, ej) = |Mei \u2229 Mej | / |Mei |", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}, {"question": " What metrics are used to evaluate the bi-encoder and cross-encoder in the experiments?", "answer": " Recall@k, Recall@min, Strict Accuracy, Macro F1, Micro F1", "ref_chunk": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}], "doc_text": "linking mentions to multiple KB events. However, they predict a \ufb02at set of events, overlooking any hierarchical relationships amongst them. To explicitly incorporate this hierarchy, we add a hierarchy aware loss in the bi-encoder training (Murty et al. 2018). In addition to scoring mention-event pairs, the bi-encoder is also optimized to learn a scoring function s(ep, ec) with the parent-child event pair ep, ec \u2208 K. We parameterize s based on the ComplEx embedding (Trouillon et al. 2016). It has been shown to be effective in scoring asymmetric, transitive relations such as hyper- nym in WordNet and hierarchical entity typing (Murty et al. 2018; Chen, Chen, and Van Durme 2020). ComplEx trans- forms type and relation embeddings into a complex space and scores the tuple with the real part of the Hermitian inner product. In our implementation, we use only the asymmetric portion of the product. In particular, given the embeddings of parent-child event pair ep, ec \u2208 K as ep, ec \u2208 Rd respectively, the score s(ep, ec) is obtained by: s(ep, ec) =hIm(r), Re(ec), Im(ep)i \u2212hIm(r), IM(ec), Re(ep)i =Im(ep) \u00b7 (Re(ec) \u2299 Im(r)) \u2212Re(ep) \u00b7 (IM(ec) \u2299 Im(r)) Where \u2299 is the element-wise product, r \u2208 Rd is a learn- able relation embedding. Re(e) = WRe \u00b7 e + bRe, Im(e) = WIm \u00b7 e + bIm Re(e) and Im(e) are the biased linear projections of event embedding into real and imaginary parts of complex space respectively. WRe, WIm \u2208 Rd\u00d7d and bRe, bIm \u2208 Rd are learn- able weights. During training, a batch of Nh parent-child event pairs is independently sampled and the bi-encoder is (1) (2) (3) trained to minimize the in-batch BCE loss: Lh = 1 Nh Nh X i=1 (\u2212 X ecj \u2208Ci log(\u03c3(s(epi, ecj))) + X eck /\u2208Ci log(\u03c3(s(epi, eck)))) Where Ci denotes the set of children events in the batch that are the children of epi. We further explore three strategies for incorporating the hierarchy prediction in learning the bi- encoder: \u2022 Pretraining: the bi-encoder is pre-trained with the hierarchy-aware loss, followed by training with the men- tion linking loss. Joint Learning: in each epoch, the bi-encoder is jointly optimized with both the hierarchy-aware and mention linking loss. Pretraining + Joint Learning: bi-encoder is pretrained with the hierarchy-aware loss, followed by joint training with the hierarchy-aware and mention linking loss. For each of the above bi-encoder con\ufb01gurations, we train a cross-encoder using the same training recipe as the base- line. We leave the development of hierarchy-aware cross- encoder models to future work. 5.3 Hierarchical Relation Extraction In addition to the mention-linking, we propose a methodol- ogy to leverage the trained bi-encoders for hierarchical rela- tion extraction. For each mention, we \ufb01rst retrieve the top-k event candidates. We then construct a list of mentions (Me) for each event e in the dictionary. Finally, given a pair of events (ei, ej), we compute a score for potential child-parent (ei\u2013ej) relation as follows, h(ei, ej) = |Mei \u2229 Mej | |Mei | The scoring function h is derived based on the intuition that if ej is the parent event of ei, then all the mentions which are linked to ei should also be linked to ej by our linker. In such case, Mei would be the subset of Mej which indicates that h(ei, ej) = 1 approaches its maximum. For each event, we iteratively calculate the child-parent score with every other event and rank them in descending order of the h score. With this process, we could obtain a ranking of all other events as its candidate parents. 6 Experiments We experiment with the proposed con\ufb01gurations of bi-encoders and corresponding cross-encoders on the Wikipedia dataset for both multilingual and crosslingual tasks. To further assess out-of-domain generalization per- formance, we conduct the same experiments for baseline and the best-performing hierarchy-aware system on the Wikinews evaluation set. For the hierarchy relation extrac- tion task, we evaluate the approach proposed in \u00a75.3 based on the retrieval results of the best-performing bi-encoder. (4) (5) 6.1 Metrics Bi-encoder: For bi-encoder, we follow prior work to re- port Recall@k and extend it to multi-label version: measur- ing the fraction of mentions where all the gold events con- tained in the top-k retrieved candidates. Since the longest path of hierarchies in the collected dataset consists of 4 events, we only evaluate with k \u2265 4. And for k < 4, we instead report Recall@min: the fraction of mentions where all the gold events contained in the top-x retrieved candi- dates, where x is the number of gold events for that mention. Recall@min measures whether the bi-encoder could predict all and only the gold events with the minimal number of retrievals. For cases with single gold event, Recall@min = Recall@1 which falls back to single event linking. Our task requires the model to predict all the relevant events, from the atomic event to the root of the hierarchy tree. As an upper bound for model performance, we also report scores for predicting the most atomic event. In par- ticular, the set of gold events is considered fully contained in top-k retrieval of a mention if the most atomic gold event in the hierarchy is contained in the top-k candidates. Our original task reduces to this atomic-only prediction task if the event hierarchy is known at test time. However, such an assumption might not be true in real-world settings. While Recall@k is a strict and binary metric, i.e. the re- trieval is counted as successful if and only if all the gold events are predicted, we further introduce a fraction version of it, denoted as Recall@k (fraction), that allows for partial retrieval, with details in Appendix D.1. Cross-encoder: Similar to bi-encoder, we also follow pre- vious work on entity linking to evaluate strict accuracy, macro F1, and micro F1 on the performance of cross- encoder. For a mention mi, denote its gold events set as Ei, predicted events set as \u02c6Ei, with N mentions: Strict Accuracy = P N i=1 1 N Ei= \u02c6"}