{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Best-Case_Retrieval_Evaluation:_Improving_the_Sensitivity_of_Reciprocal_Rank_with_Lexicographic_Precision_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of measuring ranking agreement between sgnLP and rrLP?", "answer": " To assess whether the choice between rrLP and sgnLP affects their correlation with reciprocal rank.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " How is agreement in system preference measured?", "answer": " As a percentage of preferences agreed upon.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " What does \u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) agree in sign with?", "answer": " \u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e)", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " According to the text, what indicates a robust approach in lexiprecision?", "answer": " Fewer ties observed amongst pairs of rankings and higher discriminative power.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " What statistical tests are used to compute p-values for discriminative power?", "answer": " Tukey\u2019s honestly significant difference (HSD) test and classic paired test.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " How is Best-Case Retrieval Evaluation related to improving the Sensitivity of Reciprocal Rank with Lexicographic Precision?", "answer": " By measuring ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " What does the correlation between \ud835\udeffRR1 and LP indicate?", "answer": " A high correlation between the two.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " Why is lexiprecision consistently preferred over the suffix \ud835\udeffRR1 across all datasets?", "answer": " Because it consistently agrees more with the target \ud835\udeffRR1.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " How is the relationship between reciprocal rank and lexiprecision tested under incomplete information?", "answer": " By considering the agreement between preferences with incomplete data and \ud835\udeffRR1 on complete data.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}, {"question": " What does the percentage of ties between rankings from different systems indicate?", "answer": " Lexiprecision has substantially fewer ties compared to other metrics.", "ref_chunk": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}], "doc_text": "Because sgnLP and rrLP always agree in sign, we will only show results for one of the metrics when computing ranking agreement. Agreement in system preference tests whether E\ud835\udc5e\u223cQ E\ud835\udc5e\u223cQ . This measures whether our choice of rrLP or sgnLP affects its correlation with reciprocal rank. Agreement is measured as a percentage of preferences agreed upon. (cid:104)\u0394LP (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) (cid:105) agrees in sign with (cid:105) (cid:104)\u0394RR (\ud835\udf0b\ud835\udc5e, \ud835\udf0b \u2032 \ud835\udc5e) In order to assess the robustness of lexiprecision, we measure the number of ties observed amongst pairs of rankings and discrim- inative power. We claim that a robust approach has fewer ties and higher discriminative power. For discriminative power, we adopt Sakai\u2019s approach of measuring the number of statistically signif- icant differences between runs [16], using both Tukey\u2019s honestly significant difference (HSD) test [3] and classic paired test to com- pute \ud835\udc5d-values. The paired test uses the Student\u2019s \ud835\udc61-test for reciprocal rank and rrLP [18]; and the binomial test for sgnLP. Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Table 2: Ranking agreement between \ud835\udeffRR1 and preferences based on the positions of the last \ud835\udc5a \u2212 1 relevant items. The computation of sgnLP in the table is based on the \ud835\udc5a \u2212 1 posi- tions of relevant items after the top-ranked relevant item. sgnLP \ud835\udeffRR2 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 85.78 89.23 88.01 85.87 87.29 88.91 87.22 86.51 88.02 86.56 83.73 92.41 90.45 92.86 91.97 78.90 66.50 58.84 83.44 87.30 86.58 84.79 85.41 87.54 85.45 84.45 85.82 83.10 79.34 89.78 88.87 91.08 90.14 77.56 66.08 58.25 5 RESULTS 5.1 Correlation with Reciprocal Rank By construction, we know that \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 LP(\ud835\udf0b, \ud835\udf0b \u2032) > 0 and, so, the correlation between the two will be high. We can further test this by comparing how well lexiprecision predicts a ground truth preference between rankings based on \ud835\udeffRR1. In our first analysis, given an observed \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \u2260 0, we measure the ability of lexiprecision and reciprocal based only on \ud835\udc5a \u2212 1 subsequent recall levels to predict the sign of \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032). That is, we use \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) as a target value and compute \ud835\udeffRR1 and LP using suffixes \ud835\udc5d2:\ud835\udc5a and \ud835\udc5d\u2032 2:\ud835\udc5a. Although artificial, this analy- sis provides an indication of the predictive value gained through cascaded modeling (as opposed to just looking at the top-ranked relevant item). We present the results in Table 2. As we can see, lexiprecision consistently agrees more with the target (masked) \ud835\udeffRR1 than \ud835\udeffRR1 of the suffix across all datasets, indicating that the additional information in higher recall levels can be used to predict the target (masked) \ud835\udeffRR1. This agrees with our preliminary analysis in Section 3.6. We can also test the relationship between reciprocal rank and lexiprecision by measuring the agreement under incomplete infor- mation. Specifically, we consider removing either labels (treating unlabeled items as non-relevant) or requests (i.e. queries or users). We then measure the agreement between preferences with incom- plete data and \ud835\udeffRR1 on complete data (i.e. all requests and labels). Methods that agree more with reciprocal rank on complete data are considered more correlated. We present results for ranking and system agreement when removing labels (Figure 8a) and queries 0.60.70.80.9 deep 0.60.70.80.9 recsys 0.40.50.60.70.80.9 newsranking 0.70.80.9 web 0.20.40.60.8label fractionsign agreement web system 0.20.40.60.8label fractionsign agreement news 0.70.80.9 deep 0.80.9 recsys 0.90.95 0.70.80.9 (a) Removing labels. deep news 0.20.40.60.8query fractionsign agreement 0.90.95 recsys 0.50.60.70.80.9 0.70.80.9 0.70.80.9 web system (b) Removing queries. Figure 8: Preference agreement with \ud835\udeffRR1 with full data. La- bels and requests removed randomly. Results averaged across ten samples. Solid green lines: \ud835\udeffRR1 with incomplete infor- mation. Dashed red lines: rrLP with incomplete informa- tion. Dotted blue lines: sgnLP with incomplete information. Shaded areas: one standard deviation across samples. Rank- ing agreement with incomplete labels for sgnLP is identical to rrLP and omitted for clarity. (Figure 8b). Across all conditions, we observe that the rrLP has as Table 3: Percentage of ties between pairs of rankings from two systems for the same request. We collapse rrLP and sgnLP for clarity. rrLP, sgnLP \ud835\udeffRR1 news robust (2004) core (2017) core (2018) web web (2009) web (2010) web (2011) web (2012) web (2013) web (2014) deep deep-docs (2019) deep-docs (2020) deep-docs (2021) deep-pass (2019) deep-pass (2020) deep-pass (2021) recsys ml-1M (2018) libraryThing (2018) beerAdvocate (2018) 0.39 0.23 1.72 4.93 0.61 1.02 0.34 0.83 0.64 1.06 2.43 0.23 2.63 2.58 1.32 3.38 16.48 41.73 44.22 48.50 31.43 15.13 25.85 41.99 34.01 31.09 41.93 68.45 73.99 80.84 56.89 50.30 47.41 21.39 25.85 45.72 high or slightly higher agreement with \ud835\udeffRR1 with complete infor- mation than \ud835\udeffRR1 with incomplete information. This means that rrLP can accurately predict \ud835\udeffRR1 with complete information as well or better than using reciprocal rank. Moreover, we observed that sgnLP shows weaker system agreement which occurs because its magnitude does not decay with rank position and, therefore, result- ing averages are inconsistent with averages of position-discounted reciprocal rank values. 5.2 Sensitivity In Section 2, we motivated our work by showing that RL1 metrics theoretically and empirically suffer from ceiling effects. The primary instrument we used to determine this was the probability of ties between rankings. In Table 3, we present the percentage of tied rankings from different systems for the same request. As predicted by our analysis in Section 3.3, lexiprecision has substantially fewer ties because this only happens when two rankings place relevant items in exactly the same positions. In Section 3.3, we showed that lexiprecision implicitly and ex- ponentially increased its fidelity as the number of relevant items \ud835\udc5a increased, while RL1 would quickly suffer from ties. In Figure 9, we show the number of tied rankings as a function of incomplete labels. This allows us to"}