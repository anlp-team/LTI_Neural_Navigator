{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Tensor_decomposition_for_minimization_of_E2E_SLU_model_toward_on-device_processing_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the different tensor decomposition techniques mentioned in the text?,answer: CP decomposition and Tensor-Train decomposition", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " What was the compression ratio set to in the experiment for the encoder and decoder?,answer: 0.25 and 0.3, respectively", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " Why did the CP decomposition reach a saturation point at small epochs?,answer: Because it decomposed all dimensions with the same rank, leading to an excessive loss of expressiveness", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " Which decomposition technique performed significantly well according to the text?,answer: Tensor-Train decomposition", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " What improvement did the E-Branchformer with Tucker decomposition achieve compared to the Conformer-based model?,answer: Higher performance in both 15M and 30M limitations", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " What compression ratio was found to be efficient for the E-Branchformer in the text?,answer: Around 0.3 points", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " What conclusion was drawn about the efficiency of Tucker decomposition compared to CP decomposition and Tensor-Train decomposition?,answer: Tucker decomposition was relatively efficient", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " What EM percentage did the E-Branchformer-based decomposed E2E SLU model achieve with 15M parameter limitation on STOP data?,answer: 70.9%", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " Who are some of the authors mentioned in the references section related to spoken language understanding?,answer: Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Kumar, Baiyang Liu, Yoshua Bengio, Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and many others", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}, {"question": " What is the title of the paper by Akshit Gupta mentioned in the references section?,answer: On building spoken language understanding systems for low resourced languages", "ref_chunk": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}], "doc_text": "appropriate compression ra- tio when applying tensor decomposition. 4.4. Comparison of different tensor decompositions We further examined the performance with different tensor de- composition techniques. In addition to the Tucker decompo- sition used in our system, we applied CP decomposition and Tensor-Train decomposition to the convolution layer. Table 3 shows the performance on the STOP dataset. In this experi- ment, the compression ratio was set to 0.25 and 0.3 for the en- coder and decoder, respectively, targeting 15M. CP decomposi- tion improved the training accuracy from the initial parameters, but it reached a saturation point at small epochs. This was be- cause the CP decomposition decomposed all dimensions with the same rank, which led to an excessive loss of expressiveness. In contrast, the Tensor-Train decomposition performed signifi- cantly well, as did the Tucker decomposition. EM was slightly better for the Tensor-Train decomposition, but we observed a large difference in token accuracy. 5. Conclusion In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied se- quential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, E- Branchformer with Tucker decomposition achieved higher per- formance in both 15M and 30M limitations. In addition, it ob- tained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposi- tion with the CP decomposition and Tensor-Train decomposi- tion showed that the Tucker decomposition was a relatively ef- ficient decomposition. Finally, our system, E-Branchformer- based decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, \u201cTowards end-to-end spo- ken language understanding,\u201d in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, \u201cEnd-to-end neural transformer based spoken language under- standing,\u201d Proc. Interspeech 2020, pp. 866\u2013870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, \u201cImproving spoken language understanding with cross-modal contrastive learning,\u201d Proc. Interspeech 2022, pp. 2693\u20132697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, \u201cAdversarial knowledge distillation for ro- bust spoken language understanding,\u201d Proc. Interspeech 2022, pp. 2708\u20132712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, \u201cTwo-pass low latency end-to- end spoken language understanding,\u201d Proc. Interspeech 2022, pp. 3478\u20133482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and \u201cFANS: Fusing ASR and NLU for on-device Ariya Rastrow, SLU,\u201d in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, \u201cDe- liberation model for on-device spoken language understanding,\u201d in Interspeech 2022, 2022, pp. 3468\u20133472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl\u00b4ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th\u00b4eodore Bluche, et al., \u201cSpoken language understanding on the edge,\u201d in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57\u201361. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, \u201cFast intent classifi- cation for spoken language understanding systems,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119\u20138123. [10] Thierry Desot, Franc\u00b8ois Portet, and Michel Vacher, \u201cEnd-to-end spoken language understanding: Performance analyses of a voice command task in a low resource setting,\u201d Computer Speech & Language, vol. 75, pp. 101369, 2022. [11] Akshat Gupta, \u201cOn building spoken language understanding sys- tems for low resourced languages,\u201d in Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonet- ics, Phonology, and Morphology, Seattle, Washington, July 2022, pp. 1\u201311, Association for Computational Linguistics. [12] Pu Wang et al., \u201cBottleneck low-rank transformers for low- resource spoken language understanding,\u201d Proceedings Inter- speech 2022, 2022. [13] Anderson R Avila, Khalil Bibi, Rui Heng Yang, Xinlin Li, Chao Xing, and Xiao Chen, \u201cLow-bit shift network for end-to-end spo- ken language understanding,\u201d Proc. Interspeech 2022, pp. 2698\u2013 2702, 2022. [14] Yingying Gao, Junlan Feng, Chao Deng, and Shilei Zhang, \u201cMeta auxiliary learning for low-resource spoken language understand- ing,\u201d Proc. Interspeech 2022, pp. 2703\u20132707, 2022. [15] Marco Dinarelli, Marco Naguib, and Franc\u00b8ois Portet, \u201cToward low-cost end-to-end spoken language understanding,\u201d Proc. In- terspeech 2022, pp. 2728\u20132732, 2022. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolution-augmented trans- former for speech recognition,\u201d Proc. Interspeech 2020, pp. 5036\u2013 5040, 2020. [17] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture lo- cal and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 17627\u201317643. [18] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Srid- har, Kyu J Han, and Shinji Watanabe, \u201cE-Branchformer: Branch- former with enhanced merging for speech recognition,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391. [19] Jeffrey Josanne Michael, Nagendra Kumar Goel, Jonas Robert- \u201cComparison of svd and fac- arXiv preprint son, Shravan Mishra, et al., torized tdnn approaches for speech to text,\u201d arXiv:2110.07027, 2021. [20] Yuekai Zhang, Sining Sun, and Long Ma, \u201cTiny transducer: A highly-efficient speech recognition model on edge devices,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6024\u2013 6028. [21] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d Advances in neural information processing systems, vol. 27, 2014. [22] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., \u201cEspnet-slu: Advancing spo- ken language understanding through espnet,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7167\u20137171. [23] Andros Tjandra, Sakriani Sakti, and Satoshi"}