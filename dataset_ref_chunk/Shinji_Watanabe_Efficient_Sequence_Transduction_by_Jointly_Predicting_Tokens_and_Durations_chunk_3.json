{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Efficient_Sequence_Transduction_by_Jointly_Predicting_Tokens_and_Durations_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the Greedy Inference of Conventional Transducer algorithm?,answer: To decode an acoustic input by iteratively generating tokens based on the encoder and decoder outputs.", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " How is the gradient with respect to the duration probabilities PD(d|t, u) defined?,answer: It is defined as \u2212 \u03b1(t, u)c(d, t, u) P (y|x).", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " What method is used in TDT models to under-normalize the logits?,answer: The pseudo \u201cprobability\u201d P \u2032 T (v|t, u) is used in its forward and backward computation.", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " Explain the purpose of Transducer function merging.,answer: It is used to compute the gradient of the Transducer loss with respect to the pre-softmax logits directly.", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " Why is under-normalization only used in training for TDT models?,answer: It encourages TDT models to prioritize emissions of any token with longer durations.", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " What is the key difference in the TDT Inference algorithm compared to the conventional Transducer algorithm?,answer: TDT Inference can increment t by more than one based on the duration output, while conventional Transducer could only increment t by 1 at a time for blank emissions.", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " How is log PT (v|t, u) computed in TDT models?,answer: It is computed as log softmaxv\u2032(hv\u2032 (t, u)).", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " What is the purpose of the joint operation in the Greedy Inference of TDT Models algorithm?,answer: To compute the probability distribution over both tokens and durations.", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " What is the relative speed-up of TDT models compared to RNNT models, as shown in Table 1?,answer: The relative speed-up ranges from 1.43X to 2.19X.", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}, {"question": " What toolkit is used for all experiments involving TDT models?,answer: The NeMo toolkit (Kuchaiev et al., 2019) is used for all experiments.", "ref_chunk": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}], "doc_text": "u)PD(d|t, u), d\u2208D\\{0} 0, v = yu+1 v = \u00d8 otherwise. (11) Note, b(v, t, u) can be interpreted as a weighted sum of \u03b2\u2019s that are reachable from (t, u), where the weights are from the duration probabilities. The second part is the gradient with respect to the duration probabilities PD(d|t, u): Algorithm 1 Greedy Inference of Conventional Transducer 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined) 8: if token is not blank then 9: 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) else t += 1 end if \u2202LTDT \u2202PD(d|t, u) = \u2212 \u03b1(t, u)c(d, t, u) P (y|x) (12) where c(d, t, u) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 \u03b2(t, u + 1)PT (yu+1|t, u), \u03b2(t + d, u + 1)PT (yu+1|t, u) +\u03b2(t + d, u)PT (\u00d8|t, u), d = 0 d > 0. The TDT model uses the pseudo \u201cprobability\u201d P \u2032 T (v|t, u) in its forward and backward computation, which under- normalize the logits in the following way: (13) T (v|t, u) = log softmaxv\u2032(hv\u2032 log P \u2032 (t, u)) \u2212 \u03c3. 3.2. Gradient with Transducer Function Merging The PT (v|t, u) terms in the Transducer loss are usually com- puted with a softmax function. Thus the gradients of the TDT loss have to go through the gradient of the softmax function to be passed to the previous layers, which could be costly. We use Transducer function merging proposed in (Li et al., 2019) to directly compute the gradient of the Trans- ducer loss with respect to the pre-softmax logits (hv(t, u)): \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v, t, u) PTDT(y|x) (14) where b(v, t, u) is defined in Eq. 11. Note we apply function merging only to the token logits, not duration logits since the latter usually has very small dimensions, and the negli- gible efficiency improvements do not outweigh the added complexity in implementation. (cid:105) The under-normalization is only used in training, which encourages TDT models to prioritize emissions of any token (blank or non-blank) with longer durations. The gradients that incorporate the logit under-normalization method are shown in Eq. 17, \u2202LTDT(y|x) \u2202hv(t, u) = (cid:104) PT (v|t, u)\u03b1(t, u) \u03b2(t, u) \u2212 b(v,t,u) exp(\u03c3) (cid:105) (cid:104) LTDT(y|x) exp (17) where b(v, t, u) are defined in Eq. 11.7 Note that Eq. 17 is similar to Eq. 14, with the only difference being for TDT, the b(v, t, u) term is scaled by 1 exp(\u03c3) . 3.4. TDT Inference 3.3. Logits Under-normalization We adopt the logit under-normalization method from (Xu et al., 2022) during the training of TDT models, in order to encourage longer durations. In our TDT implementa- tions, we compute PT (v|t, u) in the log domain in order to have better numerical stability. The log probabilities log PT (v|t, u) are computed from the logits hv(t, u) corre- sponding to token v: We compare the inference algorithms of conventional Trans- ducer models (Algorithm 1) and TDT models (Algorithm 2), which fully utilize the duration output. Note, that for TDT, an additional distribution over durations is computed from the joiner (line 9). This duration can increment t by more than one (line 13), compared with line 12 of conventional Transducer algorithm, where t could only be incremented by 1 at a time, and this only happens for blank emissions. This is the key place that makes TDT inference faster. log PT (v|t, u) = log softmaxv\u2032(hv\u2032 (t, u)). (15) 7PT (.) in Eq. 17, represents \u201creal\u201d probability, while the loss function L is computed with pseudo-probabilities. 4 (16) (cid:105) Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Algorithm 2 Greedy Inference of TDT Models TDT config WER(%) time(s) rel. speed-up 1: input: acoustic input x 2: enc = encoder(x) 3: hyp = [] 4: t = 0 5: while t < len(enc) do dec = decoder(hyp) 6: joined = joint(enc[t], dec) 7: idx = argmax(joined[:vocab size]) 8: duration idx = argmax(joined[vocab size:]) 9: if token is not blank then 10: 11: 12: 13: 14: end while 15: return hyp hyp.append(idx2token[idx]) end if t += duration idx2duration[duration idx] RNNT 0-2 0-4 0-6 0-8 2.14 2.35 2.17 2.14 2.11 256 175 129 119 117 1.46X 1.98X 2.15X 2.19X Table 1. English ASR, Librispeech test-clean. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. TDT config WER(%) time(s) rel. speed-up RNNT 0-2 0-4 0-6 0-8 5.11 5.50 5.06 5.05 5.16 244 171 128 118 115 1.43X 1.91X 2.07X 2.12X 4. Experiments Table 2. English ASR, Librispeech test-other. TDT vs RNNT: WER, decoding time, and relative speed-up against the RNNT. We evaluate our model in three different tasks: speech recog- nition, speech translation, and spoken language understand- ing. We use the NeMo (Kuchaiev et al., 2019) toolkit for all experiments. Unless specified otherwise, we use Conformer- Large for all tasks. 8 For acoustic feature extraction, we use audio frames of 10 ms and window sizes of 25 ms. Our model has a conformer encoder with 17 layers with num- heads = 8, and relative position embeddings. The hidden dimension of all the conformer layers is set to 512, and for the feed-forward layers in the conformer, an expansion factor of 4 is used. The convolution layers use a kernel size of 31. At the beginning of the encoder, convolution-based subs-ampling is performed with subsampling rate 4. All models have around 120M parameters, The exact number of parameters may vary, depending on the size of the sub- word vocabulary and durations used with TDT models. We use different subword-based tokenizers for different models, which will be described in their respective sections. Un- less specified otherwise, logit under-normalization is used during training with \u03c3 = 0.05. For all experiments, we train our models for no more than 200 epochs, and run checkpoint-averaging performed on 5 checkpoints"}