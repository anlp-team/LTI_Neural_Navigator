{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Self-Refine:_Iterative_Refinement_with_Self-Feedback_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the difference between actionable feedback and generic feedback discussed in the text?", "answer": " Actionable feedback pinpoints an issue and suggests a clear improvement, whereas generic feedback lacks precision and direction.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " How does the performance change in the Code Optimization task when transitioning from SELF-REFINE feedback to generic feedback?", "answer": " The performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback).", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " What is the impact of feedback on the Sentiment Transfer task performance?", "answer": " Changing from specific feedback to generic feedback leads to a significant performance drop from 43.2 to 31.2.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " In the Acronym Generation task, how does the performance change without actionable feedback?", "answer": " Performance drops from 56.4 to 48.0 without actionable feedback.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " According to the text, what is the importance of multiple iterations of FEEDBACK-REFINE?", "answer": " The quality of the output improves as the number of iterations increases, leading to enhanced performance.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " How does the output score in Code Optimization change after three iterations of refinement?", "answer": " The score improves from 22.0 (initial) to 28.8 after three iterations.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " What trend is observed in the improvement of output quality with more iterations in Constrained Generation?", "answer": " The score increases from 29.0 to 49.7 after three iterations, highlighting the trend of improvement.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " What does Figure 4 demonstrate regarding the improvement in output quality with iterations?", "answer": " Early iterations significantly improve output quality, with scores generally continuing to improve with more iterations.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " How does SELF-REFINE enhance the output quality compared to just generating multiple outputs without refinement?", "answer": " Despite the increased difficulty, outputs of SELF-REFINE are preferred by humans over all initial outputs, showing the importance of refinement over just generation.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}, {"question": " Does SELF-REFINE work effectively with weaker models according to the experiments discussed?", "answer": " SELF-REFINE struggles with smaller or weaker models as they often fail to adhere to prompts for refinement, rendering the outputs less effective.", "ref_chunk": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}], "doc_text": "provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback\u2019s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance \u2013 specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6 Task Code Opt. Sentiment Rev. Constrained Gen. y0 22.0 33.9 29.0 y1 27.0 34.9 40.3 y2 27.9 36.1 46.7 y3 28.8 36.8 49.7 10 5 5 11.3 1 0.9 6.4 1.2 0.9 C. Opt. C. Gen. S. Rev. 3 0.7 0 \u2206(y0\u2192y1) \u2206(y1\u2192y2) \u2206(y2\u2192y3) Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380 (cid:44)\u2192 # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): (cid:44)\u2192 1): if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - (cid:44)\u2192 coins[i]] + prices[i]) return dp[amount] return best_price Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount \u2217 coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a 7 less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code"}