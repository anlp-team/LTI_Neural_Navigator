{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Making_Scalable_Meta_Learning_Practical_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the primary focus of the meta learning research mentioned in the text?", "answer": " The primary focus of the meta learning research mentioned in the text is on communication cost optimization and efficient distributed training.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " How does SAMA enable efficient Distributed Data Parallel (DDP)?", "answer": " SAMA enables efficient DDP by developing a communication strategy that performs the first two backward passes locally on each device, and then overlaps computation in the final backward pass with communication.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " What is the significance of the workflow diagram of SAMA presented in the text?", "answer": " The workflow diagram of SAMA helps to visualize and understand the communication optimization strategy implemented in the framework.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " What is the application of meta learning focused on in the text other than few-shot learning?", "answer": " The application of meta learning focused on in the text, other than few-shot learning, is data optimization.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " How does weak supervision aim to reduce data labeling costs?", "answer": " Weak supervision aims to reduce data labeling costs by generating labels for a large amount of data using multiple weak labeling functions, such as hand-designed rules and other neural networks.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " What data optimization operations are utilized in the text for weak supervision?", "answer": " Data reweighting and label correction are utilized as data optimization operations for weak supervision in the text.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " What is the bilevel optimization formulation mentioned in the text for optimizing noisy training data?", "answer": " The bilevel optimization formulation is \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc))", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " What benchmark datasets are used to evaluate the scaling efficiency of SAMA in text classification?", "answer": " The WRENCH benchmark datasets are used to evaluate the scaling efficiency of SAMA in text classification.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " What observations were made from the experiments comparing SAMA-based data reweighting and label correction?", "answer": " The experiments showed noticeable improvements in noisy text classification accuracy of the large Transformer model with the help of small additional clean data in the meta level. SAMA performed better compared to SAMA-NA, emphasizing the importance of algorithmic adaptation when an adaptive optimizer is used.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}, {"question": " How does SAMA compare to other implicit-differentiation-based meta learning algorithms in terms of compute/memory efficiency?", "answer": " SAMA is compared to other implicit-differentiation-based meta learning algorithms, namely Neumann Series and conjugate gradient, in terms of GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset to evaluate compute/memory efficiency.", "ref_chunk": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}], "doc_text": "calculated analytically without backpropagation. 5 libraries, such as Betty [6], have preliminary DDP support, they do not offer further communication cost optimization. As a result, meta learning research to date has either been limited to a single-GPU setup or suffered from communication inefficiency. Solution Since we avoid any explicit computations of second-order gradient information in SAMA, we can utilize various efficient distributed training tricks that have been implemented for first-order gradients. More specifically, to enable efficient DDP in SAMA, we develop a novel communication strategy that performs first two backward passes locally on each device, and then overlaps com- putation in the final backward pass with communication. In PyTorch, this can be neatly achieved by implementing the first two backward passes with torch.autograd.grad and the last one with torch.autograd.backward. The overall workflow diagram of SAMA is presented in Figure 2. To facilitate research in scalable meta learning, we provide our implementation of SAMA with the above communication optimization in Betty3 that only requires a one-line change in the configuration. 4 Experiments While few-shot learning has traditionally been the most popular application of meta learning, most recent large models such as GPT [5], ViT [13], and Whisper [50], provide few-shot generalization capability out of the box. Therefore, we in this work focus on another promising application of meta learning, data optimization, where we transform (e.g., reweight, label correct) downstream/pretraining data given the specific objectives in the meta optimization problem. Indeed, there is an increasing number of works originating in data-centric AI that empirically show that the quality of training data significantly affects the final performance of large models [1, 5, 18, 50, 56, 65]. Nevertheless, solutions proposed in these works to improve training data quality mostly rely on hand-designed heuristics, which typically result in suboptimal performance. Given that training data of large models serves as extremely high-dimensional inductive biases in machine learning, we expect that GBML\u2019s ability to efficiently optimize high-dimensional inductive biases can be fully unlocked in the data optimization application. From a technical perspective, large-scale data optimization has a substantial compute/memory cost and frequently involves models that are trained with adaptive optimizers. Therefore, it serves as an ideal benchmark to (a) evaluate the scalability of SAMA compared to existing approaches, (b) study the effectiveness of each component in SAMA, and (c) investigate the practical usefulness of scalable meta learning across diverse domains. Specifically, in this section, we consider three data optimization applications, namely: Noisy finetuning of large language models (Sec. 4.1), continued pretraining of large language models (Sec. 4.2), and scale-agnostic data pruning (Sec. 4.3). While not an application of data optimization, we also include a preliminary analysis on the effect of model size on few-shot image classification accuracy in Appendix D. Finally, we note that all experiment details including baselines, hyperparameters, and compute resources are provided in Appendix B. 4.1 Noisy Finetuning of Large Language Models Weak supervision [53] proposes to achieve a significant reduction in the data labeling cost by letting users quickly generate labels for a large amount of data by exploiting multiple weak labeling functions, such as hand-designed rules and other neural networks. While increasing the quantity of labeled data with weak supervision has led to noticeable improvements in multiple applications, a poor quality of generated labels results in a degradation in test accuracy, leaving room for further improvement. Here, we attempt to alleviate this data quality issue in weak supervision by utilizing meta learning to automatically optimize noisy training data guided by a small amount of clean data in the meta level. In detail, we use data reweighting [58] and label correction [70] as our data optimization operations, of which the bilevel optimization formulation is as follows: \u03bb\u2217 = (\u03bb\u2217 r, \u03bb\u2217 c ) = argmin L(Dclean; \u03b8\u2217(\u03bbr, \u03bbc)) \u03bbr ,\u03bbc s.t. \u03b8\u2217(\u03bbr, \u03bbc) = argmin \u03b8 1 |Dnoisy| (cid:88) (x,y)\u2208Dnoisy w(L; \u03bbr) \u00b7 L(f (x; \u03b8), c(x, y; \u03bbc)) where w(\u00b7; \u03bbr) and c(\u00b7; \u03bbc) are meta learners, respectively, for data reweighting and label correction. To evaluate the scaling efficiency of SAMA, we perform text classification with a BERT-base model 3https://github.com/leopard-ai/betty 6 with 110M parameters on multiple weak supervision datasets from the WRENCH benchmark [67]. Furthermore, to study the effectiveness of our algorithmic adaptation strategy (Sec. 3.2), we conduct experiments with a variant of SAMA (i.e., SAMA-NA) that does not include algorithmic adaptation, and present the experiment results in Table 1. Algorithm TREC SemEval IMDB ChemProt AGNews Yelp Finetune (orig) [67] COSINE [66] - 66.56 (2.31) 76.56 (0.08) 83.93 (1.74) 86.80 (0.46) 79.73 (2.60) 82.98 (0.05) 56.09 (1.08) 58.47 (0.08) 86.27 (0.53) 87.03 (0.00) 82.26 (3.50) 89.22 (0.05) Finetune (ours) 67.93 (2.55) 79.28 (1.78) 78.16 (2.28) 57.35 (1.43) 85.79 (0.49) 84.32 (2.55) +R +R & C SAMA-NA SAMA-NA 74.33 (2.34) 79.00 (2.62) 87.11 (1.11) 87.67 (1.36) 81.92 (1.74) 80.44 (0.97) 60.88 (0.60) 64.05 (0.52) 86.83 (0.19) 87.05 (0.39) 80.96 (3.04) 80.73 (3.11) +R +R & C SAMA SAMA 85.73 (0.81) 87.93 (1.17) 89.67 (0.67) 88.83 (2.32) 84.31 (1.86) 85.71 (0.82) 76.89 (1.39) 77.78 (0.59) 89.05 (0.34) 89.79 (0.27) 93.64 (0.40) 93.77 (0.08) Table 1: WRENCH results. R and C in the first column stand for data reweighting and label correction operations. The number in parentheses indicates standard deviation for each experiment over 3 runs. In this experiment, we are able to make two observations. First, we notice that SAMA-based data reweighting and label correction both lead to noticeable improvements over finetuning and self- training (COSINE [66]) baselines in noisy text classification accuracy of the large Transformer model across all benchmarks with the help of small additional clean data Dclean in the meta level. Second, given the superior performance of SAMA compared to SAMA-NA, we empirically verify the importance of algorithmic adaptation when an adaptive optimizer is used to train this Transformer base learner. Additionally, we compare compute/memory efficiency of SAMA to that of two other implicit-differentiation-based meta learning algorithms, namely Neumann Series [40] and conjugate gradient [51]. For fair comparison, we evaluate GPU memory usage (MB) and throughput (samples/sec) on the AGNews dataset"}