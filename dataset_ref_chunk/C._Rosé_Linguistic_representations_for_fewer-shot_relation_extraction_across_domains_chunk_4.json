{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/C._Rose\u0301_Linguistic_representations_for_fewer-shot_relation_extraction_across_domains_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of aligning MSCorpus with other datasets by combining some labels?,        answer: The purpose is to bring MSCorpus into alignment with the other annotation schemas.    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " What model is used as the base BERT model for both the baseline and graph-aware variants?,        answer: bert-base-uncased    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " How many random seeds is each model trained on in the experiments?,        answer: 3 random seeds    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " What optimization algorithm is used to train each model?,        answer: Adam optimizer    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " What is the learning rate used in the training process?,        answer: 2 \u00d7 10^-5    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " How is the best model selected during training?,        answer: Based on the macro-averaged F1 score on the dev split with a patience of 5 epochs.    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " What are the graph-aware models referred to as when they add dependencies and AMRs?,        answer: +Dep and +AMR, respectively    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " What is the Few-shot transfer learning formulated as?,        answer: An N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain.    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " Why is the label imbalance in the datasets important when dealing with few-shot transfer learning?,        answer: It affects the sampling process, potentially resulting in fewer than K examples for a given class.    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " What do the few-shot models train on in each domain from scratch?,        answer: For each of the settings K described above, using the same settings.    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}, {"question": " How was the effect of introducing linguistic formalisms tested?,        answer: With an ANOVA model with multiple independent variables.    ", "ref_chunk": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}], "doc_text": "and one relation indicating the flow of operations. MSCorpus de- fines a rich set of relations between entites, which is atypical for the other datasets. We thus com- bine some of these labels to bring MSCorpus into alignment with the other annotation schemas. 5 Experiments 5.1 In-Domain Experiments We train both the baseline and graph-aware mod- els on each dataset, using the train/dev/test splits where provided. If no dev split was provided, we randomly split the training dataset 80/20 into new train and dev splits. We use bert-base-uncased as available on the Huggingface Hub 4 as our base BERT model, for both the baseline and graph- aware variants. For our graph-aware variants, we use R-GCN as our graph network. We train each model with the Adam optimizer (Kingma and Ba, 2014) to minimize the cross-entropy loss between predicted and true labels. We use a learning rate of 2 \u00d7 10\u22125 and a batch size of 16. Each model is trained on 3 random seeds for 30 epochs, us- ing early stopping criterion based on the macro- averaged F1 score on the dev split with a patience of 5 epochs. We keep the model that performs best on the dev split, and calculate its correspond- ing macro F1 score on the test set. We refer to the graph aware models that add dependencies and AMRs as +Dep and +AMR, respectively. 5.2 Few-shot Experiments We formulate few-shot transfer learning as an N - way K-shot problem, where a model is trained on K instances of each of the N classes in the target domain. We experiment with K \u2208 {1, 5, 10, 20, 50, 100}. Because of the label im- balance in our datasets, where K is greater than the number of labeled examples for a given class, we sample all of the labeled instances without replace- ment. This can result in fewer than K examples for a given class. For the transfer process, we begin with the mod- els trained in the in-domain experiments, and re- place the MLP classification head with a freshly initialized head with a suitable number of outputs for the target domain\u2019s number of classes. We reuse the BERT and R-GCN components of the in-domain model, and allow their weights to be updated in the transfer finetuning. We continue to train each model using the same settings as in-domain training using a batch size of 4, sampling each dataset three times with different seeds. In addition, to control for the effects of the source and target dataset interactions and our sam- pling strategies, we train few-shot models in each domain from scratch, for each of the settings K described above, using the same settings. All of our experiments were run on NVIDIA A4500 GPUs, and we used roughly 33 days of GPU time for all of the experiments in this project, including hyperparameter tuning. 4https://huggingface.co/bert-base-uncased 6 Results and Discussion We expect that more powerful linguistic represen- tations than plain text will aid in few shot transfer between domains. In order for few shot transfer to be successful, the target data points used for transfer need to increase the relevant shared rep- resentation between the source and target datasets. Because of this, we expect that any effect of repre- sentation on test set performance will depend upon how much shared representation there was between the two domains to begin with and how much the few added examples closes the gap. A more effi- cient representation may lose its advantage once there are enough target domain examples to obviate the need for efficiency. In this section, we aim to answer a number of questions. Dataset Case Mean (std) EFGC +AMR +Dep Baseline MSCorpus +AMR RISeC +Dep Baseline +AMR +Dep Baseline 83.9 (0.3) 84.6 (1.3) 85.0 (0.8) 87.8 (1.0) 88.4 (0.5) 87.5 (0.5) 82.8 (1.6) 81.7 (2.1) 82.7 (1.3) Table 2: Results from in-domain experiments. Each value represents the mean of runs with three random seeds, with standard deviation in parentheses. Do linguistic representation aid in either in- domain or cross-domain transfer? We present our in-domain results on the complete datasets in table 2. Overall, we do not see significant differ- ences between the baseline and +Dep and +AMR cases, even though they appear to overperform the baseline case on RISeC and MSCorpus. Notably, however, these models do not overfit more than the baseline: performance on the unseen test set remains similar. We do, however, see differences in performance between the baseline and graph-aware cases in the few-shot transfer setting. In Figure 2, we visual- ize the difference between the macro-averaged F1 performance in each of our graph-aware cases and the baseline against the few-shot setting. We see that while in the 1-shot case, our results are highly variable, the 5-, 10-, and 20- shot cases yield notice- able improvement, peaking in the 5- and 10-shot settings. In our best-performing results, we see a 6-point absolute gain in F1 score. We find that both dependency parse and AMR representations show a statistically significant posi- tive effect on performance. In particular, we test the significance of the effect with an ANOVA model with multiple independent variables: namely, source and target dataset (EFGC, RISeC, MSCor- pus), representation case (Baseline, +Dep, +AMR), few-shot setting (1, 5, 10, 20, 50, 100), and transfer setting (in-domain vs out-of-domain). The depen- dent variable is test set F1. The data table for the analysis includes 3 runs for each combination of variables each with a separate random seed. We train our models under a full-factorial experimental design, i.e. we ran trials for all combinations of variables. This design allows us to test the reliabil- ity of the effect of our variables under a variety of conditions while making the necessary statistical adjustments to avoid spurious significant effects that may occur when multiple statistical compar- isons are made. We use this design rather than pairwise significance tests so that we can measure the effect of introducing linguistic formalisms as a whole, rather than arguing the statistical signifi- cance"}