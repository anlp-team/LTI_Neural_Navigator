{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Large_Language_Models_Enable_Few-Shot_Clustering_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two datasets experimented with in the text?", "answer": " OPIEC59k and ReVerb45k", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " How many noun phrases are in the OPIEC59k dataset?", "answer": " 22,000", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " How many unique entity surface forms are in the OPIEC59k dataset?", "answer": " 2,138", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " What is the ground truth source for the clusters in the OPIEC59k dataset?", "answer": " Anchor texts from Wikipedia", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " What is the output of the ReVerb system in the ReVerb45k dataset?", "answer": " Noun phrases", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " How many mentions are in the ReVerb45k dataset?", "answer": " 15,500", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " How many unique entity surface forms are in the ReVerb45k dataset?", "answer": " 12,295", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " What do the ground-truth entity clusters in ReVerb45k come from?", "answer": " Automatically linking entities to the Freebase knowledge graph", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " What are the standard metrics used by Shen et al. (2022) in the text?", "answer": " Macro Precision and Recall, Micro Precision and Recall, Pairwise Precision and Recall", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}, {"question": " What is computed to obtain Macro F1, Micro F1, and Pairwise F1?", "answer": " The harmonic mean of precision and recall for each pair", "ref_chunk": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}], "doc_text": "few points per cluster, making this a difficult clustering task that requires lots of human feedback to be effective. Datasets. We experiment with two datasets: OPIEC59k (Shen et al., 2022) contains 22K noun phrases (with 2138 unique entity surface forms) belonging to 490 ground truth clusters. The noun phrases are extracted by MinIE (Gash- teovski et al., 2017, 2019), and the ground truth entity clusters are anchor texts from Wikipedia that link to the same Wikipedia article. ReVerb45k (Vashishth et al., 2018) contains 15.5K mentions (with 12295 unique entity sur- face forms) belonging to 6700 ground truth clus- ters. The noun phrases are the output of the ReVerb (Fader et al., 2011) system, and the \u201cground-truth\u201d entity clusters come from auto- matically linking entities to the Freebase knowl- edge graph. We use the version of this dataset from Shen et al. (2022), who manually removed samples containing labeling errors. Canonicalization Metrics. We follow the stan- dard metrics used by Shen et al. (2022): Macro Precision and Recall \u2013 Prec: For what fraction of predicted clusters is every element in the same gold cluster? \u2013 Rec: For what fraction of gold clusters is every element in the same predicted cluster? Micro Precision and Recall \u2013 Prec: How many points are in the same gold cluster as the majority of their predicted clus- ter? \u2013 Rec: How many points are in the same pre- dicted cluster as the majority of their gold cluster? Pairwise Precision and Recall \u2013 Prec: How many pairs of points predicted to be linked are truly linked by a gold cluster? \u2013 Rec: How many pairs of points linked by a gold cluster are also predicted to be linked? We finally compute the harmonic mean of each pair to obtain Macro F1, Micro F1, and Pairwise F1. Fact View has team in \u201cThe Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals .\u201d United States is highest judicial body in Context View TransE UC Berkeley ruled against Supreme Court NCAA NCAA BERT Figure 4: Using the CMVC architecture, we encode a knowledge graph-based \u201cfact view\u201d and a text-based \u201ccontext-view\u201d to represent each entity. 3.2 Text Clustering Task. We then consider the case of clustering short textual documents. This clustering task has been extensively studied in the literature (Aggarwal and Zhai, 2012). Datasets. We use three datasets in this setting: \u2022 Bank77 (Casanueva et al., 2020) contains 3,080 user queries for an online banking assistant from 77 intent categories. CLINC (Larson et al., 2019) contains 4,500 user queries for a task-oriented dialog system from 150 intent categories, after removing \u201cout-of- scope\u201d queries (as in (Zhang et al., 2023). \u2022 Tweet (Yin and Wang, 2016) contains 2,472 tweets from 89 categories. Metrics. Following prior work (Zhang et al., 2021), we compare our text clusters to the ground truth using normalized mutual information and ac- curacy (obtained by finding the best alignment be- tween ground truth and predicted clusters using the Hungarian algorithm (Kuhn, 1955)). 4 Baselines 4.1 K-Means on Embeddings We build our methods on top of a baseline of K-means clustering (Lloyd, 1982) over encoded data with k-means++ cluster initialization (Arthur and Vassilvitskii, 2007). We choose the features and number of cluster centers that we use by task, largely following previous work. Entity Canonicalization Following prior work (Vashishth et al., 2018; Shen et al., 2022), we clus- ter individual entity mentions (e.g. \u201cever since the ancient Greeks founded the city of Marseille in 600 BC.\u201d) by representing unique surface forms (e.g. \u201cMarseille\u201d) globally, irrespective of their particular mention context. After clustering unique surface forms, we compose this cluster mapping onto the individual mentions (extracted from individual sen- tences) to obtain mention-level clusters. We build off of the \u201cmulti-view clustering\u201d ap- proach of Shen et al. (2022), and represent each noun phrase using textual mentions from the In- ternet and the \u201copen\u201d knowledge graph extracted from an OIE system, as shown in Figure 4. They use a BERT encoder (Devlin et al., 2019) to rep- resent the textual context where an entity occurs (called the \u201ccontext view\u201d), and a TransE knowl- edge graph encoder (Bordes et al., 2013) to repre- sent nodes in the open knowledge graph (called the \u201cfact view\u201d). They improve these encoders by fine- tuning the BERT encoder using weak supervision of coreferent entities and improving the knowledge graph representations using data augmentation on the knowledge graph. These two views of each en- tity are then combined to produce a representation. In their original paper, they propose an alter- nating multi-view K-Means procedure where clus- ter assignments that are computed in one view are used to initialize cluster centroids in the other view. After a certain number of iterations, if the per-view clusterings do not agree, they perform a \u201cconflict resolution\u201d procedure to find a final clus- tering with low inertia in both views. One of our secondary contributions is a simplification of this algorithm. We find that by simply using their fine- tuned encoders, concatenating the representations from each view, and performing K-Means clus- tering with K-Means++ initialization (Arthur and Vassilvitskii, 2007) in a shared vector space, we can match their reported performance. Finally, regarding the number of cluster cen- ters, following the Log-Jump method of Shen et al. (2022), we choose 490 and 6687 clusters for OPIEC59k and ReVerb45k, respectively. Intent Clustering For the Bank77 and CLINC datasets, we follow Zhang et al. (2023) and encode each user query using the Instructor encoder. We use a simple prompt to guide the encoder: \u201cRep- resent utterances for intent classification\u201d. Again following previous work, we choose 150 and 77 clusters for CLINC and Bank77, respectively. Tweet Clustering Following Zhang et al. (2021), we encode each tweet using a version of Distil- BERT (Sanh et al., 2019) finetuned for sentence similarity classification4 (Reimers and Gurevych, 2019). We use 89 clusters (Zhang et al., 2021). 4.2 Clustering via Contrastive Learning In addition to the methods described in Section 2, we also"}