{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Learning_to_Ask_Questions_for_Zero-shot_Dialogue_State_Tracking_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the method presented in the text for zero-shot Dialogue State Tracking (DST)?,answer: Learning to ask questions as a framework to pair the best question generation strategy with question answering methods.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " Why is a novel self-supervised QA pretraining step using in-domain data considered essential in the presented method?,answer: To learn the structure without requiring any slot-filling annotations.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " According to the text, what is one requirement for QG methods to align with in a dialogue?,answer: The grammatical person used.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " What was the main finding of the empirical evaluation on the MultiWOZ 2.1 dataset regarding the presented approach?,answer: Outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " What does slot-filling refer to in the context of a dialogue?,answer: Assigning values to the set of slot keys in an utterance.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " What kind of systems should adapt to new functionalities without needing to be fully rebuilt, according to the text?,answer: Task-oriented dialogue (TOD) systems.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " What do models with zero-shot capabilities need to be robust to, as mentioned in the text?,answer: Not only to the addition of new slots, but also entirely new domains.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " What is the ultimate goal of an ideal zero-shot system mentioned in the text?,answer: No human effort is needed to bootstrap or enable zero-shot capabilities.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " What are the two self-supervised approaches proposed in the text to bootstrap DST models?,answer: Pretraining with in-domain and out-of-domain QA data, and emphasizing relationships between semantically similar slots.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}, {"question": " What are the twofold contributions presented in the text regarding DST models?,answer: A self-supervised approach to pretrain DST models and a question generation strategy to maximize zero-shot DST performance.", "ref_chunk": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}], "doc_text": "Learning to Ask Questions for Zero-shot Dialogue State Tracking Diogo Tavares NOVA University of Lisbon NOVA LINCS Lisbon, Portugal dc.tavares@campus.fct.unl.pt David Semedo NOVA University of Lisbon NOVA LINCS Lisbon, Portugal df.semedo@fct.unl.pt Alexander Rudnicky Carnegie Mellon University LTI Pittsburgh, PA, USA air@cs.cmu.edu Joao Magalhaes NOVA University of Lisbon NOVA LINCS Lisbon, Portugal jmag@fct.unl.pt ABSTRACT We present a method for performing zero-shot Dialogue State Track- ing (DST) by casting the task as a learning-to-ask-questions frame- work. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling an- notations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation\u2014given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation. Knowing which important slots have and have not been filled al- lows a dialogue system to guide the conversation to quickly gather the information needed to complete the task at hand. We refer to assigning values to the set of slot keys in an utterance as slot-filling, and the task of maintaining this state throughout an entire dialogue as Dialogue State Tracking (DST). A TOD system deployed in the wild should adapt to new functionalities without needing to be fully rebuilt. However, to train the system on the new informa- tion, it needs dialogue data and DST annotations. Circumventing this problem requires models with zero-shot capabilities; specif- ically, models which are robust not only to the addition of new slots, but also entirely new domains. When early open-vocabulary DST approaches [13, 23] tackled this task, they were faced with a typical difficulty of the zero-shot learning setting: how to best employ external textual knowledge, such as slot keys, to maximize performance in unseen domains [3]. More recent work [16, 26] uses textual descriptions of the domains and slots, which, though richer, are also noisier [3], and require human intervention to be created when the domain is novel. In an ideal zero-shot system, no human effort is needed to bootstrap\u2014or enable\u2014zero-shot capabilities. CCS CONCEPTS \u2022 Information systems \u2192 Question answering; \u2022 Computing methodologies \u2192 Natural language processing. KEYWORDS dialogue state tracking, zero-shot, question answering ACM Reference Format: Diogo Tavares, David Semedo, Alexander Rudnicky, and Joao Magalhaes. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201923), July 23\u201327, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592010 1 INTRODUCTION Task-oriented dialogue (TOD) systems interact with a user using natural language to achieve a specific goal, such as booking airline tickets or making a restaurant reservation. The state of a dialogue is defined by a set of (key, value) pairs which represent the informa- tion requested or accepted by the user, up to the current utterance. This work is licensed under a Creative Commons Attribution International 4.0 License. To tackle these challenges, we begin by casting the DST task as a reading comprehension problem, allowing for explicit zero-shot DST support. As each slot will be directly linked to a question, we also propose ways to generate the set of questions pertaining to each slot. We propose a novel method which emphasizes the relationships between semantically similar slots, which are likely to share values. Lastly, we propose two self-supervised approaches to bootstrap DST models as QA, which address two typical situations: when in-domain dialogues exist, but no annotations are present, and when no in-domain dialogues exist. While we loosely follow the the approach presented by Honovich et al. [11], as far as we know, we are the first to use question generation techniques for pretraining in the DST task. In the latter, the larger and richer pool of data offered by the QA domain allows us to bootstrap models that can adapt to novel zero-shot classes. Our contributions are twofold: first, we propose a self-supervised approach to pretrain DST models with in-domain and out-of-domain QA data, as a way to bootstrap DST models (Section 3.1). Second, we present a question generation strategy which emphasizes the relationships between semantically similar slots\u2014slots which refer to similar types of values\u2014as a way to maximize zero-shot DST performance (Section 3.2). 1 SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9408-6/23/07. https://doi.org/10.1145/3539618.3592010 1We release all sets of questions and prompts in https://github.com/d-c-t/learning-to- ask-questions-zero-shot-dst. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Diogo Tavares, David Semedo, Alexander Rudnicky, & Joao Magalhaes Di+1 = (u0, u1, \u2026, (\u201ci need a train from birmingham to cambridge this wednesday\u201d, \u201cwe found 17 trains like that. what\u2019s the best time?\u201d)) A: wednesdayA: 1A: a table (NER + nounphrases) - stop_words Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? {\u20181\u2019, \u2018a table\u2019} {\u2018wednesday\u2019, \u20181\u2019, \u2018a table\u2019} A: [none]A: [none] Q: On what day would you like to book a table for 1?Q: How many people would you like to have a table for?Q: What would you like to book for 1 on wednesday? Di = (u0, u1, \u2026, (\u201ci\u2019d like to book a table for 1 at 17:15 on wednesday. can you do that?\u201d, \u201cabsolutely.\u201d)) Question Generation Remove all that exist in Di+1 Figure 1: Step-by-step illustration of question generation for model pretraining. We emphasize (in bold) the slot annotations in \ud835\udc62\ud835\udc56 , to highlight how they overlap with the outputs of span extraction. 2 RELATED WORK Early DST approaches [13, 23] heavily focused on not only the full- shot"}