{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Overview_of_the_TREC_2021_Fair_Ranking_Track_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What was the objective of providing equitable exposure to documents in the system?", "answer": " The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " Where can the corpus and query dataset be obtained from?", "answer": " The corpus and query data set is distributed via Globus and can be obtained via Globus from the repository link provided.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " What was the content of the corpus used in the system?", "answer": " The corpus consisted of articles from English Wikipedia with wikitext intact, provided as a JSON file, and compressed with gzip.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " How were the training topics in the system based?", "answer": " Each of the track\u2019s training topics was based on a single Wikiproject.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " Why were annotations incomplete in the system?", "answer": " Annotations were incomplete due to the large number of datasets resulting from the sequence of rankings required for the system.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " What were participants provided with for the training data in terms of fairness ground truth?", "answer": " For training data, participants were provided with a geographical fairness ground truth.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " What was the undisclosed personal demographic attribute used for evaluating the system\u2019s fairness in rankings?", "answer": " An undisclosed personal demographic attribute (gender) was used for evaluating the system\u2019s fairness in rankings.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " What was the Wikimedia quality score used for optimizing work-needed in Task 2?", "answer": " The Wikimedia quality score ranged between 0 and 1, where 0 indicated no content on the page and 1 indicated high quality.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " How were results outputted for Task 1 in the system?", "answer": " For Task 1, participants outputted results in rank order in a tab-separated file with two columns: query ID and recommended article ID.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}, {"question": " How were the tasks in the system evaluated?", "answer": " Each task was evaluated with its own metric designed for that specific task setting to measure the exposure of relevant documents.", "ref_chunk": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}], "doc_text": "the distribu- tion realized by a stochastic ranking policy (given a query q, a distribution \u03c0q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented \u2014 other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics. 3 Data This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2. 3.1 Obtaining the Data The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution\u2019s Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal.2 This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/. The runs and evaluation qrels will be made available in the ordinary TREC archives. 3.2 Corpus The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON \ufb01le, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following \ufb01elds: id The unique numeric Wikipedia article identi\ufb01er. title The article title. url The article URL, to comply with Wikipedia licensing attribution requirements. text The full article text. The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license.3 The raw Wikipedia dump \ufb01les used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps inde\ufb01nitely. 2https://www.globus.org/globus-connect-personal 3https://creativecommons.org/licenses/by-sa/3.0/ 3 3.3 Topics Each of the track\u2019s training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (\ufb01le trec topics.json.gz), with each record containing: id A query identi\ufb01er (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string) rel docs A list of the page IDs of relevant pages (list of int) The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for re\ufb01ning system queries. In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also ex- pected to return relevant documents that need more editing work done more highly than relevant documents that need less work done. 3.4 Annotations NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is di\ufb03cult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including: Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents. Some documents were not complete and did not have enough information to match with the topic. We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers: The \ufb01rst 20 items of all rankings for Task 1 (all queries). The \ufb01rst 5 items of the \ufb01rst 25 rankings from every submission to Task 2 (about 75% of the queries). Details are included with the annotations and metric code. 3.5 Metadata and Fairness Categories For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender). We also provided a simple Wikimedia quality score (a \ufb02oat between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse\u2014i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for \ufb01nal system evaluation. This data was provided together in a metadata \ufb01le (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: 4 page id Unique page identi\ufb01er (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (\ufb02oat in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string) geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string) gender For articles with a gender, the gender of the article\u2019s subject, obtained from WikiData. 3.6 Output For Task 1, participants outputted results in rank order in a tab-separated \ufb01le with two columns: id The query ID for the topic page id ID for the recommended article For Task 2, this \ufb01le had 3 columns, to account for repeated rankings per query: id Query ID rep number Repeat Number (1-100) page id ID for the recommended article 4 Evaluation Metrics Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those"}