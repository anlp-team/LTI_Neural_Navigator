{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of sequence normalization in the training phase?", "answer": " To include external language models in the training phase, leading to MMI sequence discriminative training.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " What is Optimal Completion Distillation (OCD) proposed to minimize?", "answer": " The total edit distance using an efficient dynamic programming algorithm.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " What is the objective of speech recognition?", "answer": " To minimize word error rate (WER).", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " What is the role of sequence or discriminative training in ASR models?", "answer": " To minimize the model-based expectation of the number of word errors.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " What is the loss function constructed in Minimum Word Error Rate (MWER) training aimed at minimizing?", "answer": " The expected number of word errors.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " What are some approaches for learning rate control in training schedules?", "answer": " NewBob, global versus parameter-wise learning rate control, learning rate warm-up, warm restarts/cosine annealing, weight decay, fine-tuning, etc.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " What is the basis for optimization in training E2E ASR models?", "answer": " Stochastic gradient descent, with momentum, and adaptive approaches like Adam.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " What is an example of a data perturbation method used in data augmentation for E2E ASR models?", "answer": " SpecAugment.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " What does data augmentation aim to achieve in training E2E ASR models?", "answer": " Improved performance through data perturbation.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}, {"question": " How do most data augmentation methods perform data perturbation in E2E ASR models?", "answer": " By exploiting certain dimensions of speech signal variation.", "ref_chunk": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}], "doc_text": "sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130]. that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130]. D. Pretraining All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise \u2013 they will not be further discussed here. C. Minimum Word Error Rate Training E. Training Schedules and Curricula Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows: Lmwer = N (cid:88) (cid:88) W(Cn, C \u2032 n)P (C \u2032 n|Xn) n=1 C\u2032 n where W(Cn, C \u2032 n) is the word error count in a hypothesis C \u2032 n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in \u2018beam-search\u2019 mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136]. Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170]. Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175]. F. Optimization and Regularization Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180]. Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181]. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200]. in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211]. G. Data Augmentation Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview. Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate"}