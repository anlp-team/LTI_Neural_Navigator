{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_Multi-Objective_Improvement_of_Android_Applications_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main objective of comparing the improvements found by MO-GI and SO-GI for Android apps?", "answer": " The main objective is to see if using MO algorithms limits GI\u2019s ability to improve apps compared to improving only a single objective, especially in cases where one improvement can enhance two objectives.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " Why is assessing the runtime cost of MO-GI important for Android apps?", "answer": " Assessing the runtime cost of MO-GI is important to ensure that any improvements found justify the cost of running it.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " What is the purpose of comparing GI with state-of-the-art techniques for Android performance improvement via code modification?", "answer": " The purpose is to determine if GIDroid could offer an attractive alternative to existing state-of-the-art tools available to developers.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " How many multi-objective algorithms are implemented in GIDroid for Android?", "answer": " Three multi-objective algorithms - NSGA-II, NSGA-III, and SPEA2 are implemented in GIDroid for Android.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " Why were MO algorithms chosen for the implementation in GIDroid?", "answer": " MO algorithms were chosen because sacrificing some properties may lead to better improvements in other properties, such as with the caching operators.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " What tools are used to measure execution time, memory usage, and bandwidth in the experiments?", "answer": " Execution time is measured using Linux\u2019s time tool, memory usage is tracked using Java Runtime\u2019s memory allocation tracking, and bandwidth is measured using Linux\u2019s built-in process-level network traffic tracking.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " Why was it necessary to manually construct unit tests for every benchmark in the experiments?", "answer": " It was necessary because most open-source Android applications lack test suites, and existing automated testing tools focus on UI testing rather than generating assertions crucial for capturing correct app behavior.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " What level of branch coverage was achieved for methods used in the study?", "answer": " At least 75% branch coverage was achieved for methods used in the study.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " Why do developers find the process of writing unit tests simpler compared to researchers?", "answer": " Developers find it simpler because they already have an understanding of the application and would get many benefits from writing tests.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}, {"question": " What are the benefits of writing tests mentioned in the text?", "answer": " The benefits of writing tests include understanding the application better, getting other benefits like improving code quality, and ensuring correct app behavior.", "ref_chunk": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}], "doc_text": "How do the improvements found by MO-GI compare to those found by SO-GI for Android apps? We wish to see if using MO algorithms limits GI\u2019s ability to improve apps, when compared to improving only a single objective. This is especially important in cases 13 where one improvement can enhance two objectives (e.g., deletion can improve both runtime and memory use). We want to see if MO are still competitive in such cases. RQ5: What is the runtime cost of MO-GI for Android? Any improvements found by MO-GI must be considered against the cost of running it. The improvements found must justify this cost. RQ6: How does GI compare with available state-of-the-art techniques for Android performance improvement via code modification? We want to compare GIDroid with state-of-the-art tools that are readily available to developers to see if our tool could offer an attractive alternative. 6 Methodology In order to answer our research questions, we propose a series of experiments, running both multi- and single-objective GI on a benchmark of real-world Android applications. To answer RQ1, RQ3, and RQ5, we run GI with three multi-objective algorithms on a set of applications, in some of which we know potential improvements are present, in order to validate our approach. To answer RQ2, we use the same setup to improve the latest versions of applications, to see if our framework can find yet-undiscovered optimizations Next, to answer RQ4, we run GI with a single-objective hill climbing algorithm, to compare with a multi-objective approach. With this set of experiments, we can evaluate whether or not our multi-objective algorithms can find improvements that are as good as those found by single-objective search. This allows us to compare the trade-offs found by different search algorithms. Finally, to answer RQ6, we use an Android linter to identify performance issues within our benchmarks. Linters are the only tools available to Android developers which can identify issues with source code that may affect performance properties we target. By manually repairing these issues we can see how our tool compares in terms of both effort and effectiveness with respect to existing tools available to developers. 6.1 Genetic Improvement Framework We implement our multi-objective GI approach for Android in a tool called GIDroid, and use it to answer our RQs. Although there are many existing GI frameworks, Zuo et al (2022) found that PYGGI (An et al (2019)) and the Genetic Improvement In No time tool (Gin) (Brownlee et al (2019)) were the only GI tools that could be readily applied to new software, with more recent tool by Mesecan et al (2022) not yet avail- able. However, none of the aforementioned work can be run upon Android applications. Whilst Gin is compatible with most Java programs, and thus could potentially easiest to extend, it is not compatible with the Android compilation and testing environments. In GIDroid, we implement three MO algorithms: NSGA-II (Deb et al (2000)) as it is one of the most widely used multi-objective algorithms; NSGA-III (Deb and Jain (2014)), that was specifically developed for problems with 3 or more objectives in mind; and SPEA2 (Kim et al (2004)), which has recently proven successful for MO-GI in the desktop domain (Mesecan et al (2022)). We use MO algorithms, as we believe that we will be able to find better improvements to some properties if we are able to 14 sacrifice others. In particular, with our caching operators \u2013 these operators are likely to negatively impact the memory consumption of the applications, however a small increase in memory consumption may be worth it if it can sufficiently improve another property. The parameters used in these implementations can be found in Table 2. To measure execution time we use Linux\u2019s time tool (Kerrisk (2019)), we mea- sure memory usage with the Java Runtime\u2019s memory allocation tracking (Oracle Development Team (2020)) and we use Linux\u2019s built-in process-level network traffic tracking (Kerrisk (2022)) to measure bandwidth. 6.2 Benchmarks Genetic improvement requires a set of tests that cover the areas of code being improved, in order to validate that a non-functional property-improving patch does not negatively affect the app\u2019s functionality. Unfortunately, most open-source Android applications do not have test suites, and those that do are limited, achieving a median line coverage of 23% (Pecorelli et al (2020)). Furthermore, there is not a single tool that we have found in an extensive search of the literature which can automatically generate unit tests for Android applications. All automated testing tools for Android found (Auer et al (2022); Amalfitano et al (2015); Azim and Neamtiu (2013); Baek and Bae (2016); Mahmood et al (2014); Mao et al (2016); Su et al (2017); Li et al (2017); Yasin et al (2021)) focus on testing UI via input generation in order to induce crashes and only run on devices/emulators, so would not be compatible with our framework. Moreover, they do not generate assertions \u2014 crucial for capturing correct app behaviour. This meant that we had to manually construct unit tests for every single bench- mark. We first had to attempt to understand each application and the component being improved and then attempt to create thorough, high-quality tests for them. In many cases, we had to account for asynchronous code, which was scheduled by the target code, and ensure that it executed completely during test execution. In other instances, we had to hunt down various parts of the state of the application to ensure they were correct. For each test we created, we ensured that it covered the methods which we wished to improve. We also added assertions about the state of the compo- nents of the application that were modified during execution. We achieved at least 75% branch coverage for methods used in our study. We do, however, note that developers would find this process simpler, as they already have an understanding of the appli- cation. They would get many other benefits from writing tests (Mockus et al (2009); Bach et al (2017)) so the cost cannot be only"}