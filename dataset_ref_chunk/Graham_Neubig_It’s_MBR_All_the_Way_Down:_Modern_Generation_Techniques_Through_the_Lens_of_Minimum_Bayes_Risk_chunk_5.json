{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_It\u2019s_MBR_All_the_Way_Down:_Modern_Generation_Techniques_Through_the_Lens_of_Minimum_Bayes_Risk_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of using BEER as an MBR metric?,answer: BEER works as an MBR metric for the task.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " Why did Wiher et al. (2022) find that BEER underperforms beam search?,answer: This is likely due to the different choices in hypothesis set.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " Why does autoregressive text generation suffer from length bias?,answer: Sequence probability monotonically decreases with increasing length.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " What is the purpose of importance sampling in modifying the distribution?,answer: Importance sampling is a method for estimating the expected value under a target distribution given samples from a proposal distribution.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " How is the model distribution smoothed in the length normalization strategy?,answer: The model distribution is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " What does the length reward strategy add to the score per token generated?,answer: A fixed reward \u03b3 is added to the score per token generated.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " What do the MBR results suggest about the impact of length correction on MBR performance?,answer: Length-corrected MBR underperforms vanilla MBR, potentially due to a gap between the sampling and length-correction distributions.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " Why may models trained with Minimum Error Rate Training not require length correction in beam search?,answer: Models trained with Minimum Error Rate Training do not require length correction in beam search because they are error-aware.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " How does the use of MBR differ from previous minimum-risk techniques?,answer: MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}, {"question": " What principle of Bayesian decision theory is central to risk minimization in MBR?,answer: The principle of risk minimization, where an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer.", "ref_chunk": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}], "doc_text": "and Sima\u2019an, 2014), a transla- tion metric, works as an MBR metric for this task. However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi\u00b4c and Sima\u2019an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set \u2013 Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at \u03c4 = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR. 7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1 use the implementation (Post, 2018) with from signature 5.3 Varying the risk distribution: lessons from beam search don\u2019t translate to MBR By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y\u2032). Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A. We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T \u03b2, where T is the sequence length and \u03b2 is the length penalty, a hyperparameter. A larger \u03b2 more heavily prioritizes longer sequences. sl(y|x) = s(y|x)/T \u03b2 (b) Length reward (He et al., 2016): A fixed reward \u03b3 is added to the score per token gen- erated. sl(y|x) = s(y|x) + \u03b3T The length-corrected distribution is then pl(y|x) \u221d exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y\u2032) = Ey\u223cpl[L(y, y\u2032)], given samples drawn from the model distribution p(y|x). We compare our MBR results against beam search both with and without length normaliza- tion. We use the models\u2019 default values for length penalty (\u03b2 = 2 for BART, \u03b2 = 1 for mBART). (16) (17) Method R1 R2 RL BS Beam search, no correction Beam search 43.88 43.95 20.96 21.00 30.77 30.84 87.79 87.81 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 47.70 44.29 44.29 47.60 47.41 23.00 19.95 19.98 22.93 22.72 32.54 29.99 30.0 32.48 32.25 88.50 88.03 88.03 88.48 88.43 Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively. Method BLEU chrF BLEURT BS Beam search, no correction Beam search 33.21 33.06 59.81 60.05 65.50 65.60 94.95 94.96 MBR, No correction MBR, Length norm, \u03b2 = 0.5 MBR, Length norm, \u03b2 = 1.0 MBR, Length reward, \u03b3 = 0.5 MBR, Length reward, \u03b3 = 1.0 33.56 31.14 31.09 32.09 31.29 60.00 58.53 58.51 59.63 59.17 65.53 64.70 64.68 65.19 64.91 94.96 94.71 94.71 94.82 94.73 Table 4: MBR results for various length correction schemes on WMT\u201916 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi\u00b4c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021). Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk. However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (M\u00fcller and Sennrich, 2021). 6 MBR applications in NLP The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s. Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En- LR 108.00 114.39 111.64 110.75 110.77 112.52 112.50 LR 99.37 101.58 100.04 102.82 102.60 105.00 105.63 lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer \u2013 or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3). Adoption in NLP MBR was adopted by the speech and NLP communities in the"}