{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_StarCoder:_may_the_source_be_with_you!_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the dataset selected from the Pile, the CodeParrot dataset, and additional data for Python, Java, and C++?", "answer": " CodeGeeX", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " Which model was the initial model for GitHub Copilot?", "answer": " code-cushman-001 by OpenAI", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " Which model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing?", "answer": " code-cushman-001", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " Which benchmarks are widely used for Code LLMs?", "answer": " HumanEval and MBPP", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " What is the total number of problems in the DS-1000 code completion benchmark?", "answer": " 1,000", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " According to the text, which model is the highest-performing open-access model on HumanEval and MBPP benchmarks?", "answer": " StarCoder", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " What type of benchmarks are HumanEval and MBPP?", "answer": " Code LLMs benchmarks", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " What is the major limitation of HumanEval and MBPP benchmarks?", "answer": " They are simple programming puzzles that are not representative of the code that most programmers write", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " What benchmark has a suite of 1,000 realistic and practical data science workflows across seven libraries?", "answer": " DS-1000", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}, {"question": " Which model substantially outperforms all other models on data science problems from the DS-1000 benchmark?", "answer": " StarCoder", "ref_chunk": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}], "doc_text": "selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data for Python, Java, and C++. CodeGeeX also includes its own multi-language benchmark suite, HumanEval-X, which we discuss below. 4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub Copilot (Chen et al., 2021). The details of its training set are unknown. This model has been deprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of writing.12 5. Finally, although they are not specifically trained for code generation, we include some results from the LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly available. 6.1 StarCoder: Python Evaluation In this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and closed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science problems based on StackOverflow questions. 6.1.1 The HumanEval and MBPP Benchmarks HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs consisting of hundreds of Python programming problems that use test cases to validate the code produced by 12There had been a code-cushman-002, but it is not available at the time of writing. 18 Published in Transactions on Machine Learning Research (12/2023) Format Model M a t p l o tli b m a s o r c c i P c i k it - a r L e n s o r F l o Number of problems: 155 220 291 68 106 115 45 Completion Completion Completion CodeGen-16B-Mono Completion Completion Completion SantaCoder-1B InCoder-6B code-cushman-001 StarCoderBase StarCoder 21.6 28.3 31.7 40.7 47.0 51.7 4.6 4.4 10.9 21.8 27.1 29.7 0.9 3.1 3.4 7.9 10.1 11.4 2.6 4.4 7.0 12.4 19.5 21.4 2.4 2.8 9.0 11.3 21.7 20.2 4.8 2.8 10.8 18.0 27.0 29.5 3.1 3.8 15.2 12.2 20.5 24.5 Insertion Insertion Insertion Insertion SantaCoder-1B InCoder-6B StarCoderBase StarCoder 21.6\u2217 28.3\u2217 47.0\u2217 51.7* 13.8 4.6 26.3 30.8 2.0 2.9 10.9 10.3 3.8 4.4 16.6 21.0 5.7 2.8 20.2 20.2 6.9 3.1 30.2 27.4 14.8 7.8 22.3 20.0 Table 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All models evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy averaged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion formats are identical. a Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance using the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a problem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021), we use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples for all experiments with open-access models. For API models, we use n = 20 samples, which is enough to estimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem is solved in a single attempt by the model. Table 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and closed-access models: 1. StarCoder is the highest-performing open-access model on both benchmarks. 2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being significantly smaller. 3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a similarly-sized open-access model that was fine-tuned on Python. 4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model. 6.1.2 The DS-1000 Python Data Science Benchmarks A major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al., 2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates generations in execution against test cases. DS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for all models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried et al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas, SciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in Table 13 and draw the following conclusions: 1. StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library. 19 Overall 1,000 5.7 7.4 11.7 18.1 23.8 26.0 9.3 7.5 24.0 25.4 Published in Transactions on Machine Learning Research (12/2023) 2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000. 3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks does not always correlate with performance on the more realistic DS-1000 benchmarks. For example, CodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval and MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating models on a range of benchmarks. 6.1.3 The ODEX Open-Domain Coding Benchmark Our previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in MBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022) containing 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014 English, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation. We report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen- 16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by"}