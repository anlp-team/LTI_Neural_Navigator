{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Balancing_Exploration_and_Exploitation_in_Hierarchical_Reinforcement_Learning_via_Latent_Landmark_Graphs_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of using variational autoencoders in recent works?", "answer": " To extract low-dimensional features from observations.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " Why is it more effective to focus on the key aspects of variation in subgoal representations?", "answer": " To emphasize state factors inducing significant differences in corresponding actions and de-emphasize irrelevant features.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " How do some recent studies extract useful features as subgoals?", "answer": " By using slow feature analysis methods.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " What method does the text mention for achieving efficient exploration in GCHRL?", "answer": " Scheduling between intrinsic and extrinsic task policies.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " How is efficient exploitation typically achieved in GCHRL?", "answer": " Through planning, where high-level graphical planners have shown great promise.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " How do some methods select subgoals based on graphs?", "answer": " By using an attention mechanism.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " How does the method proposed by Zhang et al. differ from other methods in generating subgoals?", "answer": " They build graphs in the learned latent space instead of planning in the state space.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " What is the discount factor denoted by \u03b3 in GCHRL?", "answer": " A value in the range [0, 1) used to discount future rewards.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " What base RL optimizer is used in both levels of the GCHRL framework?", "answer": " Soft Actor-Critic (SAC).", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}, {"question": " What is the purpose of the Universal Value Function Approximator in GCHRL?", "answer": " To generalize to any subgoal by identifying the underlying structure across states and subgoals.", "ref_chunk": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}], "doc_text": "subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations. Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions. III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple \u27e8S, G, A, P, R, \u03b3\u27e9, where S is a state space, G is a subgoal space, A is an action space, P : S \u00d7A\u00d7S \u2192 [0, 1] is an environmental transition function, R : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy \u03c0\u03b8h(g|s) over subgoals given states and a low-level policy \u03c0\u03b8l(a|s, g) over actions given states and subgoals. The policy \u03c0\u03b8h operates at a slower timescale and samples a subgoal gt \u2208 G when t \u2261 0 (mod c), for fixed c. The policy \u03c0\u03b8h is trained to optimize the expected cumulative discounted environmental rewards E\u03c0\u03b8h ,\u03c0\u03b8l t=0 \u03b3tR(st, at)]. The policy \u03c0\u03b8l selects an action at \u223c \u03c0\u03b8l (\u00b7|st, gt) at every time step and is intrinsically re- warded with rl t(gt, \u03c6(st)) = \u2212D(gt, \u03c6(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy \u03c0\u03b8l is trained to optimize the expected cumulative intrinsic rewards E\u03c0\u03b8l B. Universal Value Function Approximator [(cid:80)\u221e (cid:104)(cid:80)ci+c\u22121 t=ci (cid:105) rl t , i = 0, 1, 2, . . . . We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal. We define a pseudo-discount function \u03c3 : S \u2192 [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that \u03c3(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) \u2248 Vg,\u03c0\u2217 is the \u03b8l (s), where \u03c0\u2217 \u03b8l Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space\u2019s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online. optimal low-level policy and Vg,\u03c0\u03b8l is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy \u03c0\u03b8l : Vg,\u03c0\u03b8l (s) = E (cid:20) \u221e (cid:88) t=0 rl t(g, \u03c6(st)) t (cid:89) k=0 (cid:12) (cid:12) \u03c3(sk) (cid:12) (cid:12) (cid:21) . s0 = s V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way. IV. METHOD We introduce HILL in detail in this section, and its main (1) The function \u03c6 is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2: Lr(st) = ||\u03c6(st) \u2212 \u03c6old(st)||2. We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating \u03c6, we sample the top k triplets with the smallest losses and calculate"}