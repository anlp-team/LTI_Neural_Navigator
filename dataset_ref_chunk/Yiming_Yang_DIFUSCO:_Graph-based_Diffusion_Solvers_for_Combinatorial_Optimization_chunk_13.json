{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_DIFUSCO:_Graph-based_Diffusion_Solvers_for_Combinatorial_Optimization_chunk_13.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many random instances are used for training TSP-100?", "answer": " 1502000", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " What batch size is used for training DIFUSCO in TSP-500?", "answer": " 64", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " What type of learning is applied for the TSP-10000 task?", "answer": " Curriculum learning", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " What checkpoint is used to initialize the model for TSP-1000?", "answer": " TSP-100", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " What approach is adopted for high-quality solutions in the TSP task?", "answer": " Monte Carlo tree search (MCTS)", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " What is the denoising diffusion implicit models (DDIMs) algorithm used for?", "answer": " Accelerating inference for continuous diffusion models", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " How many diffusion steps are shown in the first row of the continuous DIFUSCO illustration in Figure 7?", "answer": " 50", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " How many baseline methods are evaluated against DIFUSCO on TSP-50 and TSP-100?", "answer": " 10", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " What category does the baseline method Concorde belong to?", "answer": " Traditional Operations Research (OR) methods", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}, {"question": " Which MIS solver is a heuristics solver for MIS?", "answer": " KaMIS", "ref_chunk": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}], "doc_text": "size of 512. TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256. TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply curriculum learning and initialize the model from the TSP-100 checkpoint. \u2022 TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply curriculum learning and initialize the model from the TSP-500 checkpoint. \u2022 SATLIB: We use the training split of 49500 examples from [46, 92] and train DIFUSCO for 50 epochs with a batch size of 128. ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32. F Decoding Strategies Greedy Decoding We use a simple greedy decoding scheme for diffusion models, where we sample one solution from the learned distribution p\u03b8(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy decoding. Sampling Following Kool et al. [64], we also use a sampling scheme where we sample multiple solutions in parallel (i.e., each diffusion model starts with a different noise xT) and report the best one. Monte Carlo Tree Search For the TSP task, we adopt a more advanced reinforcement learning- based search approach, i.e., Monte Carlo tree search (MCTS), to find high-quality solutions. In MCTS, we sample k-opt transformation actions guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation, selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please refer to [27]. G Fast Inference for Continuous and Discrete Diffusion Models We first describe the denoising diffusion implicit models (DDIMs) [101] algorithm for accelerating inference for continuous diffusion models. Formally, consider the forward process defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T . For continuous diffusion, the marginal can still be defined as: q(x\u03c4i |x0) := N (x\u03c4i; (cid:112)\u00af\u03b1\u03c4i x0, (1 \u2212 \u00af\u03b1\u03c4i)I) And it\u2019s (deterministic) posterior is defined by: q(x\u03c4i\u22121 |x\u03c4i, x0) := N (x\u03c4i\u22121; (cid:115) \u00af\u03b1\u03c4i\u22121 \u00af\u03b1\u03c4i (cid:16) x\u03c4i \u2212 (cid:112)1 \u2212 \u00af\u03b1\u03c4i \u00b7 (cid:101)\u03f5\u03c4i (cid:17) + (cid:112)1 \u2212 \u00af\u03b1\u03c4i\u22121 \u00b7 (cid:101)\u03f5\u03c4i), 0) \u221a \u221a where (cid:101)\u03f5\u03c4i = (x\u03c4i \u2212 Next, we describe its analogy in the discrete domain, which is first proposed by Austin et al. [5]. \u00af\u03b1\u03c4ix0)/ 1 \u2212 \u00af\u03b1\u03c4i is the (predicted) diffusion noise. 23 (9) (10) Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. For discrete diffusion, the marginal can still be defined as: q(x\u03c4i |x0) = Cat (cid:0)x\u03c4i; p = x0Q\u03c4i (cid:1) , while the posterior becomes: q(x\u03c4i\u22121 |x\u03c4i, x0) = q(x\u03c4i|x\u03c4i\u22121, x0)q(x\u03c4i\u22121|x0) q(x\u03c4i|x0) = Cat \uf8eb \uf8edx\u03c4i\u22121; p = x\u03c4iQ \u22a4 \u03c4i\u22121,\u03c4i x0Q\u03c4ix\u22a4 \u03c4i \u2299 x0Q\u03c4i\u22121 \uf8f6 \uf8f8 , where Qt\u2032,t = Qt\u2032+1 . . . Qt. H Experiment Baselines TSP-50/100 We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3], an exact solver, and 2-opt [71], a heuristic method. \u2022 Learning-based methods include AM [64], GCN [53], Transformer [10], POMO [67], Sym- NCO[59], DPDP [79], Image Diffusion [32], and MDAM [114]. These are the state-of-the-art methods in recent benchmark studies. TSP-500/1000/10000 We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two categories: traditional Operations Research (OR) methods and learning-based methods. Traditional OR methods include Concorde [3] and Gurobi [35], which are exact solvers, and LKH-3 [39], which is a strong heuristic solver. We use two settings for LKH-3: (i) default: 10000 trials (the default configuration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline. \u2022 Learning-based methods include EAN [22], AM [64], GCN [53], POMO+EAS [67, 48], Att-GCN [27], and DIMES [92]. These are the state-of-the-art methods in recent benchmark studies. They 24 (11) (12) Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported without any post-processing. Discrete DIFUSCO with 50 (first row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in cosine schedule are shown. Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are reported without any post-processing. Continuous DIFUSCO with linear schedule (first row) and cosine schedule (second row) with 20 diffusion steps are shown. can be further divided into reinforcement learning (RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage [6] to fine-tune each instance. We take the results of the baselines from Fu et al. [27] and Qiu et al. [92]. Note that except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs. MIS For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL) category, i.e., LwD [1]) and DIMES [92], and the supervised learning (SL) category, i.e., Intel [70] and DGL [8]. 25 Figure 10: Qualitative illustration of"}