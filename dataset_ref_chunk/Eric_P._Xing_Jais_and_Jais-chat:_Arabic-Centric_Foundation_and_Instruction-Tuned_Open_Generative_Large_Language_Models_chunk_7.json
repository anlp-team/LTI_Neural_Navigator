{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Jais_and_Jais-chat:_Arabic-Centric_Foundation_and_Instruction-Tuned_Open_Generative_Large_Language_Models_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are MWS+23 collections?", "answer": " MWS+23 collections are prompted datasets that cover a diverse set of NLP tasks in instruction-response format.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " How many prompt types does the P3 dataset contain?", "answer": " The P3 dataset contains over 2,000 prompt types.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " What is the purpose of xP3 (Code & English) dataset?", "answer": " xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " What is included to make the model diverse?", "answer": " To make the model diverse, at most five thousand examples from each task of the Super-NaturalInstructions dataset are included.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " What is the Natural Questions dataset comprised of?", "answer": " The Natural Questions dataset comprises question-answer pairs extracted from Google Search.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " What type of dataset is Baize-Chatbot26?", "answer": " Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " What is the focus of HH-RLHF dataset?", "answer": " HH-RLHF is focused on helpful and harmless assistance through preference modeling.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " What is Self-instruct?", "answer": " Self-instruct is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " What is the purpose of Alpaca-CoT dataset?", "answer": " Alpaca-CoT is a fusion of nine Chain-of-Thought datasets released by FLAN.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}, {"question": " What does the Open Instruction Generalist dataset target?", "answer": " The Open Instruction Generalist dataset is aimed at enhancing the conversational abilities of fine-tuned models.", "ref_chunk": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}], "doc_text": "[MWS+23] are collections of prompted datasets that cover a diverse set of NLP tasks in instruction\u2013response format. The P3 dataset contains over 2,000 prompt types from 270 different public datasets in English. xP3 (Code & English) is designed for multi-lingual and cross-lingual instruction-tuning and contains more than 9M examples in 46 languages, including programming languages. To make our model diverse, we included at most five thousand examples from each task of the Super-NaturalInstructions dataset; from P3 and xP3 (Code & En- glish), we only include English and programming code examples. The Natural Questions dataset25 comprises question\u2013answer pairs extracted from Google Search; it only includes questions with concise answers, which can be addressed using the information found in English Wikipedia [KPR+19]. Baize-Chatbot26 is a multi-turn dialogue-style instruction-tuning dataset. HH-RLHF is designed for helpful and harmless assistance through preference modelling [OWJ+22], and has an accepted and a rejected response for each prompt; we only use the former. Alpaca-CoT [QS23] is a fusion of nine Chain-of-Thought (CoT) [WWS+22] datasets released by FLAN [CHL+22]. Self-instruct [WKM+23] is a bootstrapping algorithm that uses a small set of manually written instructions to prompt an LLM to generate new instructions. 24www.cerebras.net/blog/introducing-condor-galaxy-1-a-4-exaflop-supercomputer-for-generative-ai/ 25https://huggingface.co/datasets/nq_open 26https://huggingface.co/datasets/linkanjarad/baize-chat-data 12 Source P3 [SWR+21] Super-NaturalInstructions [WMA+22] Baize-Chatbot26 HH-RLHF [BJN+22] Unnatural Instruction [HSLS23] xP3 (Code & English) [MWS+23] Alpaca-Cleaned27 Stack-Exchange-Instruction36 GPT4ALL-J [AND+23] Natural Questions Self-instruct [WKM+23] Alpaca-CoT [QS23] Instruct-Wild [XJS+23] Open Instruction Generalist (OIG)29 GPTeacher28 SafetyQA GSM-General-QA31 Dolly-15k [CHM+23] NativeQA Instruction-Poems34 Math-Instruction32 Grade-School-Math33 HC3 [GZW+23] Essays-with-Instructions35 Basic-Conv38 Python-QA37 Persona Examples Words in the Prompt Words in the Response 2,432,173 1,623,200 595,700 214,342 199,416 186,936 98,664 98,197 92,324 86,944 81,430 74,028 51,603 39,581 31,331 21,936 15,955 14,794 13,859 13,679 12,373 7,827 7,123 2,040 757 525 19 341,968,765 211,172,413 62,778,796 22,940,205 8,605,602 30,669,413 1,365,561 14,543,421 11,452,095 770,708 1,905,549 3,146,343 587,335 581,858 1,130,588 221,462 75,1504 1,011,315 150,543 34,4053 44,5160 41,9171 136,182 13,7105 2,930 16,865 177 26,639,089 12,655,353 21,383,898 11,296,965 2,365,377 1,123,3079 7,837,525 12,287,752 17,736,758 224,064 1,549,345 2,037,041 5,460,064 2,087,511 1,751,643 1,259,799 742,140 888,112 661,995 3,429,455 1,085,486 391,146 980,388 3,278,426 6,795 11,899 641 Total 6,016,756 717,255,119 149,281,746 Table 6: Details about the English instruction-tuning datasets. We used the dataset provided by the authors, which was cleaned and filtered to remove low-quality or similar pairs. Alpaca-Cleaned27, Instruct-Wild [XJS+23], Unnatural Instruction [HSLS23] and GPTeacher28 are prepared using the same method, but using ChatGPT [BMR+20]. Open Instruction Generalist (OIG)29, GPT4ALL-J [AND+23], and Dolly-15k [CHM+23] were constructed to train assistant-style LLMs in a semi-automatic way, and are moderate in quality. From GPT4ALL-J, we ran- domly sampled 100,000 examples from v1.0.30 HC3 [GZW+23] is a manually curated dataset for comparing the response of humans and ChatGPT; we used the former only. From HC3, we only included examples from four domains: finance, medicine, Wikipedia, and OpenQA. GSM-General-QA 31, Math-Instruction32 and Grade- School-Math33 are instruction-tuning datasets prepared to assist in mathematical problems. Finally, Instruction- Poems 34 and Essays-with-Instructions35 target poem and essay writing, and Stack-Exchange-Instruction36 and Python-QA37 are aimed at programming code tasks. In order to enhance the conversational abilities of our fine-tuned model, we integrated dialogue-based and persona-based datasets into the instruction-tuning procedure. For this purpose, we curated 19 in-house question\u2013 answer pairs that revolved around the LLM developer, and we also processed the Basic-Conv38 dataset to incor- porate it into our instruction-tuning process. 27https://huggingface.co/datasets/yahma/alpaca-cleaned 28https://huggingface.co/datasets/causal-lm/gpt_teacher 29https://huggingface.co/datasets/iamketan25/oig-instructions-dataset 30https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations 31https://huggingface.co/datasets/iamketan25/gsm-general-qa-instructions 32https://huggingface.co/datasets/alpayariyak/MATH_Instruction_Format 33https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions 34https://huggingface.co/datasets/checkai/instruction-poems 35https://huggingface.co/datasets/ChristophSchuhmann/essays-with-instructions 36https://huggingface.co/datasets/ArmelR/stack-exchange-instruction 37https://huggingface.co/datasets/iamketan25/python-qa-instructions-dataset 38https://github.com/gunthercox/chatterbot-corpus/tree/master 13 Dataset xP3-Ar [MWS+23] Super-NaturalInstructions-Ar Baize-Ar Unnatural-Ar Natural Questions-Ar Bactrian-Ar [LKW+23] Alpaca-Ar SafetyQA-Ar NativeQA-Ar Dolly-15k-Ar HC3-Ar NER-Ar [BRB07] Basic-Conv-Ar Examples 1,375,257 1,251,444 590,846 199,100 86,005 66,880 51,280 22,617 15,018 14,833 7,139 1,969 756 Is Translated? Words in the Prompt Words in the Response No Yes Yes Yes Yes No Yes Mixed No Yes Yes No Yes 218,690,644 168,043,882 57,327,249 7,663,930 620,065 1,555,439 564,586 213,617 141,669 978,678 125,248 133,912 2,355 80,084,863 12,011,052 19,980,175 2,296,384 220,377 4,445,417 1,759,388 1,122,890 1,021,817 820,228 893,921 31,027 5,517 Total 3,683,144 456,061,274 124,693,056 Table 7: Details about the Arabic instruction-tuning datasets. We further created our own set of question\u2013answer pairs related to the UAE and the local region, based on information from relevant Wikipedia pages and other sources. We refer to this dataset as NativeQA and incorporate it into the fine-tuning process. We also prepared an instruction dataset to teach the model about safety issues, named it SafetyQA. As a responsible language model, we want the model to avoid engaging in unsafe conversations e.g. discussions on self-harm, sexual violence, or identity attacks. For this, we prepared prompt-response from DoNotAnswer [WLH+23] and OLID [ZMN+19]. In all these prompts, the response is a polite rejection of the question. The impact is explored in Section 6. 4.1.2 Arabic Instruction-Tuning Datasets Due to the limited availability of instruction-tuning datasets for Arabic, we translated some of the above En- glish instruction-tuning datasets to Arabic using the same machine translation system that we used for the training data: Supernatural Instruction, Unnatural, NaturalQuestions, Alpaca [TGZ+23], HC3, Dolly-15k, Baize, Basic-Conv, Bactrian [LKW+23]. We then performed a manual assessment for each task within the Super-NaturalInstructions dataset, and excluded tasks that were primarily related to translation as well as those relating to counting words, as they could break when translated to Arabic (i.e., their is no guarantee the trans- lated text has the same number of words as the original English). Apart from the translated datasets, we also included the Arabic examples from xP3 (Code & English). We further formatted AraNER [BRB07] to the instruction\u2013response format (NER-Ar) and added it as a dataset for instruction-tuning. Moreover, similarly to English, we created additional datasets NativeQA-Ar and SafetyQA- Ar with instruction\u2013response pairs related to the UAE and the region as well as safety, but this time in Arabic; note that we created these natively in Arabic. We further translated the English datasets that we created to Arabic, and we used them as additional datasets. 4.2 Instruction-Tuning Setup In instruction-tuning, each instance comprises a pair of a prompt and its corresponding response, and the model needs to be able to distinguish between them. We thus wrap each instance within a template as illustrated in Figure 4, where we have"}