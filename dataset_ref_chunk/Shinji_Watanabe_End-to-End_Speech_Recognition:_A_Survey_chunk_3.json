{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What journal has accepted this article for publication?,        answer: IEEE/ACM Transactions on Audio, Speech and Language Processing    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " What is the DOI citation information provided in the text?,        answer: DOI 10.1109/TASLP.2023.3328283    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " Why is multipass search exploited by both classical ASR models and E2E models?,        answer: To perform (external) language model rescoring    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " According to the text, what are E2E ASR models composed of?,        answer: An encoder module and a decoder module    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " How is the encoder module denoted in the text?,        answer: H(X)    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " How is the input speech utterance denoted in the text?,        answer: X    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " What is the word sequence assumed to be parameterized into in the text?,        answer: D-dimensional acoustic frames    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " According to the text, what must any statistical approach to ASR determine?,        answer: How to model the word sequence posterior probability, P(C|X)    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " How is a natural taxonomy of E2E ASR modeling based on, according to the text?,        answer: Various strategies for modeling the word sequence posterior    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}, {"question": " How are explicit and implicit E2E ASR modeling approaches distinguished?,        answer: Based on the modeling of the sequence-to-sequence alignment    ", "ref_chunk": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}], "doc_text": "This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring. All in all, we need to conclude that a) \u201cE2E\u201d does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling. A. Encoder and Decoder Modules Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T \u2032 into a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of length T (typically T \u2264 T \u2032). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output: III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T \u2032: X = (x1, \u00b7 \u00b7 \u00b7 , xT \u2032), where xt \u2208 RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]). P (C|X) = P (cid:0)C(cid:12) (cid:12)H(X)(cid:1) Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1) P (cid:0)C[, A](cid:12) = L (cid:89) P (cid:0)ci[, ai](cid:12) (cid:12)ci\u22121 1 [, ai\u22121 1 ], vi(ci\u22121 1 [, ai\u22121 1 ], H(X))(cid:1) ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes\u2019 decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt\u2032 and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment: a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A: P (C|X) = (cid:88) P (C, A|X) A i=1 where the notation mi\u22121 to the sequence the variables m; and, of vi(ci\u22121 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci\u22121 (and their latent variable context ai\u22121 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame\u2019s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector. corresponds 1 i \u2212 1 previous instances of [, ai\u22121 1 1 1 (cid:0)ci\u22121 1 b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly. Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences \u2013 the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3 3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa. Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as"}