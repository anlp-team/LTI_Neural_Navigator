{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Inference-Time_Policy_Adapters_(IPA):_Tailoring_Extreme-Scale_LMs_without_Fine-tuning_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the IPA in the text?", "answer": " The main focus of the IPA is to improve upon LLMs such as GPT-3 and reduce toxicity in generated text.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " What is the REALTOXICITYPROMPTS benchmark used for in the evaluation of IPA?", "answer": " The REALTOXICITYPROMPTS benchmark is used to assess the efficacy of IPA in reducing toxicity from LLMs by generating a fluent continuation without offensive content for a given prompt.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " How does IPA compare to expensive fine-tuned GPT-3 in terms of performance and cost?", "answer": " IPA sometimes outperforms expensive fine-tuned GPT-3 in benchmarks while being more cost-effective.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " What metrics are used to evaluate the toxicity reduction of LMs by IPA?", "answer": " Metrics used to evaluate toxicity reduction include average maximum toxicity across sampled generations, toxicity probability of at least one toxic generation, fluency, and diversity of generated output.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " What RL algorithm is used in the adapter optimization for IPA?", "answer": " QUARK is used as the RL algorithm in the adapter optimization for IPA.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " How does IPA perform when tailoring GPT-2 and GPT-3 compared to other baselines?", "answer": " IPA outperforms learning-based and decoding-based methods when tailoring GPT-2 and GPT-3, reducing toxicity while maintaining language quality.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " What is the effectiveness of directly applying the policy adapter optimized for GPT-2 on top of GPT-3?", "answer": " Directly applying the policy adapter optimized for GPT-2 on top of GPT-3, i.e., IPA-, is highly effective, showcasing the adaptability and reusability of IPA.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " How does IPA compare to domain adaptive training (DAPT) when tailoring GPT-3?", "answer": " IPA outperforms DAPT in tailoring GPT-3, emphasizing the cost-efficiency of IPA as an approach to align LLMs.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " What is the purpose of the ablation studies conducted in the text?", "answer": " The ablation studies aim to assess the effect of different RL algorithms on the performance of IPA in reducing toxicity and maintaining language quality.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}, {"question": " What is the goal of lexically constrained generation and how is it tested in the text?", "answer": " The goal of lexically constrained generation is to generate text that includes specific keywords in the correct order. It is tested using datasets like COMMONGEN, where the models are instructed to follow ordered lexical constraints.", "ref_chunk": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}], "doc_text": "dialogue (\u00a74.5). In all benchmarks, IPA consistently improve upon LLMs such as GPT- 3 (text-davinci-003), surpassing competitive baselines and sometimes even outperforming ex- pensive fine-tuned GPT-3 at a fraction of the cost. 4.1 Toxicity Reduction LMs are susceptible to generating toxic comple- tions, even when prompted with seemingly innocu- ous text (Gehman et al., 2020). Here, we assess IPA\u2019s efficacy in reducing toxicity from LMs. Datasets and Metrics. The task is to generate a fluent continuation y while avoiding offensive con- tent for a given prompt x. We evaluate this on RE- ALTOXICITYPROMPTS benchmark (Gehman et al., 2020), which contains 100k prompts designed to elicit toxic generations. Following the experimen- tal setup of Liu et al. (2021b), we use Perspective API to determine the average maximum toxicity across 25 sampled generations and the (empirical) toxicity probability of at least one toxic generation. In addition, we report fluency as the perplexity of generated output based on an off-the-shelf GPT2- XL model, and diversity as the count of unique n-grams normalized by the length of text. We also perform human evaluations; see Appendix A.1 for more details. Setup and Baselines We apply IPA to tailor off- the-shelf GPT-2 and GPT-32. To tailor GPT-2, we directly apply the base policy in the adapter train- ing, denoted as IPA(GPT-2). For tailoring GPT-3, we use an off-the-shelf GPT-2 and a distilled GPT-3 3 as the approximate policy for the adapter training, labeled as IPA-(GPT-3) and IPA*(GPT-3) respec- tively. Notice that IPA-(GPT-3) is equivalent to directly applying the policy adapter trained to tai- lor GPT-2 on top of GPT-3. We initialize all the policy adapters with a pre-trained GPT2-L model. We use QUARK as the RL algorithm in adapter optimization, and provide additional ablation stud- ies to assess the effects of different RL algorithms. We use the Perspective API as the reward func- tion, which provides a score ranging from 0 to 1 to indicate the degree of toxicity. For tailoring GPT-2, we compare IPA with previously reported baselines from Lu et al. (2022a), including decoding-based methods: PPLM (Dathathri et al., 2020a), GeDi (Krause et al., 2We refer text-davinci-003 as GPT-3 in this paper 3We finetune a GPT2-XL with prompt-output pairs from GPT-3 on REALTOXICITYPROMPTS as the distilled GPT-3. Models Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. base policy: GPT2-L GPT-2 PPLM GeDi DEXPERTS DAPT 0.527 0.520 0.363 0.314 0.428 0.520 0.518 0.217 0.128 0.360 11.31 32.58 60.03 32.41 31.21 0.85 0.86 0.84 0.84 0.84 0.85 0.86 0.83 0.84 0.84 PPO QUARK 0.218 0.196 0.044 0.035 14.27 12.47 0.80 0.80 0.84 0.84 IPA (GPT-2) base policy: GPT-3 GPT-3 DEXPERTS DAPT IPA- (GPT-3) IPA* (GPT-3) 0.138 0.275 0.223 0.254 0.150 0.101 0.031 0.197 0.112 0.176 0.056 0.028 11.94 10.65 23.41 20.19 10.34 12.68 0.80 0.78 0.79 0.80 0.79 0.79 0.84 0.81 0.82 0.83 0.81 0.83 Table 1: Automatic evaluation for Toxicity Reduction with off-the-shelf GPT2-large (top) and GPT-3 (bottom) as the base policy to tailor. RL Algo. Toxicity Fluency Diversity Avg Max. Prob. Pl. Dist-2. Dist-3. Quark PPO NLPO 0.138 0.125 0.136 0.031 0.029 0.032 11.94 12.47 12.13 0.80 0.80 0.80 0.84 0.84 0.85 Table 2: Comparison of using different RL algorithm for training IPA for Toxicity Reduction with off-the-shelf GPT2-large as the base policy to tailor. 2021), DExpert (Liu et al., 2021a), and learning- based methods: DAPT (Gururangan et al., 2020), PPO (Schulman et al., 2017), and QUARK (Lu et al., 2022a). For tailoring GPT-3, we compare IPA to the baselines described above that are compatible with GPT-3\u2019s limited accessibility: DExpert (Liu et al., 2021a) and DAPT (Gururangan et al., 2020). We also provide runtime analysis in Appendix B. Results As shown in Table 1, IPA outperforms all learning-based and decoding-based methods in tai- loring GPT-2 and GPT-3, significantly reduces the toxicity while maintaining language quality. Inter- estingly, we found that applying the policy adapter optimized for GPT-2 directly on top of GPT-3 (i.e., IPA-) is highly effective, showcasing the adaptabil- ity and reusability of IPA. Notably, when tailoring GPT-3, IPA outperforms the costly domain adap- tive training (DAPT), which exhaustively fine-tune GPT-3 on a non-toxic corpus. This further em- phasizes the promise of the IPA as a cost-efficient approach to align LLMs. Our findings are further confirmed by human evaluation (Appendix A.1). Finally, we conduct ablations on the effect of Figure 2: Performance of IPA- (blue line) with respect to the size of the adapter model (distill-GPT2, GPT2- small, GPT2-medium, GPT2-large, GPT2-XL) on top of a off-the-shelf GPT-3 as the base policy. The grey line denotes the performance of the off-the-shelf GPT-3. Models Automatic Human Cov. Fl. Qu. Pl. Overall GPT-3 GPT-3.5 GPT-4 37.01 65.17 84.81 94.89 95.89 95.49 2.84 2.93 2.95 2.81 2.88 2.97 2.60 2.90 2.96 GPT-3sft IPA\u2217 (GPT-3) 72.89 88.54 73.96 92.58 2.56 2.90 2.60 2.87 2.50 2.88 Table 3: Automatic and human evaluation results for Lexically Constrained Generation. Human evaluation scores are on a 3-point Likert Scale.4 RL algorithms. As shown in Table 2, IPA is ef- fective with various RL algorithms, all of which lead to state-of-the-art performance. Additional ablation in Figure 2 shows that a policy adapter as small as a distilled GPT-2 can effectively tailor the 1000 larger GPT-3 model, achieving comparable \u00d7 performance with our main result. 4.2 Lexically Constrained Generation Next, we test IPA in lexically constrained gener- ation. We consider a more challenging setup of ordered lexical constraints, where the generation is considered correct if it includes all the keywords with the correct order specified in the input prompt. Datasets and Metrics. We use COMMONGEN (Lin et al., 2020), a dataset for generative com- monsense reasoning. We deliberately instruct the models to generate a sentence with the given key- words while following the order they appear in the input prompt. For automatic evaluation, we gauge the constraint satisfaction with coverage, a binary metric that evaluates a generation to be correct only when it includes all the keywords and also matches the specified order. We also measure the fluency using a critic model fine-tuned"}