{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Segment-Level_Vectorized_Beam_Search_Based_on_Partially_Autoregressive_Inference_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What issue arises in the Mask-CTC-based NAR decoding if the number of masked tokens is different from the actual number of tokens?", "answer": " The accuracy degrades significantly.", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " Give an example of an incorrect result in the Mask-CTC decoding process when the masked sequence is different from the correct sequence.", "answer": " e.g., s, e, e, a or s, e, a, a", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " What percentage of the masked sequences in the preliminary experiments did not match the proper length?", "answer": " Almost 40%", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " At what probability do insertion errors caused by the absence of a target token occur?", "answer": " 2%", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " How does the PAR architecture deal with NAR accuracy issues related to incorrect target length?", "answer": " It proposes segment-level vectorized beam search.", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " What is the purpose of using beam search in the PAR architecture?", "answer": " To predict the tokens corresponding to a mask token.", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " How are tokens updated using the AR process in the PAR architecture?", "answer": " Tokens with probability less than the threshold value are considered less confident and replaced with a mask token.", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " What approach is used to reduce the computational complexity in the PAR architecture?", "answer": " Segment-level vectorized beam search", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " What is the function of the max iteration parameter in the PAR architecture?", "answer": " It denotes the maximum number of tokens for one mask in the beam search.", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}, {"question": " How is the vectorized beam search different from traditional beam search in the PAR architecture?", "answer": " It allows for the calculation of multiple beams simultaneously.", "ref_chunk": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}], "doc_text": "difference in the proportion of the encoder and decoding process. However, there is an accuracy issue with the Mask-CTC-based NAR decoding. If the number of masked tokens is different from the actual number of tokens, the accuracy degrades significantly. For ex- ample, if the correct sequence is s, e, a and the masked sequence is s, #, #, a, the result of Mask-CTC becomes incorrect, since it tends to assign the same numbers of tokens to the masks, e.g., s, e, e, a or s, e, a, a. Similarly, if the masked sequence is s, e, #, a, it will have an insertion error. In our preliminary experiments, we observed that almost 40% of the masked sequences do not match the proper length. Additionally, we have observed that insertion errors caused by the absence of a target token, occur with a probability of 2%. The architecture of PAR is illustrated in Fig. 1c. PAR first gen- erates a sequence of tokens by gCTC approach, then apply the mask process using the Mask-CTC method [13]. Finally, it predicts the tokens corresponding to a mask token using beam search, similarly to the AR approach. To reduce the computational complexity, we propose the segment-level vectorized beam search, which signifi- cantly reduces the number of search iterations. By applying this beam search, we can solve the NAR accuracy issue related to the incorrect target length. As an example, let us focus on a case where only one mask is in the sequence for simplicity, and let the Pthres represent the threshold value ranging from 0 to 1. Initially, we use the gCTC result, obtained without the AR process. However, the resulting text can contain errors due to the conditional independence assumption. We expect these errors can be corrected by the AR process, where we utilize the pre-trained decoder and beam search. To determine which tokens to update using the AR process, we use Pthres as a filter for posterior probability. Tokens with probability less than Pthres will be consid- ered less confident and replaced with the mask token. Consecutive mask tokens are merged into a single mask because we will estimate the sequences in the AR process. Finally, we use beam search to estimate the tokens. We iterate the beam search for max iteration times for any audio length, where max iteration denotes the maximum number of tokens for one mask. The value of max iteration is highly dependent on the threshold Pthres, where a higher value is required when Pthres is closer to 1 and more tokens are replaced with mask tokens. 4.2. Segment-level Vectorized Beam Search The beam search process explained in the previous section focused on the case where there is a single mask token. However, in practice, multiple mask tokens may exist for a given sequence. To handle multiple masks, we parallelize the decoding of masks by extending the vectorized beam search [23]. The vectorized beam search is an extension of traditional beam search that allows for the calculation of B beams simultaneously. In an offline scenario, it can also be extended to the parallel computation of S utterances. In other words, Algorithm 1 Segment-level Vectorized Beam Search Beam size: B and the number of masks: S Encoder output: X Initialize decoder cache: C Initialize a S-length list for ended hypotheses: FS Initialize hypotheses YS,B and end-of-sequences ES for i = 1 to max iteration do Calculate probability prob = Decoder(YS,B, X, C) Update hypotheses YS,B by top-k method. for s = 1 to S do for b = 1 to B do if last token of Ys,b is Es then Fs.push(Ys,b) end if end for end for end for for s = 1 to S do Replace s-place mask in the masked sequence with best hy- pothesis in Fs end for S \u00d7 B hypotheses can be calculated as a single batch at each step. In our work, we treat the value S as the number of masks to enable parallel computation of all mask tokens. The overview of the proposed segment-level vectorized beam search is described in Algorithm 1. First, we initialize S \u00d7 B hy- potheses YS,B from the gCTC result and masked sequence. Initially, there is only one hypothesis, so Ys,1 contains the gCTC result up to the corresponding mask as the hypothesis. The rest of the hy- potheses, from Ys,2 to Ys,B, are initialized with dummy hypotheses. Additionally, we store the next token for each mask in an end-of- sequence list, ES. Next, we calculate the probabilities of the next token for each hypothesis and update YS,B. We then check, for each mask s, if Es is predicted as the next token for each of the hypothesis from Ys,1 to Ys,B. We push the ended hypothesis to the list of ended hypotheses, Fs. Finally, after iterating max iteration times, we replace each mask token with the best hypothesis in Fs. To simplify the implementation, we apply padding to the missing hypotheses so that the number of hypotheses remains constant, similar to [34]. Table 1 shows an example of the decoding process of a sample in the LibriSpeech test-clean set. After calculating the gCTC result and merging the consecutive masks, there are three masks in this exam- ple. Since the beam size B is set to 10, there are 30 hypotheses. In the first iteration, we correctly estimated the first and third mask to- kens. Since we used the BPE token for this model, the red characters fall, bra, and s were estimated in a single iteration. In the second iteration, we successfully predicted the second mask, which has two corresponding tokens: bro and th. This example demonstrates that it is possible to predict multiple tokens from a single mask token, en- abling us to handle the accuracy issue in NAR due to incorrect target length. Moreover, the overall number of iterations in beam search is significantly lower than in AR by utilizing the segment-level vec- torized beam search, indicating that PAR is effective in avoiding"}