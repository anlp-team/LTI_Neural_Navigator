{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Reproducing_Whisper-Style_Training_Using_An_Open-Source_Toolkit_And_Publicly_Available_Data_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the text?", "answer": " The main focus of the text is on reproducing Whisper-style training using an open-source toolkit and publicly available data.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " How many hours of supervised speech data was OpenAI Whisper model trained on?", "answer": " OpenAI Whisper model was trained on 680k hours of supervised speech data.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " What are some of the challenges that researchers face in enhancing the performance of speech models?", "answer": " Researchers face challenges in comprehending the underlying mechanisms, lacking access to training dynamics, and not having access to the complete model development pipeline.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " What is the purpose of presenting the OWSM model in the text?", "answer": " The purpose of presenting the OWSM model is to reproduce Whisper-style training using open-source tools and publicly available data, and to promote open science by releasing scripts, models, and training logs.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " In what field have large-scale Transformers garnered significant attention?", "answer": " Large-scale Transformers have garnered significant attention in natural language processing (NLP).", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " What is the significance of self-supervised learning techniques in the domain of speech processing?", "answer": " Self-supervised learning techniques have demonstrated impressive achievements in the domain of speech processing.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " What is the unique feature of OWSM in terms of speech translation directions?", "answer": " OWSM supports any-to-any speech translation as opposed to solely any-to-English translation.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " What potential risk is associated with utilizing pre-trained models on novel benchmarks?", "answer": " The potential risk is data leakage, as users are deprived of knowledge regarding the actual training data.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " What does the multitask data format used by OWSM include?", "answer": " The multitask data format used by OWSM includes tasks such as language identification, multilingual automatic speech recognition, and any-to-any speech-to-text translation.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}, {"question": " How does OWSM differ from Whisper in terms of supported translation directions?", "answer": " OWSM supports more translation directions, whereas Whisper can only perform any-to-English translation.", "ref_chunk": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}], "doc_text": "3 2 0 2 t c O 4 2 ] L C . s c [ 3 v 6 7 8 3 1 . 9 0 3 2 : v i X r a REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1, Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3, Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China 3Honda Research Institute Japan, Japan ABSTRACT Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It general- izes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for develop- ing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further im- prove its performance and address training-related issues such as ef- ficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre- trained models and training logs to promote open science.1 Index Terms\u2014 Pre-training, whisper, speech recognition, speech translation actual training data. Secondly, researchers face significant difficul- ties in comprehending the underlying mechanisms and elucidating methods for enhancing the model\u2019s performance, given their lack of access to the training dynamics. Thirdly, the absence of access to the complete model development pipeline poses notable challenges in effectively tackling issues related to robustness, fairness, bias, and toxicity, all of which frequently arise as a result of the data and train- ing procedure [19\u201321]. Recently, there has been a concerted effort to foster open science in the realm of LLM research by advocating for the release of com- plete training pipelines [5]. Inspired by this, we present the Open Whisper-style Speech Model (OWSM)2, which reproduces Whisper- style training using an open-source toolkit and publicly available data. OWSM follows the design of Whisper [15] to support essential tasks such as language identification (LID), multilingual automatic speech recognition (ASR), and utterance-level segmentation. No- tably, OWSM also exhibits several technical novelties. It is designed to support any-to-any speech translation as opposed to solely any-to- English translation (see Section 3.4 for results). OWSM also adopts multiple strategies to enhance the efficiency (see Section 2.5 for dis- cussions). 1. INTRODUCTION Large-scale Transformers [1] have garnered significant attention in natural language processing (NLP) [2\u20137]. These models, trained on extensive datasets, have showcased remarkable emergent capabili- ties in diverse downstream tasks. Notably, the application of simi- lar pre-training techniques has also found success in the domain of speech processing. Self-supervised learning (SSL) techniques have demonstrated impressive achievements [8\u201314]. Furthermore, large- scale supervised learning has emerged as a promising avenue for the development of universal speech models capable of performing multiple speech tasks within a single model [15\u201318]. OpenAI Whis- per [15] is a series of multilingual multitask models trained on 680k hours of labeled speech data which is carefully curated from diverse sources on the Internet. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs, enabling researchers to delve into the specifics of the training pro- cess and gain valuable insights for their own investigations. While OWSM shows competitive or even superior performance compared to Whisper in certain benchmarks, it is essential to clarify that our objective is not to engage in a comprehensive competition with Whisper. The scope of our endeavor is constrained by the fact that our largest dataset comprises only a quarter of the training set used by Whisper, and our resource limitations restrict us from conducting multiple trial runs. Instead, by sharing these resources, we aim to promote transparency and facilitate progress and advancements in the field of large-scale pre-training for speech processing. Despite the release of pre-trained Whisper models and inference code, the comprehensive pipeline for model development (from data preparation to training) remains inaccessible to the public, which has been a common situation for large language models (LLMs). This limitation engenders several concerns. Firstly, the utilization of pre-trained models on novel benchmarks has the potential risk of data leakage, as users are deprived of knowledge regarding the 2. WHISPER-STYLE TRAINING 2.1. Multitask data format OpenAI Whisper [15] employs a single sequence-to-sequence model to perform multiple speech processing tasks, including LID, multi- lingual ASR, any-to-English ST, and utterance-level segmentation. 1https://github.com/espnet/espnet 2OWSM is pronounced as \u201cawesome\u201d. 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE Start Time NO TIMESTAMPS LANGUAGE End Time Start Time \u2026 Text-only transcription Text Text End Time Text EOS ST ASR Previous Text SOP Transcription aligned in utterance level SOS X\u00e0YTranslationX\u00e0XRecognitionLanguageIdentification Fig. 1: Multitask data format used by our OWSM, which mostly follows OpenAI Whisper [15]. Different speech processing tasks are represented in a unified format, which can be predicted by an autoregressive decoder. Note that OWSM is designed to support any-to- any speech-to-text translation, whereas Whisper can only perform any-to-English translation. Blue boxes denote standard text tokens, while orange and green boxes are special tokens. SOP, SOS, and EOS represent start-of-prompt, start-of-sentence, and end-of-sentence, respectively. Our OWSM mostly follows this design, but extends it to potentially support any-to-any ST. Figure 1 illustrates the multitask data format. Data samples from different tasks are represented in a unified format, which can be predicted by the decoder in an autoregressive manner. Specifically, each sample is converted to a sequence of tokens with two segments separated by special tokens. The first segment (be- fore \u201cSOS\u201d) is an optional text prompt used as a condition, while the second segment is the actual target. The target starts with a spe- cial token denoting the language of the input speech."}