{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Syntax_and_Semantics_Meet_in_the_\u201cMiddle\u201d:_Probing_the_Syntax-Semantics_Interface_of_LMs_Through_Agentivity_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many verbs were compiled in the list mentioned in the text?", "answer": " 23 verbs", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " How many unique nouns were there in total?", "answer": " 233 unique nouns", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " How many noun-verb-adverb combinations were there?", "answer": " 820 noun-verb-adverb combinations", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " How many sentences formed intr-agent sentences?", "answer": " 343 sentences", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " How many sentences formed intr-patient sentences?", "answer": " 477 sentences", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " How many sentences were there for trans-agent?", "answer": " 820 sentences", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " How many sentences were there for trans-patient?", "answer": " 820 sentences", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " How many annotators participated in providing judgements for the nouns?", "answer": " 19 annotators", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " What was the scale used by annotators to rate the likelihood of a noun being an agent in an event?", "answer": " A scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent)", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}, {"question": " What does calculating the agent ratio involve?", "answer": " Dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient", "ref_chunk": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}], "doc_text": "can tell there exists no defini- tive list in the linguistics literature of English verbs that display both properties. As a starting point to curate a list of verbs, we consulted literature on verbs that display object drop (Gillon 2012; Fillmore 1986, as well as Levin 1993 for an overview of English verb classes). We compiled a list of 23 verbs (see Appendix A), though this list is certainly non-exhaustive. For each verb, we list nouns and adverbs that can work in combination with each other in all of the tem- plates in Table 1. Criteria for adding nouns and adverbs are listed in the Appendix B. In total, we have 233 unique nouns and a total of 820 noun-verb-adverb combinations. Out of these combinations, 343 form intr-agent sentences and 477 form intr-patient sentences. Since we can put any noun into syntactic subject or object position for the transitive sentences, we have 820 sentences each for trans-agent and trans-patient. Sentence Template intr-agent intr-patient This <noun> <verb> <adverb>. This author writes easily. This paper writes easily. trans-agent This <noun> <verb> something <adv>. This author writes something easily. This paper writes something easily. trans-patient Something <verb> this <noun> <adv>. Something writes this author easily. Something writes this paper easily. Table 1: Templates for experiments 2 and 3. Sentences highlighted in pink contain a <noun> with an \u201cagent\u201d label, while those in blue with \u201cpatient\u201d. 2.2 Approximating \u201cground truth\u201d agentivity labels for nouns out of context Getting a gold \u201cagent\u201d or \u201cpatient\u201d label is straight- forward in the experiments with nouns in context: for sentences with the intransitive this was done ad hoc during data curation, and for sentences with the transitive this is a one-to-one mapping to syntax. However, using a hard label for nouns in isolation is problematic as a semantic role label is meaningless without context of the event; in principle, given an appropriate context, anything can act upon some- thing else or have something done to it (literally or figuratively). To get around this, we have two methods for finding an approximate label for the \u201ctypical\u201d agen- tivity of a noun. The first was to collect human judgements. 19 annotators (native/fluent bilingual English proficiency) were given nouns without any context and were tasked to judge how likely each noun is to be an agent in any arbitrary event where both an agent and patient are involved. Their judge- ments were collected via ratings on a scale from 1 (very unlikely to be an agent) to 5 (very likely to be an agent). For nouns that have multiple com- mon word senses (e.g. \u201cmodel\u201d can refer to both a fashion model or machine learning model, among other things) we include a disambiguating descrip- tion. This description does not contain any verbs or other explicit indications of what events the noun may occur in (e.g. for \u201cmodel\u201d, we give human annotators \u201cmodel (person)\u201d).4 We then average the ratings across all annotators and normalize so that the values fall between 0 and 1. To calculate inter-annotator agreement, we randomly divide the annotators into two groups (of 9 and 10), average their ratings for each noun, and calculate the cor- relation between the two; doing this seven times yields an average inter-group correlation of 0.968. The second method uses statistics from linguisti- cally annotated corpora as a proxy for the \u201ctypical\u201d agentivity of a noun. We do this by calculating the frequency of \u201cagenthood\u201d for a noun (agent ratio), i.e. dividing the number of times the noun appears as an agent by the number of times it is either an agent or patient. The ideal annotated cor- pus for this would be one with semantic role labels such as Propbank (Kingsbury and Palmer, 2002), where the \u201cARG0\u201d label corresponds to agent and \u201cARG1\u201d to patient. However, many of the nouns in our data appeared only a few times in Propbank or not at all\u2014out of all 233 nouns, only 166 of them occurred within an ARG0 or ARG1 span.5 Thus, we also tried utilizing syntax as a proxy using Google Syntactic Ngrams biarcs (Goldberg and Orwant, 2013), as it is significantly larger. The biarcs portion of the corpus covers dependency relations between three connected content words, which includes transitive predicates. To calculate a similar ratio, we divide the number of times a noun occurs as a subject by the total number of subject and direct object occurrences (we call this the sub- ject ratio). A value closer to 1 should correlate with a tendency to occur more often as an agent, as agents are generally coded as subjects of English transitive verbs and patients as direct objects. All but one of our nouns contained at least one instance of occurring with a \u201cnsubj\u201d or \u201cdobj\u201d label. 3 Experimental Results We evaluate BLOOM (Scao et al., 2022), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020) models of varying sizes for all experiments. Since previous work has shown that models are highly sensitive to the ordering of examples (Lu et al., 2021), we run each experiment twice: once with the order shown in Figure 1 where an agent 4Additional details on collecting human ratings can be found in Appendix C. 5We used Propbank annotations for BOLT, EWT, from https://github.com/ and Ontonotes propbank/propbank-release. 5.0 Figure 2: Correlation between subject ratio (from Google Syntactic Ngrams) and human ratings for each noun (r = 0.762). The semantic role label is the role the noun takes as the subject of the intransitive verb within our test set. is first (APAP ordering) and again with the first example moved to the bottom (PAPA ordering). We compare models based on their average perfor- mance across both orderings. Note, however, that some models are more sensitive to orderings than others; some models (like text-davinci-003) are largely invariant to example ordering. In Ap- pendix D, we report results from both experiments. 3.1 Exp 1: Agentivity at the lexical level In order to see if models are sensitive to the notion"}