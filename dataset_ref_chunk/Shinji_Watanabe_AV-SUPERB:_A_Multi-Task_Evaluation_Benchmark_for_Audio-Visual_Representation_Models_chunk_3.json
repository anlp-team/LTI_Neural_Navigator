{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_AV-SUPERB:_A_Multi-Task_Evaluation_Benchmark_for_Audio-Visual_Representation_Models_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the study described in the text?", "answer": " The study focuses on comparing different models for speech processing tasks using various datasets.", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " What types of tasks are evaluated in the study?", "answer": " The tasks evaluated include Automatic Speech Recognition (ASR), Speaker Verification (ASV), and Emotion Recognition (ER).", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " Which dataset is chosen for ASR in the study?", "answer": " LRS3-TED is chosen for ASR in the study.", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " What evaluation metric is used for ASR in the study?", "answer": " The evaluation metric used for ASR is the Character Error Rate (CER).", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " Which models perform the best for ASR and ASV tasks according to the study?", "answer": " AV-HuBERT performs the best for ASR and ASV tasks.", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " What is the conclusion regarding the effectiveness of fusion layers in the models?", "answer": " Fusion layers are unable to effectively utilize additional lip reading information, as performance is reduced when video is provided.", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " In what scenario may audio-visual models improve audio representations?", "answer": " Audio-visual models may improve audio representations by utilizing information learned from visual grounding even when only audio input is available at inference.", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " What is the impact of intermediate-task fine-tuning on AV-HuBERT?", "answer": " Intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reducing usability for audio-only and audio-visual inputs.", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " What is the common outcome of fine-tuning MAViL on AudioSet-2M?", "answer": " Fine-tuning on AudioSet-2M brings substantial improvements to all AEC and AR datasets, and also improves ASV while maintaining ASR performance.", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}, {"question": " What layer is commonly more dominant for MAViL except for emotion recognition on IEMOCAP?", "answer": " For MAViL, the last three layers in the audio encoder and the last two layers in the video encoder and fusion layers are commonly more dominant.", "ref_chunk": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}], "doc_text": "two datasets compared to AudioSet and UCF101. We report testing set mean average preci- sion for multi-label classification on AudioSet, and accuracy for the remaining three datasets. For speech processing, we choose LRS3-TED for ASR, Vox- Celeb2 for ASV, and IEMOCAP for ER. For ASR, we optimize CTC loss for character-level ASR, and report character error rate. For ASV, we first train for speaker identification on a subset of the dev split, then calculate cosine similarity to do verification on the test split and report equal error rate. For ER, we follow conventional evaluation, remove unbalanced classes to perform four-way classifi- cation (neutral, happy, sad, angry) and report accuracy. Additional dataset and training details are given on our submission platform2. 4.2. Overall Results We find that existing models generally obtain large gains over hand- crafted features, yet none of the five models tested were able to out- perform all others in every task. To gauge universal performance across tasks, we provide an overall score calculated as the mean of either task-specific accuracies or the complement of error rates. For the three speech processing tasks (ASR, ASV, ER), AV- HuBERT performs the best on ASR and ASV, and HuBERT achieves superior performance on ER. Notably, the unimodal HuBERT scores competitively on ASR and ASV as well, despite not being trained to utilize any visual grounding information. For the four audio processing datasets, MAViL and AVBERT consistently outperforms all other models in all three tracks. We hypothesize that this is largely due to the diversity and large size of AudioSet data used for pretraining. Despite the domain mismatch, AVBERT also performs competitively for the ASV and ER speech tasks, especially in the audio-visual fusion track. However, MAViL and AVBERT cannot perform ASR well, as simply using handcrafted FBANK features achieves lower error rates. Comparing their scores in the audio-only and fusion tracks, we see that fusion layers are unable to effectively utilize the ad- ditional lip reading information, as performance is reduced when video is provided. 4.3. When does Visual Grounding Improve Audio Representa- tion Learning? Compared to unimodal audio representation models, audio-visual models may take advantage of information learned from visual grounding to improve audio representations even when only audio input is available at inference. Of the five evaluated models, Hu- BERT and AV-HuBERT use similar architectures, and optimize the same masked cluster prediction objective using k-means clusters of MFCC features as the initial targets. While HuBERT is only trained on unimodal speech data, AV-HuBERT is further trained to predict multimodal cluster targets obtained from both audio and visual modalities. Hence we compare audio-only track results for HuBERT and AV-HuBERT, and find that visual grounding from multimodal cluster prediction can obtain small gains for VoxCeleb2, Audio-Visual Speech-Visual Intermediate Task Fine-tuning Data AS-20K (mAP \u2191) AEC VGGSound Kinetics-Sounds (Acc. \u2191) (Acc. \u2191) AR UCF101 (Acc. \u2191) ASR LRS3-TED (CER \u2193) ASV VoxCeleb2 (EER \u2193) ER IEMOCAP (Acc. \u2191) AV-HuBERT Audio Video Fusion LRS3-TED (video-text pairs) 12.6(-0.6) 2.5(+0.1) 5.1(-8.2) 22.83(-8.31) 6.12(+0.22) 17.11(-15.58) 38.19(-10.83) 25.35(+0.62) 38.52(-13.71) 28.70(-9.88) 13.89(-10.88) 22.38(-7.93) 53.92(-4.62) 42.03(+4.48) 35.48(+15.43) 11.40(+0.50) 32.69(+6.10) 22.66(-19.91) 11.35(-1.89) 43.58(-2.87) 40.74(-0.72) MAViL Audio Video Fusion AudioSet-2M 44.79(+4.89) 28.3(+6.7) 20.9(+2.9) 36.68(+4.58) 39.1(+12.4) 55.94(+8.72) 62.93(+5.65) 77.39(+3.38) 84.93(+5.42) 50.10(+4.42) 86.93(+7.56) 88.07(+10.09) 23.99(+0.44) 78.59(-4.56) 30.65(-0.47) 21.77(-1.06) 58.17(-1.29) 23.93(+0.65) 39.15(-3.88) 18.61(+1.06) 46.35(-8.59) Table 2. Intermediate-task fine-tuning does not generally improve performance across all tasks. Results after intermediate-task fine-tuning (left) and absolute improvements compared to the original self-supervised model (right) are shown. Fine-tuning data for each model is color-coded to the corresponding downstream dataset. VGGSound and UCF101. may provide improved representations for speech/audio tasks after intermediate-task training. 4.4. Layer-wise Contribution Analysis After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utiliza- tion by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dom- inant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Video- only representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reduc- ing usability for audio-only and audio-visual inputs. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VG- GSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substan- tial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that fine- tuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6."}