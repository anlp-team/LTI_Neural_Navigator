{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_Multi-Objective_Improvement_of_Android_Applications_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What threshold was set for all experiments in terms of the number of benchmarks?", "answer": " 20 benchmarks", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What study did Callan et al (2022) conduct regarding app performance improvements?", "answer": " Callan et al (2022) conducted a study of the changes that Android developers make to improve app performance.", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " According to the text, what are some examples of changes within the GI search-space that can improve app performance?", "answer": " Moving an operation outside of a for loop if it only needs to be executed once.", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What was the total number of benchmarks reached in the experiment?", "answer": " 21 benchmarks", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What analysis tool was used on the 7 applications to identify performance issues?", "answer": " PMD static analyser", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " How many times was GIDroid run for each version of the improved code?", "answer": " 20 times", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What test was used to determine the improvement of a given property at the 5% confidence level?", "answer": " Mann-Whitney U test", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " How many generations were used for the evolutionary algorithms with 400 evaluations?", "answer": " 10 generations", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What is the purpose of the Genetic programming parameters in Table 2?", "answer": " They have been used successfully in the past.", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " How many hours of computing time were roughly spent to test the approach?", "answer": " 7500 hours", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}], "doc_text": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}