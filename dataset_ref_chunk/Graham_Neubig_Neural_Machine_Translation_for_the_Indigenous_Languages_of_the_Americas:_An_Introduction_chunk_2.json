{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Neural_Machine_Translation_for_the_Indigenous_Languages_of_the_Americas:_An_Introduction_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the general consensus regarding mapping the meaning of text into another language?", "answer": " It is generally not possible to map the meaning of text exactly into another language.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " How do most Neural Machine Translation (NMT) systems factorize the probability of generating a translation?", "answer": " Most NMT systems factorize the probability of generating a translation in a left-to-right fashion.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " What are the two common algorithms for finding a high-probability translation in NMT systems?", "answer": " The two common algorithms are greedy decoding and beam search.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " What are some alternative input units that have been explored in NMT systems?", "answer": " Some alternative input units include characters, byte pair encoding (BPE), morphological representations, and syllables.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " What are the two most common options for input representations in NMT systems?", "answer": " The two most common options are word embeddings and contextualized word representations.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " What are the two most frequent architectures for NMT models?", "answer": " The two most frequent architectures are recurrent neural networks (RNN) and transformers.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " What is one major advantage of neural models in NMT systems?", "answer": " One major advantage is their ability to learn representations from raw data.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " What is one challenge faced by neural models in NMT systems due to limited data?", "answer": " One challenge is overfitting, which can occur when not enough data is provided for effective learning.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " What is the most widely used metric for evaluating translation quality in NMT systems?", "answer": " The most widely used metric is BLEU, which relies on token-level n-gram matches between translations and gold-standard references.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}, {"question": " What is a shortcoming of evaluation metrics like BLEU and chrF in NMT systems?", "answer": " A shortcoming is that the evaluation is very dependent on surface forms and not on the ultimate goal of semantic similarity and fluency.", "ref_chunk": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}], "doc_text": "in general not possi- ble to map the meaning of text exactly into another language (Nida, 1945; Sechrest et al., 1972; Baker, 2018). (1) (2) (3) Most NMT systems factorize the probability of \u02c6Y = \u02c6y1, ..., \u02c6yT in a left-to-right fashion: p( \u02c6Y ) = T (cid:89) p(\u02c6yt|\u02c6y<t, X, \u03b8) t=1 Thus, the probability of token \u02c6yt at time step t is computed using the previously generated tokens \u02c6y<t, the source sentence X and the model param- eters \u03b8. Common algorithms for finding a high- probability translation are greedy decoding, i.e., picking the token with the highest probability at each time step, and beam search (Lowerre, 1976). 2.1 Input Representations The texts X and Y are input into an NMT sys- tem as sequences of continuous vectors. How- ever, defining which units should be represented as such vectors is non-trivial. The classic way is to represent each word within X and Y as a vector (or embedding). However, in a low- resource setting, often not all vocabulary items ap- pear in the training data (Jean et al., 2015; Lu- ong et al., 2015). This issue especially effects lan- guages with a rich inflectional morphology (Sen- nrich et al., 2016c): as many word forms can represent the same lemma, the vocabulary cover- age decreases drastically. Furthermore, for many LRLs, boundaries between words or morphemes are not easy to obtain or not well defined in the case of languages without a standard orthography. Alternative input units have been explored, such as characters (Ling et al., 2015), byte pair encoding (BPE; Sennrich et al., 2016a), morphological rep- resentations (Vania and Lopez, 2017; Ataman and Federico, 2018), syllables (Zhang et al., 2019), or, recently, a visual representation of rendered text (Salesky et al., 2021). No clear advantage has been discovered for using morphological segmentations over BPEs when testing them on LRLs (Saleva and Lignos, 2021). Input representations can be pretrained. The two most common options are: i) word em- beddings, where each type is represented by a vector, e.g., Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bo- janowski et al., 2017)) embeddings, and ii) contex- tualized word representations, where entire sen- tences are being encoded at a time, e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). However, training of these methods re- quires large monolingual training corpora, which (4) may not be readily available for LRLs. As most ILA have rich morphology, this topic has gath- ered special interest. The discussion about the us- age of morpholigical segmented input for NMT (Mager et al., 2022) show models is recurrent. that the unsupervised morphologically inspired models outperform BPE pre-processing (experi- mented on 4 language pares). Similar experi- ments done on Quechua\u2013Spanish and Inuktitut\u2013 Enlgish (Schwartz et al., 2020), comparing BPEs against Morfessor (Smit et al., 2014). Also (Or- tega et al., 2020a) improves the SOTA (state-of- the-art) for Quechua\u2013Spanish MT using a mor- phological guided BPE algorithm. 2.2 Architectures NMT models typically are sequence-to-sequence models. They encode a variable-length sequence into a vector or matrix representation, which they then decode back into a variable-length sequence (Cho et al., 2014). The two most frequent archi- tectures are: i) recurrent neural networks (RNN), such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014), and ii) trans- formers (Vaswani et al., 2017), which define the current state of the art in the high-resource setting. As for most neural network models, training an NMT system on a limited number of instances is challenging (Fern\u00e1ndez-Delgado et al., 2014). There are common problems that arise from lim- ited data in the training set. One major advan- tage of neural models is their ability to learn rep- resentations from raw data, in contrast to manu- ally engineered features (Barron, 1993). However, problems arise when not enough data is provided to enable effective learning of features. Another strength of neural networks is their generalization capacity (Kawaguchi et al., 2017). However, train- ing a neural network on a small dataset easily leads to overfitting (Rolnick et al., 2017). Recent stud- ies, however, show empirically that this does not necessarily happen if the network is tuned cor- rectly (Olson et al., 2018). 2.3 Evaluation Accurately judging translation quality is difficult and, thus, often still done manually: bilingual speakers assign scores according to provided crite- ria such as fluency and adequacy (Does the output have the same meaning as the input?). However, manual evaluation is expensive and slow. More- over, in the case of endangered languages, bilin- gual speakers can be hard or impossible to find. Automatic metrics provide an alternative.2 These metrics assign a score to system output, given one or more ground truth reference transla- tions. The most widely used metric is BLEU (Pa- pineni et al., 2002), which relies on token-level n- gram matches between the translation to be rated and one or more gold-standard translations. For morphologically rich languages, character-level metrics, such as chrF (Popovi\u00b4c, 2017), are often more suitable, as they allow for more flexibility. In the AmericasNLP ST (Mager et al., 2021) this metric was used over BLEU, as it fits better to the rich morphology of many ILA. To have a concrete example, lets have the fol- lowing Wixarika phrase with an English transla- tion: yu-huta-me an-two-ns ne-p+-we-\u2019iwa 1sg:s-asi-2pl:o-brother I have two brothers As discussed in (Mager et al., 2018c) it is dif- ficult to translate back from Spanish (or other Fu- sional language) the morpheme p+ as it has not equivalent in these languages. So if we would ig- nore these morpheme at all, BLEU would penal- ize the entire word nep+we\u2019iwa. In contrast, chrF would give credit to the translation, even if the p+ is missing. One shortcoming of these evaluation metrics is that the evaluation is very dependent on the sur- face forms and not on the ultimate goal of seman- tic similarity and fluency. Recent work uses pre- trained models to evaluate semantic similarity be- tween translations and the gold standard (Zhang"}