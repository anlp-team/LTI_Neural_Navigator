{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_A_Pretrainer's_Guide_to_Training_Data:_Measuring_the_Effects_of_Data_Age,_Domain_Coverage,_Quality,_&_Toxicity_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the text?", "answer": " The main focus of the text is on pretraining data design for language models.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " What are some key factors explored in the text regarding pretraining data?", "answer": " The text explores the effects of pretraining data age, domain coverage, quality, and toxicity filters.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " How many parameter decoder-only models were pretrained in the study?", "answer": " 28 1.5B parameter decoder-only models were pretrained in the study.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " What does the study find about the effect of pretraining data age on model performance?", "answer": " A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " What trade-off is shown in the study regarding quality and toxicity filters?", "answer": " The study shows a trade-off between performance on standard benchmarks and the risk of toxic generations when implementing quality and toxicity filters.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " What does the study conclude about the filtering of training data?", "answer": " The study concludes that there is not a one-size-fits-all solution to filtering training data.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " Why does the text mention the inclusion of heterogeneous data sources like books and web?", "answer": " The text mentions that the inclusion of heterogeneous data sources like books and web is broadly beneficial and warrants greater prioritization.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " What does the experimental pretraining curation pipeline include?", "answer": " The experimental pretraining curation pipeline includes sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " What two universities are mentioned as contributors to the study?", "answer": " MIT and Cornell University are mentioned as contributors to the study.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}, {"question": " What is the hope regarding the findings of the study?", "answer": " The hope is that the findings will help support more informed data-centric decisions in language model development.", "ref_chunk": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}], "doc_text": "3 2 0 2 v o N 3 1 ] L C . s c [ 2 v 9 6 1 3 1 . 5 0 3 2 : v i X r a A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Shayne Longpre 1 \u2662 * Gregory Yauney 2 \u2662 * Emily Reif 3 \u2662 Katherine Lee 2,3 \u2662 Adam Roberts 3 Barret Zoph 4 \u2020 Denny Zhou 3 Jason Wei 4 \u2020 Kevin Robinson 3 David Mimno 2 \u2662 Daphne Ippolito 3 \u2662 1 MIT 2 Cornell University 3 Google Research 4 OpenAI Abstract Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development. Toxic 2013 Domain-Specific Knowledge Toxic Identification 2012 Eval Tasks 2019 Select Pretraining Data 2020 Eval Tasks Web Evaluate Change in Performance on Downstream Tasks Code Pubmed Academic Books 2022 Toxic Generation 2016 Wikipedia Low quality Pretrain Model Figure 1: The experimental pretraining curation pipeline includes three steps: sub-selecting data from C4 or the Pile, pretraining a language model, and evaluating its change in performance over several benchmarks. Work completed while a Student Researcher at Google Research. \u2020 Work completed at Google Research. \u2662 Core contributor. Correspondence: slongpre@media.mit.edu Contents 1 Introduction 2 Methodology 2.1 . 2.2 Data Curation Choices . . 2.3 . 2.4 Models . Pretraining Datasets . Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Impact of Data Curation on Data Characteristics 4 Impact of Dataset Age on Pretrained Models 5 Impact of Quality & Toxicity Filters on Pretrained Models 6 Impact of Domain Composition on Pretrained Models 7 Discussion 8 Limitations 9 Related Work 10 Conclusion Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 6 7 7 9 11 13 15 16 17 19 28 1 Introduction The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining on massive text datasets. All model developers implicitly or explicitly decide the composition of these datasets: what data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. While many of the most prominent models do not document their curation procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those protocols or what effect they had. This documentation debt leaves practitioners to be guided by intuitions and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has detracted from responsible data use and hampered effective model development (Rogers, 2021; Gebru et al., 2021; Bender and Friedman, 2018). Among the small number of general-purpose LMs dominating community use and discussion, the prevailing focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions affect model performance\u2014specifically: the time of collection, content filtering strategy (toxicity/quality), and domain composition. We study the impacts in two ways. First, we present observational measurements of the effect of existing quality and toxicity filtering methods (Section 3). We document how these filters affect a range of characteristics in two major pretraining datasets, C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modified along one dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as findings and recommendations to model developers. The Age of a Dataset (Section 4). We see performance degradation if evaluation data is either before or after pretraining data collection,"}