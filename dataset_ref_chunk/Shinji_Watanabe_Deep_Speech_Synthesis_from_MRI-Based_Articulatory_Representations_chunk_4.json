{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Deep_Speech_Synthesis_from_MRI-Based_Articulatory_Representations_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How are the MRI features ranked in the text?", "answer": " The MRI features are ranked by score, with lower rank values being better.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " What does the darker green points in Figure 4 correspond to?", "answer": " The darker green points in Figure 4 correspond to better MRI features.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " How many EMA locations have an MRI point ranked as important?", "answer": " Each of the six EMA locations described have an MRI point ranked as important.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " What does the sparsity of points ranked as important suggest for the upper lip?", "answer": " The sparsity of points ranked as important suggests that the upper lip is not as important as the other EMA locations.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " Which regions besides the EMA locations are ranked as important in the text?", "answer": " Points around and in the pharyngeal region and velic region are also ranked as important.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " What sounds are the pharyngeal features relevant to?", "answer": " The pharyngeal features are relevant to the production of various speech sounds like /a/ and some variants of /r/ in English.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " Why are pharyngeal and velic features planned to be incorporated in all articulatory synthesis models?", "answer": " Pharyngeal and velic features are essential for fully-specified, high-fidelity articulatory synthesis.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " What is suggested to be prioritized when designing sparse articulatory feature sets?", "answer": " Constriction locations around the lips, tongue, or throat are generally ranked as important and may be useful to prioritize.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " What were the measurements used to evaluate the model in the text?", "answer": " The model was evaluated based on MCD, ASR, human evaluation, timing, and memory measurements.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}, {"question": " What tasks will the work be extended to in the future?", "answer": " The work will be extended to multi-speaker synthesis and inversion tasks.", "ref_chunk": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}], "doc_text": "the average of the MCD values corresponding to experiments where that feature was unmasked. Since our subsets are chosen randomly, we try each feature an equal number of times in ex- pectation. We rank the MRI features by score, with lower rank values being better and corresponding to a lower score and av- erage MCD. Figure 4, with darker green points corresponding to better MRI features. We note that each of the six EMA loca- tions described in Section 6 have an MRI point that is ranked as important. Moreover, for these six locations, besides, the upper lip, the number of points ranked as important is fairly sparse. This suggests that six points chosen in the EMA feature set are all very valuable for articulatory synthesis. The important MRI features also correspond well to the phonetic constriction task variables, e.g., those used in [37] to model articulatory synergies from real-time MRI images. Beyond the corresponding EMA locations, points around and in the pharyngeal region (e.g., be- tween tongue root or epiglottis and rear pharyngeal wall) and velic region are also ranked as important. This suggests that these features are also essential for fully-specified, high-fidelity articulatory synthesis. The pharyngeal features are relevant to the production of various speech sounds [38], like /a/ [39] and some variants of /r/ in English [40]. The velic features are cru- cial to the production of nasal sounds. Both of these features are not available from EMA. Thus, moving forward, we plan to in- corporate pharyngeal and velic features in all of our articulatory synthesis models. Points around constriction locations, whether at the lips, tongue, or throat, are generally ranked as important. Thus, when designing sparse articulatory feature sets, it may be useful to prioritize these constriction locations. 8. Conclusion and Future Directions In this work, we devise a new articulatory synthesis method us- ing MRI-based features, providing preprocessing and modelling strategies for working with such data. Based on MCD, ASR, hu- man evaluation, timing, and memory measurements, our model achieves noticeably better fidelity and computational efficiency than the prior intermediate-representation approach. Through speech synthesis ablations, we also show the advantages of MRI over EMA and identify the most important MRI features for ar- ticulatory synthesis. Moving forward, we will extend our work to multi-speaker synthesis and inversion tasks [19, 17]. 9. References [1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, \u201cEspnet2-tts: Extending the edge of tts research,\u201d arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, \u201cCross- lingual transfer for speech processing using acoustic language similarity,\u201d in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, \u201cJets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,\u201d 09 2022, pp. 21\u201325. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations,\u201d in Inter- speech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn generative spoken language modeling from raw au- dio,\u201d Transactions of the Association for Computational Linguis- tics, vol. 9, pp. 1336\u20131354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, \u201cSpeech syn- thesis from neural decoding of spoken sentences,\u201d Nature, vol. 568, no. 7753, pp. 493\u2013498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. Tu- Chan, K. Ganguly et al., \u201cGeneralizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paral- ysis,\u201d Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, \u201cWhat can basic research contribute to speech synthe- sis?\u201d Journal of Phonetics, vol. 19, no. 1, pp. 75\u201390, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, \u201cAn articulatory synthesizer for perceptual research,\u201d The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321\u2013328, 1981. [12] C. Scully, \u201cArticulatory synthesis,\u201d in Speech production and speech modelling. Springer, 1990, pp. 151\u2013186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anu- manchipalli, \u201cDeep speech synthesis from articulatory represen- tations,\u201d in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. T\u00b4oth, \u201cReconstructing speech from real-time articulatory mri using neural vocoders,\u201d in EUSIPCO, 2021, pp. 945\u2013949. [15] G. Begu\u02c7s, A. Zhou, P. Wu, and G. K. Anumanchipalli, \u201cArtic- ulation gan: Unsupervised modeling of articulatory learning,\u201d ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, \u201cAcoustic-to-articulatory in- version mapping with gaussian mixture model,\u201d in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, \u201cSpeaker-independent acoustic-to-articulatory speech inversion,\u201d in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, \u201cEv- idence of vocal tract articulation in self-supervised learning of speech,\u201d ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., \u201cA multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,\u201d Scientific data, vol. 8, no. 1, pp. 1\u201314, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, \u201cDeep neural convolutive matrix factorization for articulatory rep- resentation decomposition,\u201d in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, \u201cArticulatory representation learning via joint factor"}