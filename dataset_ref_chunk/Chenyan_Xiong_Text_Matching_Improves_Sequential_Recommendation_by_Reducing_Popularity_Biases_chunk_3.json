{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Chenyan_Xiong_Text_Matching_Improves_Sequential_Recommendation_by_Reducing_Popularity_Biases_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the sequential recommendation task aimed at?", "answer": " The sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time.", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " How does TASTE propose to encode long text sequences of user-item interactions?", "answer": " TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions.", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " How is a user-item interaction history H represented?", "answer": " A user-item interaction history H is represented as a concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H.", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " What is T5 used for in the encoding process?", "answer": " T5 is used to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63.", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " How is the relevance between user-item interaction history H and item \ud835\udc63 calculated?", "answer": " The relevance between user-item interaction history H and item \ud835\udc63 is calculated using the ranking probability \ud835\udc43 (\ud835\udc63 | H) between the encoded representations \u210e H and \u210e\ud835\udc63.", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " What loss function is used to optimize the model parameters?", "answer": " The model parameters are optimized using the CrossEntropy loss function.,", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " What is the attention sparsity mechanism in TASTE used for?", "answer": " The attention sparsity mechanism in TASTE learns session-based user behaviors by employing a fusion-in-decoder architecture.", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " How is the text sequence of user-item interaction history split in TASTE?", "answer": " The text sequence of user-item interaction history is split into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }.", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " What mechanism is employed to reduce encoding computational complexity in TASTE?", "answer": " TASTE employs a mechanism that eliminates attention computations among different sessions within Transformer encoder modules, reducing the encoding computational complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b).", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}, {"question": " What datasets are used in the experiments described?", "answer": " Four sequential recommendation datasets are used in the experiments, including Yelp, Amazon Beauty, Sports, and Toys.", "ref_chunk": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}], "doc_text": "We first describe how to verbalize users and items and then model the se- quential recommendation using text matching (Sec. 3.1). Finally, TASTE proposes an attention sparsity method to encode long text sequences of user-item interactions (Sec. 3.2). 3.1 Text Matching based Sequential Recommendation Modeling Given a user-item interaction history H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121}, the sequential recommendation task aims to recommend the item \ud835\udc63\ud835\udc61 to satisfy the user\u2019s needs at \ud835\udc61-th time. TASTE encodes H and item \ud835\udc63 using pretrained language model T5 [48] and models the relevance between users and items by matching their text representations. Text Representation. For each item \ud835\udc63, we can verbalize it using item ids and \ud835\udc58 item attributes <Attr> using the following template: \ud835\udc4b (\ud835\udc63) = id:\ud835\udc63 (id)...<Attr>\ud835\udc58 : \ud835\udc63 (<Attr>\ud835\udc58 ), where <Attr>\ud835\udc58 is the name of attribute. \ud835\udc63 (id) and \ud835\udc63 (<Attr>\ud835\udc56 ) are the text descriptions of item identifier and \ud835\udc56-th attribute of item \ud835\udc63. In this case, one item in Yelp can be verbalized: \u201cid: 5908 title: DoMazing address: 6659 S Las Vegas Blvd, Ste B-101 Las Vegas NV\u201d. Here \ud835\udc63 (id) is a kind of id prompt to help language models to capture matching signals among users and items, which is beyond the descriptions of item attributes. Then we verbalize the user-item interaction history H with the template: \u201cHere is the visit history list of user: \ud835\udc4b (H ) recommend (1) CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom next item\u201d. It demonstrates the definition of sequential recommen- dation and helps pretrained language models better characterize model behaviors [11, 20]. \ud835\udc4b (H ) is the concatenation of verbalized items {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61 \u22121} that are in H : \ud835\udc4b ( H) = \ud835\udc4b (\ud835\udc63\ud835\udc61 \u22121 ); ...; \ud835\udc4b (\ud835\udc631 ), where ; denotes the concatenation operation. We reverse the item sequences for truncating more previously interacted items. Encoding. We use T5 [48] to encode the user-item interaction history H and item \ud835\udc63 as embeddings \u210e H and \u210e\ud835\udc63, using the repre- sentation of the first input token from the T5 decoder [43]: \u210eH = T5(\ud835\udc4b ( H) ); \u210e\ud835\udc63 = T5(\ud835\udc4b (\ud835\udc63) ). (3) The relevance between user-item interaction history H and item \ud835\udc63 can be calculated using the ranking probability \ud835\udc43 (\ud835\udc63 |H ) between the encoded representations \u210e H and \u210e\ud835\udc63: \ud835\udc43 (\ud835\udc63 | H) = Softmax\ud835\udc63 (\u210eH \u00b7 \u210e\ud835\udc63 ), where \u00b7 is the dot product operation. Training. We optimize the model parameters using the follow- ing loss function: L = CrossEntropy(\ud835\udc43 (\ud835\udc63 | H), \ud835\udc63\u2217 ), (5) where \ud835\udc63 \u2217 denotes the ground truth item that is interacted by the user at the \ud835\udc61-th time. We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models. 3.2 Long Text Encoding for User-Item Interactions with Attention Sparsity In real-world scenarios, purchase or visit histories typically involve long-term interactions. The longer interaction usually contains more information to better model user behaviors and achieve more accurate recommendation results [45]. Instead of only using item ids to model user behavior [20], TASTE verbalizes the items in user- item interactions (Eq. 2) and makes the text utterance \ud835\udc4b (H ) longer, which challenges the long text processing ability of pretrained language models due to the max length boundary [13, 48, 54]. As shown in the bottom part of Figure 3, our attention spar- sity mechanism learns session-based user behaviors by employing a fusion-in-decoder architecture [27]. We first split the text se- quence \ud835\udc4b (H ) of user-item interaction history into \ud835\udc5b sub-sequences \u02c6\ud835\udc4b (H ) = { \u02c6\ud835\udc4b (H )1, ..., \u02c6\ud835\udc4b (H )\ud835\udc5b }. \u02c6\ud835\udc4b (H )\ud835\udc56 contains \ud835\udc5a tokens, which is regarded as a user-item interaction session. It reflects the user preferences of a specific period and can be characterized using a set of items that are interacted with users in the session [18, 36, 57]. We use T5-Encoder to independently encode these subsequences: \u02c6\u210eEncode = T5-Encoder( \u02c6\ud835\udc4b ( H)\ud835\udc56 ). \ud835\udc56 Then we concatenate \u02c6\u210eEncode = { \u02c6\u210eEncode } as the hidden state of the user-item interaction history H . It eliminates attention computations among different sessions within Trans- former encoder modules, thereby reducing the encoding computa- tional complexity from \ud835\udc42 (\ud835\udc5b2) to \ud835\udc42 (\ud835\udc5b). This approach also facilitates the encoding of longer text sequences of user-item interactions. ; \u02c6\u210eEncode 2 ; ...; \u02c6\u210eEncode \ud835\udc5b 1 Finally, we can get the representation of user-item interaction by feeding the sparsely encoded user-item interaction sequences to the decoder module of T5: \u210eH = T5-Decoder( \u02c6\u210eEncode, \u210eDecode 0 ), (2) (4) (6) (7) Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu Table 1: Statistics of Preprocessed Datasets. Dataset Beauty Yelp Sports Toys Data Information #Items 12,101 20,068 18,357 11,924 #Users 22,363 30,499 35,598 19,412 #Actions 198,502 317,182 296,337 167,597 Train 131,413 225,685 189,543 109,361 Split Dev 22,363 30,499 35,598 19,412 Test 22,363 30,499 35,598 19,412 where \u210eDecoder is the token embedding of the first input token of 0 the decoder, which is the same as Eq. 3. T5-Decoder uses the cross- attention mechanism to reweight different user-item interaction sessions and model the user behaviors by capturing text-matching signals from all tokens in the verbalized user-item interactions. 4 EXPERIMENTAL METHODOLOGY This section describes datasets, evaluation metrics, baselines, and implementation details in our experiments. Dataset. Four sequential recommendation datasets are used in our experiments, including Yelp1, Amazon Beauty, Sports and Toys [42], which aim to recommend locations and Amazon prod- ucts2 to satisfy users. All data statistics are shown in Table 1. We use Recbole [70] to process all datasets and keep all exper- iment settings the same as previous work [58, 71], which allows us to directly compare TASTE with baseline models [58]. For all datasets, we filter out the items and users that have less than five user-item interaction times in the whole datasets and treat all user- item interactions as implicit feedback [7, 52, 58, 71]."}